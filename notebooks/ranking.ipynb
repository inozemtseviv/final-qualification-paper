{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор моделей для этапа ранжирования кандидатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание\n",
    "Для пакетной рекомендательной системы характерны 3 этапа работы:\n",
    "1. Генерация кандидатов. На этом этапе происходит отбор сотен или тысяч объектов-кандидатов из множества объектов, которых могут быть миллиарды. Цель заключается в том, чтобы включить в этот набор макси-мально релевантные пользователю объекты и исключить нерелевантные.\n",
    "2. Скоринг и ранжирование. Здесь система оценивает кандидатов, полученных на предыдущем этапе, и присваивает каждому из них баллы, основанные на их релевантности для пользователя. Т.к. количество объек-тов на этом этапе ограничено, то можно использовать больший набор при-знаков объектов, контекстные факторы, например время и геопозицию.\n",
    "3. Повторное ранжирование. В рамках данного этапа проводится корректировка списка кандидатов, применяются бизнес-правила, использу-ются метрики разнообразия и новизны для уточнения позиций объектов-кандидатов в итоговом списке рекомендаций.\n",
    "\n",
    "В рамках этой задачи выполняется подбор моделей для этапа скоринга и ранжирования.\n",
    "Ограничение: использование только CPU.\n",
    "\n",
    "### Имеющиеся данные\n",
    "Социально-демографическая информация о пользователях и список их покупок - дата, название, цена, количество.\n",
    "Список сгенерированных кандидатов в разрезе покупателей.\n",
    "\n",
    "Разделение на трейн и тест: даты покупок. Самые последние покупки в тесте.\n",
    "Названия датафреймов с данными: train_data, test_data, candidates.\n",
    "\n",
    "**Состав train_data и test_data**:\n",
    "```\n",
    " 0   buyer_id                           object  идентификатор покупателя\n",
    " 1   buyer_birth_date                   object  дата рождения покупателя\n",
    " 2   buyer_sms_allowed                  int64   покупатель разрешил присылать СМС\n",
    " 3   buyer_emails_allowed               int64   покупатель разрешил присылать email\n",
    " 4   buyer_account_status_loyal         int64   покупатель в статусе \"Лояльный\" (0 или 1)\n",
    " 5   buyer_account_status_unregistered  int64   покупатель в статусе \"Не зарегистрирован\" (0 или 1)\n",
    " 6   buyer_account_status_new           int64   покупатель в статусе \"Новый\" (0 или 1)\n",
    " 7   buyer_account_status_potential     int64   покупатель в статусе \"Потенциальный\" (0 или 1)\n",
    " 8   buyer_account_status_lost          int64   покупатель в статусе \"Потерянный\" (0 или 1)\n",
    " 9   buyer_account_status_sleeping      int64   покупатель в статусе \"Спящий\" (0 или 1)\n",
    " 10  buyer_is_female                    int64   покупатель - женщина (0 или 1)\n",
    " 11  buyer_age                          int64   возраст покупателя\n",
    " 12  order_id                           object  идентификатор покупки\n",
    " 13  order_date                         object  дата покупки\n",
    " 14  order_day_of_week                  int64   день недели, в который была совершена покупка\n",
    " 15  order_hour_of_day                  int64   час дня, в который была совершена покупка\n",
    " 16  product_id                         object  идентификатор товара\n",
    " 17  product_name                       object  название товара\n",
    " 18  product_group                      object  название группы товара\n",
    " 19  product_count                      float64 количество товара\n",
    " 20  product_sum                        float64 сумма за товар\n",
    "```\n",
    "\n",
    "**Состав списка сгенерированных кандидатов**:\n",
    "```\n",
    " 0   buyer_id                           object  идентификатор покупателя\n",
    " 1   product_id                         object  идентификатор товара\n",
    "```\n",
    "\n",
    "\n",
    "### Метрики\n",
    "- NDCG@K\n",
    "- Precision@K\n",
    "- Recall@K\n",
    "- Diversity@K\n",
    "- Novelty@K\n",
    "- Serendipity@K\n",
    "\n",
    "Оптимизируем метрику NDCG@K, т.к. на этапе скоринга и ранжирования основной задачей является формирование упорядоченного по релевантности списка рекомендаций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорты и конфигурация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import logging\n",
    "import warnings\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import optuna\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from catboost import CatBoost, Pool as CBPool\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from optuna.trial import Trial\n",
    "from lightfm import LightFM\n",
    "import scipy.sparse as sparse\n",
    "import xgboost as xgb\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORGANIZATION_ID = \"\"\n",
    "PROCESSING_DATE = \"\"\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классы-утилиты и общие методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    \"\"\"\n",
    "    Калькулятор метрик для оценки качества рекомендаций.\n",
    "\n",
    "    Args:\n",
    "        k_values: список значений K для расчета метрик @K\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k_values: List[int]):\n",
    "        self.k_values = sorted(k_values)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_ndcg_for_user(\n",
    "        data_tuple: Tuple[str, Dict[str, np.ndarray], int],\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Вспомогательная функция для расчета NDCG одного пользователя.\n",
    "\n",
    "        Args:\n",
    "            data_tuple: кортеж (user_id, user_data, k) с данными пользователя и параметром k\n",
    "\n",
    "        Returns:\n",
    "            Optional[float]: значение NDCG для пользователя или None в случае ошибки\n",
    "        \"\"\"\n",
    "        user_id, user_data, k = data_tuple\n",
    "        try:\n",
    "            return ndcg_score(\n",
    "                y_true=[user_data[\"ideal\"]], y_score=[user_data[\"relevance\"]], k=k\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Ошибка при расчете NDCG для пользователя {user_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _calculate_ndcg_at_k(\n",
    "        self,\n",
    "        k: int,\n",
    "        prepared_data: Dict[str, Dict[str, np.ndarray]],\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Расчет метрики NDCG@K (Normalized Discounted Cumulative Gain) для заданного значения K.\n",
    "\n",
    "        NDCG - это метрика, которая оценивает качество ранжирования рекомендаций с учетом\n",
    "        их позиции в списке. В отличие от Precision и Recall, NDCG учитывает порядок\n",
    "        рекомендаций, придавая больший вес релевантным товарам на верхних позициях.\n",
    "\n",
    "        Формула расчета:\n",
    "        DCG@K = rel₁ + Σᵢ₌₂ᵏ (relᵢ / log₂(i + 1))\n",
    "        NDCG@K = DCG@K / IDCG@K\n",
    "        где:\n",
    "        - relᵢ - релевантность i-го товара (в нашем случае 0 или 1)\n",
    "        - IDCG@K - максимально возможное значение DCG@K (идеальное ранжирование)\n",
    "\n",
    "        Args:\n",
    "            k: количество рекомендаций для оценки\n",
    "            prepared_data: словарь подготовленных данных для расчета NDCG\n",
    "                {\n",
    "                    user_id: {\n",
    "                        \"relevance\": np.ndarray,  # вектор релевантности рекомендаций\n",
    "                        \"ideal\": np.ndarray       # вектор идеального ранжирования\n",
    "                    }\n",
    "                }\n",
    "\n",
    "        Returns:\n",
    "            float: среднее значение NDCG@K по всем пользователям.\n",
    "            Возвращает 0.0 в следующих случаях:\n",
    "            - если нет данных для расчета\n",
    "            - если все попытки расчета NDCG завершились ошибкой\n",
    "            - если prepared_data пуст\n",
    "\n",
    "        Example:\n",
    "            >>> prepared_data = {\n",
    "            ...     'user1': {\n",
    "            ...         'relevance': np.array([1, 0, 1, 0]),\n",
    "            ...         'ideal': np.array([1, 1, 0, 0])\n",
    "            ...     }\n",
    "            ... }\n",
    "            >>> calculator._calculate_ndcg_for_k(2, prepared_data)\n",
    "            0.6309  # NDCG@2 для данного примера\n",
    "\n",
    "        Notes:\n",
    "            - Особенности метрики:\n",
    "            * Учитывает порядок рекомендаций\n",
    "            * Нормализована на [0, 1]\n",
    "            * Чувствительна к позициям релевантных товаров\n",
    "            * Использует логарифмическое дисконтирование\n",
    "\n",
    "            - Интерпретация значений:\n",
    "            * 1.0 - идеальное ранжирование\n",
    "            * 0.0 - наихудшее возможное ранжирование\n",
    "            * Типичные значения в реальных системах: 0.3-0.7\n",
    "\n",
    "            - Важные замечания:\n",
    "            * Метод использует готовые векторы релевантности\n",
    "            * Требует предварительной подготовки данных\n",
    "            * Обрабатывает ошибки для каждого пользователя отдельно\n",
    "            * Использует реализацию из sklearn.metrics\n",
    "        \"\"\"\n",
    "        if not prepared_data:\n",
    "            return 0.0\n",
    "\n",
    "        # Подготавливаем данные для параллельной обработки\n",
    "        data_for_parallel = [\n",
    "            (user_id, user_data, k) for user_id, user_data in prepared_data.items()\n",
    "        ]\n",
    "\n",
    "        # Используем контекстный менеджер для автоматического закрытия пула\n",
    "        with Pool() as pool:\n",
    "            # Запускаем параллельные вычисления\n",
    "            results = pool.map(\n",
    "                MetricsCalculator._calculate_ndcg_for_user, data_for_parallel\n",
    "            )\n",
    "\n",
    "            # Фильтруем None значения и считаем среднее\n",
    "            valid_results = [r for r in results if r is not None]\n",
    "\n",
    "            return float(np.mean(valid_results)) if valid_results else 0.0\n",
    "\n",
    "    def _calculate_precision_for_user(\n",
    "        self,\n",
    "        pred_items_at_k: List[str],\n",
    "        true_items: set[str],\n",
    "        k: int,\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Расчет Precision@K для одного пользователя.\n",
    "\n",
    "        Precision (точность) показывает, какая доля рекомендованных системой товаров\n",
    "        в топ-K оказалась релевантной для пользователя. Эта метрика оценивает\n",
    "        способность системы давать точные рекомендации без \"шума\".\n",
    "\n",
    "        Формула расчета:\n",
    "        Precision@K = |релевантные товары ∩ рекомендованные товары в топ-K| / K\n",
    "\n",
    "        Args:\n",
    "            pred_items_at_k: список первых K рекомендованных товаров\n",
    "            true_items: множество фактически купленных товаров\n",
    "            k: количество рекомендаций для оценки\n",
    "\n",
    "        Returns:\n",
    "            float: значение Precision@K для пользователя\n",
    "            None: если нет данных для расчета\n",
    "\n",
    "        Example:\n",
    "            >>> pred_items_at_k = ['item1', 'item2']\n",
    "            >>> true_items = {'item1', 'item4'}\n",
    "            >>> calculator._calculate_precision_for_user(pred_items_at_k, true_items, k=2)\n",
    "            0.5  # из двух рекомендованных товаров только один оказался релевантным\n",
    "\n",
    "        Notes:\n",
    "            - Высокое значение Precision означает, что большинство рекомендаций релевантны\n",
    "            - Особенности метрики:\n",
    "            * Всегда нормализована на K\n",
    "            * При увеличении K обычно уменьшается\n",
    "            * Не учитывает общее количество релевантных товаров\n",
    "            * Чувствительна к порядку рекомендаций (в топ-K попадают первые K товаров)\n",
    "        \"\"\"\n",
    "        if not pred_items_at_k:\n",
    "            return None\n",
    "\n",
    "        relevant_count = len(set(pred_items_at_k) & true_items)\n",
    "        return relevant_count / k\n",
    "\n",
    "    def _calculate_recall_for_user(\n",
    "        self,\n",
    "        pred_items_at_k: List[str],\n",
    "        true_items: set[str],\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Расчет метрики Recall@K (полноты) для одного пользователя.\n",
    "\n",
    "        Recall (полнота) показывает, какую долю от всех релевантных товаров пользователя\n",
    "        система смогла рекомендовать в топ-K рекомендациях. Эта метрика оценивает\n",
    "        способность системы находить все интересные пользователю товары.\n",
    "\n",
    "        Формула расчета:\n",
    "        Recall@K = |релевантные товары ∩ рекомендованные товары в топ-K| / |релевантные товары|\n",
    "\n",
    "        Args:\n",
    "            pred_items_at_k: список первых K рекомендованных товаров\n",
    "            true_items: множество фактически купленных товаров\n",
    "\n",
    "        Returns:\n",
    "            float: значение Recall@K для пользователя\n",
    "            None: если нет данных для расчета (пустой список рекомендаций или нет релевантных товаров)\n",
    "\n",
    "        Example:\n",
    "            >>> pred_items_at_k = ['item1', 'item2']\n",
    "            >>> true_items = {'item1', 'item2', 'item4'}\n",
    "            >>> calculator._calculate_recall_for_user(pred_items_at_k, true_items)\n",
    "            0.667  # из трех релевантных товаров (item1, item2, item4)\n",
    "                # в топ-2 рекомендациях найдено два (item1, item2)\n",
    "\n",
    "        Notes:\n",
    "            - Особенности метрики для одного пользователя:\n",
    "            * Чувствительна к количеству релевантных товаров\n",
    "            * При увеличении K обычно растет\n",
    "            * Может быть низкой, если у пользователя много релевантных товаров\n",
    "            * Не учитывает порядок рекомендаций в топ-K\n",
    "            * Не учитывает нерелевантные рекомендации\n",
    "\n",
    "            - Важные замечания:\n",
    "            * Возвращает None для пустого списка рекомендаций\n",
    "            * Возвращает None если нет релевантных товаров\n",
    "            * Значение всегда в диапазоне [0, 1]\n",
    "            * Равен 1.0, если все релевантные товары найдены\n",
    "            * Равен 0.0, если не найдено ни одного релевантного товара\n",
    "        \"\"\"\n",
    "        if not pred_items_at_k or not true_items:\n",
    "            return None\n",
    "\n",
    "        relevant_count = len(set(pred_items_at_k) & true_items)\n",
    "        return relevant_count / len(true_items)\n",
    "\n",
    "    def _calculate_diversity_for_user(\n",
    "        self,\n",
    "        pred_items_at_k: List[str],\n",
    "        item_categories: Dict[str, str],\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Расчет метрики Diversity (разнообразия) для одного пользователя.\n",
    "\n",
    "        Diversity (разнообразие) измеряет насколько разнообразны рекомендации с точки зрения\n",
    "        категорий товаров. Высокое разнообразие означает, что система способна рекомендовать\n",
    "        товары из разных категорий, а не концентрируется только на нескольких популярных\n",
    "        категориях.\n",
    "\n",
    "        Формула расчета:\n",
    "        Diversity = количество уникальных категорий в рекомендациях / общее количество категорий\n",
    "\n",
    "        Args:\n",
    "            pred_items_at_k: список рекомендованных товаров\n",
    "            item_categories: словарь соответствия товаров и их категорий {item_id: category_id}\n",
    "\n",
    "        Returns:\n",
    "            float: значение Diversity для пользователя\n",
    "            None: если нет данных для расчета (пустой список рекомендаций или нет категорий)\n",
    "\n",
    "        Example:\n",
    "            >>> pred_items_at_k = ['item1', 'item2', 'item3']\n",
    "            >>> item_categories = {\n",
    "            ...     'item1': 'category1',\n",
    "            ...     'item2': 'category1',\n",
    "            ...     'item3': 'category2'\n",
    "            ... }\n",
    "            >>> calculator._calculate_diversity_for_user(pred_items_at_k, item_categories)\n",
    "            0.5  # рекомендации содержат товары из 2 категорий из 4 возможных\n",
    "\n",
    "        Notes:\n",
    "            - Высокое значение разнообразия может указывать на то, что система\n",
    "            предлагает пользователям широкий спектр товаров\n",
    "            - Низкое значение может говорить о том, что система \"зациклилась\"\n",
    "            на определенных категориях\n",
    "            - Особенности метрики для одного пользователя:\n",
    "            * Нормализована на общее количество доступных категорий\n",
    "            * Не зависит от порядка рекомендаций\n",
    "            * Учитывает только уникальные категории\n",
    "            * Игнорирует товары без категорий\n",
    "\n",
    "            - Важные замечания:\n",
    "            * Возвращает None для пустого списка рекомендаций\n",
    "            * Возвращает None если нет информации о категориях\n",
    "            * Значение всегда в диапазоне [0, 1]\n",
    "            * Равен 1.0, если рекомендации охватывают все категории\n",
    "            * Равен 0.0, если все рекомендации из одной категории\n",
    "\n",
    "        \"\"\"\n",
    "        if not pred_items_at_k or not item_categories:\n",
    "            return None\n",
    "\n",
    "        total_categories = len(set(item_categories.values()))\n",
    "        if total_categories == 0:\n",
    "            return None\n",
    "\n",
    "        recommended_categories = {\n",
    "            item_categories[item] for item in pred_items_at_k if item in item_categories\n",
    "        }\n",
    "\n",
    "        if not recommended_categories:\n",
    "            return None\n",
    "\n",
    "        return len(recommended_categories) / total_categories\n",
    "\n",
    "    def _calculate_novelty_for_user(\n",
    "        self,\n",
    "        pred_items_at_k: List[str],\n",
    "        test_items: set[str],\n",
    "        k: int,\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Расчет метрики Novelty (новизны) для пользователя.\n",
    "\n",
    "        Новизна оценивается как доля тестовых покупок пользователя,\n",
    "        которые не попали в топ-K рекомендаций. Низкое значение новизны\n",
    "        означает, что система хорошо предсказывает будущие покупки пользователя.\n",
    "\n",
    "        Формула расчета:\n",
    "        Novelty = |test_items - pred_items_at_k| / |test_items|\n",
    "\n",
    "        Args:\n",
    "            pred_items_at_k (List[str]):\n",
    "                Список рекомендованных товаров.\n",
    "\n",
    "            test_items (set[str]):\n",
    "                Множество товаров из тестового периода пользователя.\n",
    "\n",
    "            k (int):\n",
    "                Количество рекомендаций для оценки (топ-K).\n",
    "\n",
    "        Returns:\n",
    "            Optional[float]:\n",
    "                Значение новизны в диапазоне [0, 1], где:\n",
    "                - 0.0 означает, что все тестовые покупки были в рекомендациях\n",
    "                - 1.0 означает, что ни одна тестовая покупка не была рекомендована\n",
    "                - None возвращается, если у пользователя нет тестовых покупок\n",
    "\n",
    "        Example:\n",
    "            >>> test_items = {'item1', 'item2', 'item3'}  # 3 покупки в тесте\n",
    "            >>> pred_items = ['item1', 'item4', 'item2']  # top-3 рекомендации\n",
    "            >>> calculator._calculate_novelty_for_user(pred_items, test_items, 3)\n",
    "            0.33  # item3 не попал в рекомендации (1/3 покупок)\n",
    "\n",
    "        Notes:\n",
    "            - Особенности метрики:\n",
    "            * Оценивает способность системы предсказывать будущие покупки\n",
    "            * Учитывает только тестовые покупки пользователя\n",
    "            * Не зависит от общего количества рекомендаций\n",
    "            * Нормализована на количество тестовых покупок\n",
    "\n",
    "            - Интерпретация значений:\n",
    "            * Низкие значения - система хорошо предсказывает будущие покупки\n",
    "            * Высокие значения - система не смогла предсказать многие покупки\n",
    "        \"\"\"\n",
    "        if not test_items:\n",
    "            return None\n",
    "\n",
    "        # Берем только первые k рекомендаций\n",
    "        pred_items_set = set(pred_items_at_k[:k])\n",
    "\n",
    "        # Считаем долю тестовых покупок, которых нет в рекомендациях\n",
    "        missed_items = len(test_items - pred_items_set)\n",
    "        return missed_items / len(test_items)\n",
    "\n",
    "    def _calculate_serendipity_for_user(\n",
    "        self,\n",
    "        pred_items_at_k: List[str],\n",
    "        true_items: set[str],\n",
    "        popular_items: set[str],\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Расчет метрики Serendipity (неожиданности) для одного пользователя.\n",
    "\n",
    "        Serendipity (неожиданность) измеряет способность системы рекомендовать релевантные,\n",
    "        но неочевидные товары. Товар считается неожиданным, если он релевантен для пользователя\n",
    "        (присутствует в тестовых данных) и при этом не входит в множество популярных товаров.\n",
    "\n",
    "        Эта метрика помогает оценить способность системы находить \"скрытые жемчужины\" -\n",
    "        товары, которые нравятся пользователю, но не являются очевидным выбором.\n",
    "\n",
    "        Формула расчета:\n",
    "        Serendipity = количество релевантных неожиданных товаров / количество рекомендаций\n",
    "        где релевантный неожиданный товар - это товар, который:\n",
    "        1) есть в тестовых покупках пользователя (релевантный)\n",
    "        2) не входит в множество популярных товаров (неожиданный)\n",
    "\n",
    "        Args:\n",
    "            pred_items_at_k: список рекомендованных товаров\n",
    "            true_items: множество фактически купленных товаров из тестового периода\n",
    "            popular_items: множество идентификаторов популярных товаров\n",
    "\n",
    "        Returns:\n",
    "            float: значение Serendipity для пользователя\n",
    "            None: если нет данных для расчета (пустой список рекомендаций)\n",
    "\n",
    "        Example:\n",
    "            >>> pred_items_at_k = ['item1', 'item2', 'item3']\n",
    "            >>> true_items = {'item1', 'item3'}\n",
    "            >>> popular_items = {'item1', 'item2'}\n",
    "            >>> calculator._calculate_serendipity_for_user(\n",
    "            ...     pred_items_at_k, true_items, popular_items\n",
    "            ... )\n",
    "            0.333  # только item3 является релевантным и неожиданным\n",
    "                # (item1 релевантный, но популярный; item2 популярный и нерелевантный)\n",
    "\n",
    "        Notes:\n",
    "            - Особенности метрики для одного пользователя:\n",
    "            * Нормализована на количество рекомендаций\n",
    "            * Не зависит от порядка рекомендаций\n",
    "            * Учитывает как релевантность, так и популярность\n",
    "            * Более строгая метрика, чем Novelty или Precision\n",
    "            * Помогает оценить качество \"неочевидных\" рекомендаций\n",
    "\n",
    "            - Важные замечания:\n",
    "            * Возвращает None для пустого списка рекомендаций\n",
    "            * Значение всегда в диапазоне [0, 1]\n",
    "            * Равен 1.0, если все рекомендации релевантные и непопулярные\n",
    "            * Равен 0.0, если нет релевантных непопулярных рекомендаций\n",
    "            * Обычно имеет более низкие значения, чем другие метрики\n",
    "            * Зависит от выбранного порога популярности товаров\n",
    "        \"\"\"\n",
    "        if not pred_items_at_k:\n",
    "            return None\n",
    "\n",
    "        serendipitous_items = sum(\n",
    "            1\n",
    "            for item in pred_items_at_k\n",
    "            if item in true_items and item not in popular_items\n",
    "        )\n",
    "\n",
    "        return serendipitous_items / len(pred_items_at_k)\n",
    "\n",
    "    def _prepare_data(\n",
    "        self,\n",
    "        recommendations: Dict[str, List[str]],\n",
    "        test_data: pd.DataFrame,\n",
    "    ) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Подготовка данных для расчета NDCG.\n",
    "\n",
    "        Args:\n",
    "            recommendations: словарь {user_id: list of recommended items}\n",
    "            test_data: тестовый датафрейм с колонками 'buyer_id' и 'product_id'\n",
    "\n",
    "        Returns:\n",
    "            Dict с подготовленными данными для каждого пользователя\n",
    "        \"\"\"\n",
    "        # Получаем фактические покупки пользователей\n",
    "        true_items = test_data.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    "\n",
    "        # Собираем все уникальные товары\n",
    "        all_items = sorted(\n",
    "            list(set(item for items in recommendations.values() for item in items))\n",
    "        )\n",
    "        item_to_idx = {item: idx for idx, item in enumerate(all_items)}\n",
    "\n",
    "        # Готовим структуры для расчета NDCG\n",
    "        prepared_data = {}\n",
    "\n",
    "        for user_id, pred_items in recommendations.items():\n",
    "            if user_id not in true_items:\n",
    "                continue\n",
    "\n",
    "            # Создаем вектор релевантности для всех товаров\n",
    "            true_set = set(true_items[user_id])\n",
    "            n_items = len(all_items)\n",
    "\n",
    "            # Создаем полный вектор релевантности\n",
    "            relevance = np.zeros(n_items)\n",
    "            for item in pred_items:\n",
    "                if item in item_to_idx:\n",
    "                    relevance[item_to_idx[item]] = 1 if item in true_set else 0\n",
    "\n",
    "            # Создаем идеальный вектор релевантности\n",
    "            ideal = np.zeros(n_items)\n",
    "            for item in true_set:\n",
    "                if item in item_to_idx:\n",
    "                    ideal[item_to_idx[item]] = 1\n",
    "\n",
    "            prepared_data[user_id] = {\"relevance\": relevance, \"ideal\": ideal}\n",
    "\n",
    "        return prepared_data\n",
    "\n",
    "    def calculate(\n",
    "        self,\n",
    "        recommendations: Dict[str, List[str]],\n",
    "        train_data: pd.DataFrame,\n",
    "        test_data: pd.DataFrame,\n",
    "        item_categories: Optional[Dict[str, str]] = None,\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Расчет всех метрик\n",
    "\n",
    "        Args:\n",
    "            recommendations: словарь {user_id: list of recommended items}\n",
    "            train_data: тренировочный датафрейм для расчета новизны\n",
    "            test_data: тестовый датафрейм с колонками 'buyer_id' и 'product_id'\n",
    "            item_categories: словарь {item_id: category_id} для расчета разнообразия\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: словарь с метриками NDCG@K, Precision@K, Recall@K,\n",
    "            Diversity@K, Novelty@K и Serendipity@K для каждого значения K\n",
    "        \"\"\"\n",
    "        # Подготавливаем общие данные\n",
    "        prepared_data = self._prepare_data(recommendations, test_data)\n",
    "        true_items = test_data.groupby(\"buyer_id\")[\"product_id\"].agg(set).to_dict()\n",
    "\n",
    "        # Подготавливаем данные для serendipity\n",
    "        item_counts = train_data[\"product_id\"].value_counts()\n",
    "        threshold = np.percentile(\n",
    "            item_counts.values, 80\n",
    "        )  # 1 - 0.2 (percentile по умолчанию)\n",
    "        popular_items = set(item_counts[item_counts >= threshold].index)\n",
    "\n",
    "        # Инициализируем словарь с метриками\n",
    "        metrics = {}\n",
    "\n",
    "        # Инициализируем списки для хранения значений метрик\n",
    "        precision_values = []\n",
    "        recall_values = []\n",
    "        diversity_values = []\n",
    "        novelty_values = []\n",
    "        serendipity_values = []\n",
    "\n",
    "        # Рассчитываем все метрики\n",
    "        for k in self.k_values:\n",
    "            metrics[f\"ndcg_{k}\"] = self._calculate_ndcg_at_k(k, prepared_data)\n",
    "\n",
    "            for user_id, pred_items in recommendations.items():\n",
    "                pred_items_at_k = pred_items[:k]\n",
    "                user_id_in_true_items = user_id in true_items\n",
    "\n",
    "                # Precision\n",
    "                if user_id_in_true_items:\n",
    "                    precision_value = self._calculate_precision_for_user(\n",
    "                        pred_items_at_k=pred_items_at_k,\n",
    "                        true_items=true_items[user_id],\n",
    "                        k=k,\n",
    "                    )\n",
    "                    if precision_value is not None:\n",
    "                        precision_values.append(precision_value)\n",
    "\n",
    "                # Recall\n",
    "                if user_id_in_true_items:\n",
    "                    recall_value = self._calculate_recall_for_user(\n",
    "                        pred_items_at_k=pred_items_at_k, true_items=true_items[user_id]\n",
    "                    )\n",
    "                    if recall_value is not None:\n",
    "                        recall_values.append(recall_value)\n",
    "\n",
    "                # Diversity\n",
    "                diversity_value = self._calculate_diversity_for_user(\n",
    "                    pred_items_at_k=pred_items_at_k,\n",
    "                    item_categories=item_categories or {},\n",
    "                )\n",
    "                if diversity_value is not None:\n",
    "                    diversity_values.append(diversity_value)\n",
    "\n",
    "                # Novelty\n",
    "                novelty_value = self._calculate_novelty_for_user(\n",
    "                    pred_items_at_k=pred_items, test_items=true_items[user_id], k=k\n",
    "                )\n",
    "                if novelty_value is not None:\n",
    "                    novelty_values.append(novelty_value)\n",
    "\n",
    "                # Serendipity\n",
    "                if user_id_in_true_items:\n",
    "                    serendipity_value = self._calculate_serendipity_for_user(\n",
    "                        pred_items_at_k=pred_items_at_k,\n",
    "                        true_items=true_items[user_id],\n",
    "                        popular_items=popular_items,\n",
    "                    )\n",
    "                    if serendipity_value is not None:\n",
    "                        serendipity_values.append(serendipity_value)\n",
    "\n",
    "            # Сохраняем средние значения метрик\n",
    "            metrics[f\"precision_{k}\"] = (\n",
    "                float(np.mean(precision_values)) if precision_values else 0.0\n",
    "            )\n",
    "            metrics[f\"recall_{k}\"] = (\n",
    "                float(np.mean(recall_values)) if recall_values else 0.0\n",
    "            )\n",
    "            metrics[f\"diversity_{k}\"] = (\n",
    "                float(np.mean(diversity_values)) if diversity_values else 0.0\n",
    "            )\n",
    "            metrics[f\"novelty_{k}\"] = (\n",
    "                float(np.mean(novelty_values)) if novelty_values else 0.0\n",
    "            )\n",
    "            metrics[f\"serendipity_{k}\"] = (\n",
    "                float(np.mean(serendipity_values)) if serendipity_values else 0.0\n",
    "            )\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бейзлайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_train.csv\"\n",
    ")\n",
    "test_data = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_test.csv\"\n",
    ")\n",
    "candidates = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_als_recommendations.csv\"\n",
    ")\n",
    "\n",
    "# Создаем словарь категорий товаров\n",
    "item_categories = {\n",
    "    row[\"product_id\"]: row[\"product_group\"] for _, row in train_data.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularItemsRanker:\n",
    "    \"\"\"\n",
    "    Ранжировщик на основе популярности товаров.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.item_scores_ = None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        item_col: str = \"product_id\",\n",
    "    ) -> \"PopularItemsRanker\":\n",
    "        \"\"\"\n",
    "        Обучение ранжировщика на исторических данных.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            Датафрейм с историческими данными о покупках\n",
    "        item_col : str, по умолчанию 'product_id'\n",
    "            Название столбца с идентификаторами товаров\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self : PopularItemsRanker\n",
    "            Обученный объект\n",
    "        \"\"\"\n",
    "        # Подсчет количества покупок каждого товара\n",
    "        item_counts = data[item_col].value_counts()\n",
    "\n",
    "        # Нормализация скоров\n",
    "        self.item_scores_ = (item_counts / item_counts.max()).to_dict()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def recommend(\n",
    "        self,\n",
    "        candidates: pd.DataFrame,\n",
    "        top_n: int = 10,\n",
    "        user_col: str = \"buyer_id\",\n",
    "        item_col: str = \"product_id\",\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ранжирование кандидатов на основе популярности товаров.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        candidates : pd.DataFrame\n",
    "            Датафрейм с парами пользователь-товар для ранжирования\n",
    "        top_n : int, по умолчанию 10\n",
    "            Количество рекомендуемых товаров для каждого пользователя\n",
    "        user_col : str, по умолчанию 'buyer_id'\n",
    "            Название столбца с идентификаторами пользователей\n",
    "        item_col : str, по умолчанию 'product_id'\n",
    "            Название столбца с идентификаторами товаров\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        pd.DataFrame\n",
    "            Датафрейм с ранжированными рекомендациями и их скорами\n",
    "        \"\"\"\n",
    "        if self.item_scores_ is None:\n",
    "            raise ValueError(\"Модель не обучена. Сначала вызовите метод fit().\")\n",
    "\n",
    "        # Копируем датафрейм кандидатов\n",
    "        recommendations = candidates.copy()\n",
    "\n",
    "        # Добавляем скоры популярности\n",
    "        recommendations[\"score\"] = recommendations[item_col].map(\n",
    "            lambda x: self.item_scores_.get(x, 0.0)\n",
    "        )\n",
    "\n",
    "        # Сортируем рекомендации по пользователям и скорам\n",
    "        recommendations = recommendations.sort_values(\n",
    "            by=[user_col, \"score\"], ascending=[True, False]\n",
    "        )\n",
    "\n",
    "        # Оставляем только top_n рекомендаций для каждого пользователя\n",
    "        recommendations = recommendations.groupby(user_col).head(top_n)\n",
    "\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 10:38:30 - INFO - Создаем и обучаем ранжировщик\n",
      "2025-04-27 10:38:30 - INFO - Получаем рекомендации от ранжировщика\n",
      "2025-04-27 10:39:28 - INFO - Расчет метрик для рекомендаций от PopularItemsRanker\n",
      "2025-04-27 10:42:05 - INFO - Результаты:\n",
      "2025-04-27 10:42:05 - INFO - Метрики для K=10:\n",
      "2025-04-27 10:42:05 - INFO - NDCG@10: 0.3438\n",
      "2025-04-27 10:42:05 - INFO - Precision@10: 0.0622\n",
      "2025-04-27 10:42:05 - INFO - Recall@10: 0.1476\n",
      "2025-04-27 10:42:05 - INFO - Diversity@10: 0.0049\n",
      "2025-04-27 10:42:05 - INFO - Novelty@10: 0.8524\n",
      "2025-04-27 10:42:05 - INFO - Serendipity@10: 0.0000\n",
      "2025-04-27 10:42:05 - INFO - --------------------------------\n",
      "2025-04-27 10:42:05 - INFO - Метрики для K=100:\n",
      "2025-04-27 10:42:05 - INFO - NDCG@100: 0.4375\n",
      "2025-04-27 10:42:05 - INFO - Precision@100: 0.0342\n",
      "2025-04-27 10:42:05 - INFO - Recall@100: 0.1476\n",
      "2025-04-27 10:42:05 - INFO - Diversity@100: 0.0049\n",
      "2025-04-27 10:42:05 - INFO - Novelty@100: 0.8524\n",
      "2025-04-27 10:42:05 - INFO - Serendipity@100: 0.0000\n",
      "2025-04-27 10:42:05 - INFO - --------------------------------\n",
      "2025-04-27 10:42:05 - INFO - Метрики для K=1000:\n",
      "2025-04-27 10:42:05 - INFO - NDCG@1000: 0.4375\n",
      "2025-04-27 10:42:05 - INFO - Precision@1000: 0.0230\n",
      "2025-04-27 10:42:05 - INFO - Recall@1000: 0.1476\n",
      "2025-04-27 10:42:05 - INFO - Diversity@1000: 0.0049\n",
      "2025-04-27 10:42:05 - INFO - Novelty@1000: 0.8524\n",
      "2025-04-27 10:42:05 - INFO - Serendipity@1000: 0.0000\n",
      "2025-04-27 10:42:05 - INFO - --------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Создаем и обучаем ранжировщик на тренировочных данных\n",
    "logging.info(\"Создаем и обучаем ранжировщик\")\n",
    "ranker = PopularItemsRanker()\n",
    "ranker.fit(train_data)\n",
    "\n",
    "# Получаем рекомендации от ранжировщика\n",
    "logging.info(\"Получаем рекомендации от ранжировщика\")\n",
    "ranked_recommendations = ranker.recommend(candidates, top_n=10)\n",
    "\n",
    "# Преобразуем результаты ранжировщика в формат словаря для метрик\n",
    "ranker_recommendations = (\n",
    "    ranked_recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    ")\n",
    "\n",
    "# Создаем калькулятор метрик и считаем NDCG\n",
    "metrics_calculator = MetricsCalculator(k_values=[10, 100, 1000])\n",
    "\n",
    "logging.info(\"Расчет метрик для рекомендаций от PopularItemsRanker\")\n",
    "best_metrics = metrics_calculator.calculate(\n",
    "    recommendations=ranker_recommendations,\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    item_categories=item_categories,\n",
    ")\n",
    "\n",
    "# Вывод результатов\n",
    "logging.info(\"Результаты:\")\n",
    "for k in metrics_calculator.k_values:\n",
    "    logging.info(f\"Метрики для K={k}:\")\n",
    "    logging.info(f\"NDCG@{k}: {best_metrics[f'ndcg_{k}']:.4f}\")\n",
    "    logging.info(f\"Precision@{k}: {best_metrics[f'precision_{k}']:.4f}\")\n",
    "    logging.info(f\"Recall@{k}: {best_metrics[f'recall_{k}']:.4f}\")\n",
    "    logging.info(f\"Diversity@{k}: {best_metrics[f'diversity_{k}']:.4f}\")\n",
    "    logging.info(f\"Novelty@{k}: {best_metrics[f'novelty_{k}']:.4f}\")\n",
    "    logging.info(f\"Serendipity@{k}: {best_metrics[f'serendipity_{k}']:.4f}\")\n",
    "    logging.info(\"--------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель 1: LightFM (Матричная факторизация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_train.csv\"\n",
    ")\n",
    "test_data = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_test.csv\"\n",
    ")\n",
    "candidates = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_als_recommendations.csv\"\n",
    ")\n",
    "\n",
    "item_categories = {\n",
    "    row[\"product_id\"]: row[\"product_group\"] for _, row in train_data.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationRanker:\n",
    "    \"\"\"\n",
    "    Ранжировщик на основе матричной факторизации с использованием LightFM.\n",
    "\n",
    "    Attrs:\n",
    "    ----------\n",
    "    model : LightFM\n",
    "        Модель матричной факторизации\n",
    "    user_encoder : LabelEncoder\n",
    "        Энкодер для преобразования ID пользователей в индексы\n",
    "    item_encoder : LabelEncoder\n",
    "        Энкодер для преобразования ID товаров в индексы\n",
    "    user_features_matrix : sparse.csr_matrix\n",
    "        Разреженная матрица признаков пользователей\n",
    "    n_users : int\n",
    "        Количество пользователей\n",
    "    n_items : int\n",
    "        Количество товаров\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_components: int = 64,\n",
    "        learning_rate: float = 0.05,\n",
    "        loss: str = \"warp\",\n",
    "        random_state: int = 42,\n",
    "        n_epochs: int = 30,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация ранжировщика.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        n_components : int, по умолчанию 64\n",
    "            Размерность латентных векторов\n",
    "        learning_rate : float, по умолчанию 0.05\n",
    "            Скорость обучения\n",
    "        loss : str, по умолчанию 'warp'\n",
    "            Функция потерь ('warp', 'bpr', 'warp-kos')\n",
    "        random_state : int, по умолчанию 42\n",
    "            Seed для воспроизводимости результатов\n",
    "        n_epochs : int, по умолчанию 30\n",
    "            Количество эпох обучения\n",
    "        \"\"\"\n",
    "        self.model = LightFM(\n",
    "            no_components=n_components,\n",
    "            learning_rate=learning_rate,\n",
    "            loss=loss,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        self.n_epochs = n_epochs\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        self.user_features_matrix = None\n",
    "        self.n_users = None\n",
    "        self.n_items = None\n",
    "\n",
    "    def _prepare_user_features(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        user_col: str = \"buyer_id\",\n",
    "    ) -> sparse.csr_matrix:\n",
    "        \"\"\"\n",
    "        Подготовка признаков пользователей.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            Датафрейм с данными пользователей\n",
    "        user_col : str, по умолчанию 'buyer_id'\n",
    "            Название столбца с идентификаторами пользователей\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        sparse.csr_matrix\n",
    "            Разреженная матрица признаков пользователей\n",
    "        \"\"\"\n",
    "        feature_columns = [\n",
    "            \"buyer_sms_allowed\",\n",
    "            \"buyer_emails_allowed\",\n",
    "            \"buyer_account_status_loyal\",\n",
    "            \"buyer_account_status_unregistered\",\n",
    "            \"buyer_account_status_new\",\n",
    "            \"buyer_account_status_potential\",\n",
    "            \"buyer_account_status_lost\",\n",
    "            \"buyer_account_status_sleeping\",\n",
    "            \"buyer_is_female\",\n",
    "            \"buyer_age\",\n",
    "        ]\n",
    "\n",
    "        # Получаем уникальные значения для каждого пользователя\n",
    "        user_features = data.groupby(user_col)[feature_columns].first().reset_index()\n",
    "        user_features[\"user_idx\"] = self.user_encoder.transform(user_features[user_col])\n",
    "\n",
    "        # Создаем массивы для разреженной матрицы\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data_values = []\n",
    "\n",
    "        # Для каждого признака создаем соответствующие элементы разреженной матрицы\n",
    "        for feature_idx, feature_name in enumerate(feature_columns):\n",
    "            feature_values = user_features[feature_name].values\n",
    "            # Для числовых признаков (например, возраста) используем значение напрямую\n",
    "            if feature_name == \"buyer_age\":\n",
    "                # Нормализуем возраст и обрабатываем пропущенные значения\n",
    "                valid_mask = ~np.isnan(feature_values)\n",
    "                rows.extend(user_features.loc[valid_mask, \"user_idx\"])\n",
    "                cols.extend([feature_idx] * np.sum(valid_mask))\n",
    "                data_values.extend(feature_values[valid_mask])\n",
    "            else:\n",
    "                # Для бинарных признаков используем 1 только если значение True\n",
    "                valid_mask = feature_values == True\n",
    "                rows.extend(user_features.loc[valid_mask, \"user_idx\"])\n",
    "                cols.extend([feature_idx] * np.sum(valid_mask))\n",
    "                data_values.extend([1.0] * np.sum(valid_mask))\n",
    "\n",
    "        # Создаем разреженную матрицу\n",
    "        return sparse.csr_matrix(\n",
    "            (data_values, (rows, cols)), shape=(self.n_users, len(feature_columns))\n",
    "        )\n",
    "\n",
    "    def _prepare_interaction_matrix(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        user_col: str = \"buyer_id\",\n",
    "        item_col: str = \"product_id\",\n",
    "    ) -> sparse.coo_matrix:\n",
    "        \"\"\"\n",
    "        Подготовка матрицы взаимодействий пользователь-товар.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            Датафрейм с данными о взаимодействиях\n",
    "        user_col : str, по умолчанию 'buyer_id'\n",
    "            Название столбца с идентификаторами пользователей\n",
    "        item_col : str, по умолчанию 'product_id'\n",
    "            Название столбца с идентификаторами товаров\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        sparse.coo_matrix\n",
    "            Разреженная матрица взаимодействий\n",
    "        \"\"\"\n",
    "        interactions = (\n",
    "            data.groupby([user_col, item_col]).size().reset_index(name=\"count\")\n",
    "        )\n",
    "\n",
    "        interactions[\"user_idx\"] = self.user_encoder.transform(interactions[user_col])\n",
    "        interactions[\"item_idx\"] = self.item_encoder.transform(interactions[item_col])\n",
    "\n",
    "        return sparse.coo_matrix(\n",
    "            (\n",
    "                interactions[\"count\"],\n",
    "                (interactions[\"user_idx\"], interactions[\"item_idx\"]),\n",
    "            ),\n",
    "            shape=(self.n_users, self.n_items),\n",
    "        )\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        user_col: str = \"buyer_id\",\n",
    "        item_col: str = \"product_id\",\n",
    "    ) -> \"MatrixFactorizationRanker\":\n",
    "        \"\"\"\n",
    "        Обучение ранжировщика на исторических данных.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            Датафрейм с историческими данными о покупках\n",
    "        user_col : str, по умолчанию 'buyer_id'\n",
    "            Название столбца с идентификаторами пользователей\n",
    "        item_col : str, по умолчанию 'product_id'\n",
    "            Название столбца с идентификаторами товаров\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self : MatrixFactorizationRanker\n",
    "            Обученный объект\n",
    "        \"\"\"\n",
    "        # Кодируем ID пользователей и товаров\n",
    "        self.user_encoder.fit(data[user_col].unique())\n",
    "        self.item_encoder.fit(data[item_col].unique())\n",
    "\n",
    "        self.n_users = len(self.user_encoder.classes_)\n",
    "        self.n_items = len(self.item_encoder.classes_)\n",
    "\n",
    "        # Подготавливаем признаки и матрицу взаимодействий\n",
    "        self.user_features_matrix = self._prepare_user_features(data, user_col)\n",
    "        interaction_matrix = self._prepare_interaction_matrix(data, user_col, item_col)\n",
    "\n",
    "        num_threads = cpu_count()\n",
    "        # Обучаем модель\n",
    "        self.model.fit(\n",
    "            interaction_matrix,\n",
    "            user_features=self.user_features_matrix,\n",
    "            epochs=self.n_epochs,\n",
    "            verbose=True,\n",
    "            num_threads=num_threads,\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def rank(\n",
    "        self,\n",
    "        candidates: pd.DataFrame,\n",
    "        top_n: int = 10,\n",
    "        user_col: str = \"buyer_id\",\n",
    "        item_col: str = \"product_id\",\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ранжирование кандидатов на основе матричной факторизации.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        candidates : pd.DataFrame\n",
    "            Датафрейм с парами пользователь-товар для ранжирования\n",
    "        top_n : int, по умолчанию 10\n",
    "            Количество рекомендуемых товаров для каждого пользователя\n",
    "        user_col : str, по умолчанию 'buyer_id'\n",
    "            Название столбца с идентификаторами пользователей\n",
    "        item_col : str, по умолчанию 'product_id'\n",
    "            Название столбца с идентификаторами товаров\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        pd.DataFrame\n",
    "            Датафрейм с ранжированными рекомендациями и их скорами\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\") or self.model is None:\n",
    "            raise ValueError(\"Модель не обучена. Сначала вызовите метод fit().\")\n",
    "\n",
    "        recommendations = candidates.copy()\n",
    "\n",
    "        # Фильтруем только известные пользователи и товары\n",
    "        known_users_mask = recommendations[user_col].isin(self.user_encoder.classes_)\n",
    "        known_items_mask = recommendations[item_col].isin(self.item_encoder.classes_)\n",
    "\n",
    "        valid_recommendations = recommendations[\n",
    "            known_users_mask & known_items_mask\n",
    "        ].copy()\n",
    "\n",
    "        if valid_recommendations.empty:\n",
    "            return pd.DataFrame(columns=recommendations.columns + [\"score\"])\n",
    "\n",
    "        # Преобразуем ID в индексы только для известных пользователей и товаров\n",
    "        valid_recommendations[\"user_idx\"] = self.user_encoder.transform(\n",
    "            valid_recommendations[user_col]\n",
    "        )\n",
    "        valid_recommendations[\"item_idx\"] = self.item_encoder.transform(\n",
    "            valid_recommendations[item_col]\n",
    "        )\n",
    "\n",
    "        num_threads = cpu_count()\n",
    "        # Получаем предсказания модели для всех пар сразу\n",
    "        scores = self.model.predict(\n",
    "            user_ids=valid_recommendations[\"user_idx\"].values,\n",
    "            item_ids=valid_recommendations[\"item_idx\"].values,\n",
    "            user_features=self.user_features_matrix,\n",
    "            num_threads=num_threads,\n",
    "        )\n",
    "\n",
    "        valid_recommendations[\"score\"] = scores\n",
    "\n",
    "        # Сортируем рекомендации\n",
    "        valid_recommendations = valid_recommendations.sort_values(\n",
    "            by=[user_col, \"score\"], ascending=[True, False]\n",
    "        )\n",
    "\n",
    "        # Оставляем top_n рекомендаций для каждого пользователя\n",
    "        valid_recommendations = valid_recommendations.groupby(user_col).head(top_n)\n",
    "\n",
    "        # Удаляем временные столбцы\n",
    "        valid_recommendations = valid_recommendations.drop(\n",
    "            [\"user_idx\", \"item_idx\"], axis=1\n",
    "        )\n",
    "\n",
    "        return valid_recommendations\n",
    "\n",
    "    def save_model(self, model_path: str, export_parameters: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Сохранение модели и её параметров.\n",
    "\n",
    "        Args:\n",
    "            model_path: путь для сохранения модели\n",
    "            export_parameters: сохранять ли дополнительные параметры модели\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Модель не обучена. Сначала вызовите метод fit()\")\n",
    "\n",
    "        # Создаем директорию если её нет\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "        # Сохраняем модель LightFM\n",
    "        joblib.dump(self.model, model_path)\n",
    "\n",
    "        if export_parameters:\n",
    "            # Сохраняем дополнительные параметры модели\n",
    "            parameters = {\n",
    "                \"user_encoder\": self.user_encoder,\n",
    "                \"item_encoder\": self.item_encoder,\n",
    "                \"user_features_matrix\": self.user_features_matrix,\n",
    "                \"n_users\": self.n_users,\n",
    "                \"n_items\": self.n_items,\n",
    "                \"n_epochs\": self.n_epochs,\n",
    "            }\n",
    "\n",
    "            # Путь для сохранения параметров\n",
    "            params_path = f\"{os.path.splitext(model_path)[0]}_params.joblib\"\n",
    "\n",
    "            joblib.dump(parameters, params_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(\n",
    "        cls, model_path: str, load_parameters: bool = True\n",
    "    ) -> \"MatrixFactorizationRanker\":\n",
    "        \"\"\"\n",
    "        Загрузка сохранённой модели и её параметров.\n",
    "\n",
    "        Args:\n",
    "            model_path: путь к сохранённой модели\n",
    "            load_parameters: загружать ли дополнительные параметры модели\n",
    "\n",
    "        Returns:\n",
    "            MatrixFactorizationRanker: загруженная модель\n",
    "        \"\"\"\n",
    "        # Создаем экземпляр класса\n",
    "        ranker = cls()\n",
    "\n",
    "        # Загружаем модель LightFM\n",
    "        ranker.model = joblib.load(model_path)\n",
    "\n",
    "        if load_parameters:\n",
    "            # Путь к файлу с параметрами\n",
    "            params_path = f\"{os.path.splitext(model_path)[0]}_params.joblib\"\n",
    "\n",
    "            if os.path.exists(params_path):\n",
    "                parameters = joblib.load(params_path)\n",
    "\n",
    "                # Загружаем параметры\n",
    "                ranker.user_encoder = parameters[\"user_encoder\"]\n",
    "                ranker.item_encoder = parameters[\"item_encoder\"]\n",
    "                ranker.user_features_matrix = parameters[\"user_features_matrix\"]\n",
    "                ranker.n_users = parameters[\"n_users\"]\n",
    "                ranker.n_items = parameters[\"n_items\"]\n",
    "                ranker.n_epochs = parameters[\"n_epochs\"]\n",
    "            else:\n",
    "                logging.warning(\n",
    "                    f\"Файл с параметрами {params_path} не найден. \"\n",
    "                    \"Загружена только модель без дополнительных параметров.\"\n",
    "                )\n",
    "\n",
    "        return ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lightfm_parameters(\n",
    "    train_data: pd.DataFrame,\n",
    "    test_data: pd.DataFrame,\n",
    "    candidates: pd.DataFrame,\n",
    "    metrics_calculator: MetricsCalculator,\n",
    "    n_trials: int = 10,\n",
    "    timeout: int = 7200,\n",
    "    item_categories: Dict[str, str] = None,\n",
    ") -> Tuple[Dict, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Оптимизация параметров MatrixFactorizationRanker с помощью Optuna.\n",
    "\n",
    "    Args:\n",
    "        train_data: тренировочные данные\n",
    "        test_data: тестовые данные\n",
    "        candidates: кандидаты для ранжирования\n",
    "        metrics_calculator: калькулятор метрик\n",
    "        n_trials: количество итераций оптимизации\n",
    "        timeout: максимальное время оптимизации в секундах\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict, Dict[str, float]]:\n",
    "            - Лучшие параметры\n",
    "            - Значения метрик на лучших параметрах\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial: Trial) -> float:\n",
    "        # Определяем пространство гиперпараметров\n",
    "        model_params = {\n",
    "            \"n_components\": trial.suggest_int(\"n_components\", 32, 256),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.5, log=True),\n",
    "            \"loss\": trial.suggest_categorical(\"loss\", [\"warp\", \"bpr\", \"warp-kos\"]),\n",
    "            \"n_epochs\": trial.suggest_int(\"n_epochs\", 10, 50),\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "        # Создаем и обучаем модель\n",
    "        ranker = MatrixFactorizationRanker(\n",
    "            n_components=model_params[\"n_components\"],\n",
    "            learning_rate=model_params[\"learning_rate\"],\n",
    "            loss=model_params[\"loss\"],\n",
    "            random_state=model_params[\"random_state\"],\n",
    "            n_epochs=model_params[\"n_epochs\"],\n",
    "        )\n",
    "\n",
    "        # Обучаем модель\n",
    "        ranker.fit(train_data)\n",
    "\n",
    "        # Получаем предсказания\n",
    "        recommendations = ranker.rank(\n",
    "            candidates=candidates, top_n=max(metrics_calculator.k_values)\n",
    "        )\n",
    "\n",
    "        if recommendations.empty:\n",
    "            return 0.0\n",
    "\n",
    "        # Преобразуем в формат для расчета метрик\n",
    "        recommendations_dict = (\n",
    "            recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    "        )\n",
    "\n",
    "        # Рассчитываем метрики\n",
    "        metrics = metrics_calculator.calculate(\n",
    "            recommendations=recommendations_dict,\n",
    "            train_data=train_data,\n",
    "            test_data=test_data,\n",
    "        )\n",
    "\n",
    "        # Возвращаем метрику для оптимизации (NDCG@K)\n",
    "        target_k = max(metrics_calculator.k_values)\n",
    "        return metrics.get(f\"ndcg_{target_k}\", 0.0)\n",
    "\n",
    "    # Создаем исследование\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=f'lightfm_ranker_optimization_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    )\n",
    "\n",
    "    # Запускаем оптимизацию\n",
    "    logging.info(\"Начало оптимизации параметров...\")\n",
    "    study.optimize(\n",
    "        objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # Получаем лучшие параметры\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Добавляем фиксированные параметры\n",
    "    best_model_params = {\n",
    "        \"n_components\": best_params[\"n_components\"],\n",
    "        \"learning_rate\": best_params[\"learning_rate\"],\n",
    "        \"loss\": best_params[\"loss\"],\n",
    "        \"n_epochs\": best_params[\"n_epochs\"],\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    # Обучаем модель на лучших параметрах\n",
    "    logging.info(\"Обучение модели на лучших параметрах...\")\n",
    "    best_ranker = MatrixFactorizationRanker(**best_model_params)\n",
    "    best_ranker.fit(train_data)\n",
    "\n",
    "    # Получаем предсказания\n",
    "    recommendations = best_ranker.rank(\n",
    "        candidates=candidates, top_n=max(metrics_calculator.k_values)\n",
    "    )\n",
    "\n",
    "    # Рассчитываем финальные метрики\n",
    "    if recommendations.empty:\n",
    "        final_metrics = {f\"ndcg_{k}\": 0.0 for k in metrics_calculator.k_values}\n",
    "    else:\n",
    "        recommendations_dict = (\n",
    "            recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    "        )\n",
    "\n",
    "        final_metrics = metrics_calculator.calculate(\n",
    "            recommendations=recommendations_dict,\n",
    "            train_data=train_data,\n",
    "            test_data=test_data,\n",
    "            item_categories=item_categories\n",
    "        )\n",
    "\n",
    "    return best_params, final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 16:11:35,544] A new study created in memory with name: lightfm_ranker_optimization_20250425_161135\n",
      "2025-04-25 16:11:35 - INFO - Начало оптимизации параметров...\n",
      "Epoch: 100%|██████████| 23/23 [04:46<00:00, 12.44s/it]\n",
      "Best trial: 0. Best value: 0.832345:  10%|█         | 1/10 [12:14<1:50:07, 734.17s/it, 734.17/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 16:23:49,714] Trial 0 finished with value: 0.8323453801529145 and parameters: {'n_components': 184, 'learning_rate': 0.36596795884334166, 'loss': 'warp-kos', 'n_epochs': 23}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [03:11<00:00,  9.56s/it]\n",
      "                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 16:34:37,780] Trial 1 finished with value: 0.8323453801529145 and parameters: {'n_components': 247, 'learning_rate': 0.0058284023495519775, 'loss': 'warp', 'n_epochs': 20}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 37/37 [02:22<00:00,  3.84s/it]2/10 [23:02<1:31:08, 683.52s/it, 1382.24/7200 seconds]\n",
      "Best trial: 0. Best value: 0.832345:  20%|██        | 2/10 [31:50<1:31:08, 683.52s/it, 1382.24/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 16:43:25,618] Trial 2 finished with value: 0.8323453801529145 and parameters: {'n_components': 94, 'learning_rate': 0.01657483076242569, 'loss': 'warp', 'n_epochs': 37}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 39/39 [08:43<00:00, 13.43s/it]3/10 [31:50<1:11:27, 612.43s/it, 1910.08/7200 seconds]\n",
      "Best trial: 0. Best value: 0.832345:  40%|████      | 4/10 [48:09<1:15:43, 757.26s/it, 2889.35/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 16:59:44,893] Trial 3 finished with value: 0.8323453801529145 and parameters: {'n_components': 250, 'learning_rate': 0.03111976957285522, 'loss': 'warp-kos', 'n_epochs': 39}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 30/30 [04:41<00:00,  9.37s/it]\n",
      "Best trial: 0. Best value: 0.832345:  50%|█████     | 5/10 [1:00:04<1:01:51, 742.22s/it, 3604.90/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:11:40,445] Trial 4 finished with value: 0.8323453801529145 and parameters: {'n_components': 209, 'learning_rate': 0.005721982255349841, 'loss': 'bpr', 'n_epochs': 30}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 24/24 [01:48<00:00,  4.53s/it]\n",
      "Best trial: 0. Best value: 0.832345:  60%|██████    | 6/10 [1:08:16<43:48, 657.04s/it, 4096.59/7200 seconds]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:19:52,137] Trial 5 finished with value: 0.8323453801529145 and parameters: {'n_components': 75, 'learning_rate': 0.025472054734049353, 'loss': 'warp-kos', 'n_epochs': 24}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 42/42 [02:47<00:00,  3.98s/it]\n",
      "Best trial: 0. Best value: 0.832345:  70%|███████   | 7/10 [1:17:18<30:58, 619.37s/it, 4638.40/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:28:53,944] Trial 6 finished with value: 0.8323453801529145 and parameters: {'n_components': 67, 'learning_rate': 0.04832839049581044, 'loss': 'warp-kos', 'n_epochs': 42}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 11/11 [02:21<00:00, 12.88s/it]\n",
      "Best trial: 0. Best value: 0.832345:  80%|████████  | 8/10 [1:27:08<20:19, 609.90s/it, 5228.03/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:38:43,575] Trial 7 finished with value: 0.8323453801529145 and parameters: {'n_components': 190, 'learning_rate': 0.4273497720739165, 'loss': 'warp-kos', 'n_epochs': 11}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 27/27 [03:19<00:00,  7.39s/it]\n",
      "Best trial: 0. Best value: 0.832345:  90%|█████████ | 9/10 [1:37:25<10:12, 612.13s/it, 5845.05/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:49:00,598] Trial 8 finished with value: 0.8323453801529145 and parameters: {'n_components': 162, 'learning_rate': 0.004469104521297052, 'loss': 'bpr', 'n_epochs': 27}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 25/25 [02:27<00:00,  5.88s/it]\n",
      "                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 17:58:18,650] Trial 9 finished with value: 0.8323453801529145 and parameters: {'n_components': 145, 'learning_rate': 0.0014783076507807222, 'loss': 'warp', 'n_epochs': 25}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.832345: 100%|██████████| 10/10 [1:46:43<00:00, 640.31s/it, 6403.11/7200 seconds]\n",
      "2025-04-25 17:58:18 - INFO - Обучение модели на лучших параметрах...\n",
      "Epoch: 100%|██████████| 23/23 [04:49<00:00, 12.59s/it]\n",
      "2025-04-25 18:12:26 - INFO - Лучшие параметры: {'n_components': 184, 'learning_rate': 0.36596795884334166, 'loss': 'warp-kos', 'n_epochs': 23}\n",
      "2025-04-25 18:12:26 - INFO - Результаты:\n",
      "2025-04-25 18:12:26 - INFO - Метрики для K=10:\n",
      "2025-04-25 18:12:26 - INFO - NDCG@10: 0.8398\n",
      "2025-04-25 18:12:26 - INFO - Precision@10: 0.0019\n",
      "2025-04-25 18:12:26 - INFO - Recall@10: 0.0035\n",
      "2025-04-25 18:12:26 - INFO - Diversity@10: 0.0056\n",
      "2025-04-25 18:12:26 - INFO - Novelty@10: 0.9948\n",
      "2025-04-25 18:12:26 - INFO - Serendipity@10: 0.0001\n",
      "2025-04-25 18:12:26 - INFO - --------------------------------\n",
      "2025-04-25 18:12:26 - INFO - Метрики для K=100:\n",
      "2025-04-25 18:12:26 - INFO - NDCG@100: 0.8277\n",
      "2025-04-25 18:12:26 - INFO - Precision@100: 0.0029\n",
      "2025-04-25 18:12:26 - INFO - Recall@100: 0.0417\n",
      "2025-04-25 18:12:26 - INFO - Diversity@100: 0.0237\n",
      "2025-04-25 18:12:26 - INFO - Novelty@100: 0.9927\n",
      "2025-04-25 18:12:26 - INFO - Serendipity@100: 0.0001\n",
      "2025-04-25 18:12:26 - INFO - --------------------------------\n",
      "2025-04-25 18:12:26 - INFO - Метрики для K=1000:\n",
      "2025-04-25 18:12:26 - INFO - NDCG@1000: 0.8323\n",
      "2025-04-25 18:12:26 - INFO - Precision@1000: 0.0032\n",
      "2025-04-25 18:12:26 - INFO - Recall@1000: 0.2871\n",
      "2025-04-25 18:12:26 - INFO - Diversity@1000: 0.0784\n",
      "2025-04-25 18:12:26 - INFO - Novelty@1000: 0.9920\n",
      "2025-04-25 18:12:26 - INFO - Serendipity@1000: 0.0001\n",
      "2025-04-25 18:12:26 - INFO - --------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    n_samples = 1000\n",
    "    # buyer_ids = test_data[\"buyer_id\"].unique()[:n_samples]\n",
    "    buyer_ids = test_data[\"buyer_id\"].unique()\n",
    "\n",
    "    temp_candidates = candidates[candidates[\"buyer_id\"].isin(buyer_ids)].copy()\n",
    "\n",
    "    product_ids = temp_candidates[\"product_id\"].unique()\n",
    "    temp_train_data = train_data[\n",
    "        (train_data[\"buyer_id\"].isin(buyer_ids))\n",
    "        & (train_data[\"product_id\"].isin(product_ids))\n",
    "    ].copy()\n",
    "\n",
    "    temp_test_data = test_data[\n",
    "        (test_data[\"buyer_id\"].isin(buyer_ids))\n",
    "        & (test_data[\"product_id\"].isin(product_ids))\n",
    "    ].copy()\n",
    "\n",
    "    metrics_calculator = MetricsCalculator([10, 100, 1000])\n",
    "\n",
    "    best_params, best_metrics = optimize_lightfm_parameters(\n",
    "        train_data=temp_train_data,\n",
    "        test_data=temp_test_data,\n",
    "        candidates=temp_candidates,\n",
    "        metrics_calculator=metrics_calculator,\n",
    "        n_trials=10,\n",
    "        timeout=7200,\n",
    "        item_categories=item_categories,\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Лучшие параметры: {best_params}\")\n",
    "    logging.info(\"Результаты:\")\n",
    "    for k in metrics_calculator.k_values:\n",
    "        logging.info(f\"Метрики для K={k}:\")\n",
    "        logging.info(f\"NDCG@{k}: {best_metrics[f'ndcg_{k}']:.4f}\")\n",
    "        logging.info(f\"Precision@{k}: {best_metrics[f'precision_{k}']:.4f}\")\n",
    "        logging.info(f\"Recall@{k}: {best_metrics[f'recall_{k}']:.4f}\")\n",
    "        logging.info(f\"Diversity@{k}: {best_metrics[f'diversity_{k}']:.4f}\")\n",
    "        logging.info(f\"Novelty@{k}: {best_metrics[f'novelty_{k}']:.4f}\")\n",
    "        logging.info(f\"Serendipity@{k}: {best_metrics[f'serendipity_{k}']:.4f}\")\n",
    "        logging.info(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель 2: CatBoost (Градиентный бустинг над решающими деревьями)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_train.csv\"\n",
    ")\n",
    "test_data = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_test.csv\"\n",
    ")\n",
    "candidates = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_als_recommendations.csv\"\n",
    ")\n",
    "\n",
    "item_categories = {\n",
    "    row[\"product_id\"]: row[\"product_group\"] for _, row in train_data.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostRanker:\n",
    "    \"\"\"\n",
    "    Ранжировщик на основе CatBoost.\n",
    "\n",
    "    Attrs:\n",
    "        model: CatBoost\n",
    "            Модель CatBoost для ранжирования\n",
    "        feature_names: List[str]\n",
    "            Список используемых признаков\n",
    "        categorical_features: List[str]\n",
    "            Список категориальных признаков\n",
    "        numeric_fill_values_: Dict[str, float]\n",
    "            Словарь для хранения значений для заполнения пропусков в числовых признаках\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        iterations: int = 1000,\n",
    "        depth: int = 6,\n",
    "        l2_leaf_reg: float = 3.0,\n",
    "        random_seed: int = 42,\n",
    "        thread_count: int = -1,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация ранжировщика.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: скорость обучения\n",
    "            iterations: количество итераций\n",
    "            depth: глубина деревьев\n",
    "            l2_leaf_reg: L2 регуляризация\n",
    "            random_seed: seed для воспроизводимости\n",
    "            thread_count: количество потоков (-1 для использования всех)\n",
    "            verbose: выводить ли прогресс обучения\n",
    "        \"\"\"\n",
    "        self.model_params = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"iterations\": iterations,\n",
    "            \"depth\": depth,\n",
    "            \"l2_leaf_reg\": l2_leaf_reg,\n",
    "            \"random_seed\": random_seed,\n",
    "            \"thread_count\": thread_count,\n",
    "            \"verbose\": verbose,\n",
    "            \"task_type\": \"CPU\",\n",
    "            \"loss_function\": \"Logloss\",\n",
    "            \"eval_metric\": \"NDCG\",\n",
    "            \"early_stopping_rounds\": 50,\n",
    "        }\n",
    "\n",
    "        self.feature_names = [\n",
    "            # Социально-демографические признаки\n",
    "            \"buyer_sms_allowed\",\n",
    "            \"buyer_emails_allowed\",\n",
    "            \"buyer_account_status_loyal\",\n",
    "            \"buyer_account_status_unregistered\",\n",
    "            \"buyer_account_status_new\",\n",
    "            \"buyer_account_status_potential\",\n",
    "            \"buyer_account_status_lost\",\n",
    "            \"buyer_account_status_sleeping\",\n",
    "            \"buyer_is_female\",\n",
    "            \"buyer_age\",\n",
    "            # Временные признаки\n",
    "            \"order_day_of_week\",\n",
    "            \"order_hour_of_day\",\n",
    "            # Признаки покупок\n",
    "            \"product_count\",\n",
    "            \"product_sum\",\n",
    "            # Категориальные признаки\n",
    "            \"product_group\",\n",
    "        ]\n",
    "\n",
    "        self.categorical_features = [\"product_group\"]\n",
    "        self.model = None\n",
    "        self.numeric_fill_values_ = {}  # Добавляем инициализацию словаря\n",
    "\n",
    "    def _prepare_features(\n",
    "        self, data: pd.DataFrame, is_train: bool = True\n",
    "    ) -> Tuple[pd.DataFrame, Optional[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Подготовка признаков для обучения или предсказания.\n",
    "\n",
    "        Args:\n",
    "            data: исходный датафрейм\n",
    "            is_train: флаг обучения/предсказания\n",
    "\n",
    "        Returns:\n",
    "            Tuple с подготовленными признаками и целевой переменной (если is_train=True)\n",
    "        \"\"\"\n",
    "        features = data[self.feature_names].copy()\n",
    "\n",
    "        # Заполняем пропуски\n",
    "        for col in features.columns:\n",
    "            if col in self.categorical_features:\n",
    "                features[col] = features[col].fillna(\"unknown\")\n",
    "            else:\n",
    "                # Для числовых признаков используем 0 или медиану\n",
    "                if is_train:\n",
    "                    fill_value = (\n",
    "                        features[col].median() if features[col].notnull().any() else 0\n",
    "                    )\n",
    "                    self.numeric_fill_values_ = self.numeric_fill_values_ or {}\n",
    "                    self.numeric_fill_values_[col] = fill_value\n",
    "                else:\n",
    "                    fill_value = self.numeric_fill_values_.get(col, 0)\n",
    "                features[col] = features[col].fillna(fill_value)\n",
    "\n",
    "        if is_train:\n",
    "            # Для обучения создаем целевую переменную (1 для реальных покупок)\n",
    "            target = np.ones(len(features))\n",
    "            return features, target\n",
    "        else:\n",
    "            return features, None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_data: pd.DataFrame,\n",
    "        candidates: pd.DataFrame,\n",
    "        validation_size: float = 0.2,\n",
    "    ) -> \"CatBoostRanker\":\n",
    "        \"\"\"\n",
    "        Обучение модели ранжирования.\n",
    "\n",
    "        Args:\n",
    "            train_data: исторические данные о покупках\n",
    "            candidates: датафрейм с кандидатами\n",
    "            validation_size: доля данных для валидации\n",
    "\n",
    "        Returns:\n",
    "            self: обученная модель\n",
    "        \"\"\"\n",
    "        # Сортируем данные по buyer_id\n",
    "        train_data = train_data.sort_values(\"buyer_id\")\n",
    "        candidates = candidates.sort_values(\"buyer_id\")\n",
    "\n",
    "        # Подготовка положительных примеров\n",
    "        X_pos, y_pos = self._prepare_features(train_data, is_train=True)\n",
    "        groups_pos = train_data[\"buyer_id\"].values\n",
    "\n",
    "        # Подготовка отрицательных примеров (кандидаты)\n",
    "        features_df = train_data[[\"buyer_id\", \"product_id\"] + self.feature_names].copy()\n",
    "        features_df = features_df.drop_duplicates(subset=[\"buyer_id\", \"product_id\"])\n",
    "\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "        chunk_size = 100000\n",
    "        X_chunks = []\n",
    "        groups_chunks = []\n",
    "        total_negative_samples = 0\n",
    "\n",
    "        for start_idx in tqdm(\n",
    "            range(0, len(candidates), chunk_size),\n",
    "            desc=\"Подготовка отрицательных примеров\",\n",
    "        ):\n",
    "            end_idx = start_idx + chunk_size\n",
    "            chunk = candidates.iloc[start_idx:end_idx]\n",
    "\n",
    "            chunk_with_features = chunk.merge(\n",
    "                features_df, on=[\"buyer_id\", \"product_id\"], how=\"left\"\n",
    "            )\n",
    "\n",
    "            X_chunk, _ = self._prepare_features(chunk_with_features, is_train=True)\n",
    "            groups_chunk = chunk_with_features[\"buyer_id\"].values\n",
    "\n",
    "            # Добавляем подготовленные данные в списки\n",
    "            X_chunks.append(X_chunk)\n",
    "            groups_chunks.append(groups_chunk)\n",
    "            total_negative_samples += len(X_chunk)\n",
    "\n",
    "            del chunk_with_features, X_chunk, groups_chunk\n",
    "            gc.collect()\n",
    "\n",
    "        del features_df\n",
    "        gc.collect()\n",
    "\n",
    "        # Создаем массивы для отрицательных примеров\n",
    "        X_neg = pd.DataFrame(columns=X_pos.columns)\n",
    "        groups_neg = np.empty(total_negative_samples, dtype=groups_pos.dtype)\n",
    "        y_neg = np.zeros(total_negative_samples)\n",
    "\n",
    "        # Заполняем массивы порциями\n",
    "        current_idx = 0\n",
    "        for X_chunk, groups_chunk in zip(X_chunks, groups_chunks):\n",
    "            chunk_size = len(X_chunk)\n",
    "            X_neg = pd.concat([X_neg, X_chunk], ignore_index=True)\n",
    "            groups_neg[current_idx : current_idx + chunk_size] = groups_chunk\n",
    "            current_idx += chunk_size\n",
    "\n",
    "            del X_chunk\n",
    "            gc.collect()\n",
    "\n",
    "        del X_chunks, groups_chunks\n",
    "        gc.collect()\n",
    "\n",
    "        # Объединяем положительные и отрицательные примеры\n",
    "        X = pd.concat([X_pos, X_neg], ignore_index=True)\n",
    "        y = np.concatenate([y_pos, y_neg])\n",
    "        groups = np.concatenate([groups_pos, groups_neg])\n",
    "\n",
    "        del X_pos, X_neg, y_pos, y_neg, groups_pos, groups_neg\n",
    "        gc.collect()\n",
    "\n",
    "        # Сортируем все данные по группам\n",
    "        sort_idx = np.argsort(groups)\n",
    "        X = X.iloc[sort_idx].reset_index(drop=True)\n",
    "        y = y[sort_idx]\n",
    "        groups = groups[sort_idx]\n",
    "\n",
    "        # Разделение на обучающую и валидационную выборки\n",
    "        unique_groups = np.unique(groups)\n",
    "        n_groups = len(unique_groups)\n",
    "        np.random.shuffle(unique_groups)\n",
    "        train_groups = set(unique_groups[: int(n_groups * (1 - validation_size))])\n",
    "\n",
    "        train_mask = np.array([g in train_groups for g in groups])\n",
    "        valid_mask = ~train_mask\n",
    "\n",
    "        train_pool = CBPool(\n",
    "            data=X[train_mask],\n",
    "            label=y[train_mask],\n",
    "            group_id=groups[train_mask],\n",
    "            cat_features=self.categorical_features,\n",
    "        )\n",
    "\n",
    "        valid_pool = CBPool(\n",
    "            data=X[valid_mask],\n",
    "            label=y[valid_mask],\n",
    "            group_id=groups[valid_mask],\n",
    "            cat_features=self.categorical_features,\n",
    "        )\n",
    "\n",
    "        del X, y, groups, train_mask, valid_mask\n",
    "        gc.collect()\n",
    "\n",
    "        self.model = CatBoost(self.model_params)\n",
    "        self.model.fit(train_pool, eval_set=valid_pool, plot=False)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def rank(\n",
    "        self, candidates: pd.DataFrame, train_data: pd.DataFrame, top_n: int = 10\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ранжирование кандидатов.\n",
    "\n",
    "        Args:\n",
    "            candidates: датафрейм с кандидатами\n",
    "            train_data: исторические данные для получения признаков\n",
    "            top_n: количество рекомендаций для каждого пользователя\n",
    "\n",
    "        Returns:\n",
    "            DataFrame с ранжированными рекомендациями\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Модель не обучена. Сначала вызовите метод fit().\")\n",
    "\n",
    "        # Сортируем кандидатов по buyer_id\n",
    "        candidates = candidates.sort_values(\"buyer_id\").reset_index(drop=True)\n",
    "\n",
    "        # Берем только нужные колонки из train_data для уменьшения размера при merge\n",
    "        features_df = train_data[\n",
    "            [\"buyer_id\", \"product_id\"] + self.feature_names\n",
    "        ].drop_duplicates()\n",
    "\n",
    "        # Объединяем кандидатов с историческими данными\n",
    "        candidates_with_features = candidates.merge(\n",
    "            features_df, on=[\"buyer_id\", \"product_id\"], how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Проверяем, что количество строк не изменилось\n",
    "        if len(candidates_with_features) != len(candidates):\n",
    "            # Убираем возможные дубликаты\n",
    "            candidates_with_features = candidates_with_features.drop_duplicates(\n",
    "                subset=[\"buyer_id\", \"product_id\"]\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "        # Подготавливаем признаки\n",
    "        X_pred, _ = self._prepare_features(candidates_with_features, is_train=False)\n",
    "\n",
    "        # Получаем предсказания\n",
    "        scores = self.model.predict(\n",
    "            CBPool(data=X_pred, cat_features=self.categorical_features)\n",
    "        )\n",
    "\n",
    "        # Проверяем размерности\n",
    "        if len(scores) != len(candidates):\n",
    "            raise ValueError(\n",
    "                f\"Количество предсказаний ({len(scores)}) не совпадает с \"\n",
    "                f\"количеством кандидатов ({len(candidates)})\"\n",
    "            )\n",
    "\n",
    "        # Формируем рекомендации\n",
    "        recommendations = candidates.copy()\n",
    "        recommendations[\"score\"] = scores\n",
    "\n",
    "        # Сортируем и отбираем top_n рекомендаций для каждого пользователя\n",
    "        recommendations = recommendations.sort_values(\n",
    "            by=[\"buyer_id\", \"score\"], ascending=[True, False]\n",
    "        )\n",
    "        recommendations = recommendations.groupby(\"buyer_id\").head(top_n)\n",
    "\n",
    "        return recommendations[[\"buyer_id\", \"product_id\", \"score\"]]\n",
    "\n",
    "    def save_model(\n",
    "        self, model_path: str, format: str = \"cbm\", export_parameters: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Сохранение модели и её параметров.\n",
    "\n",
    "        Args:\n",
    "            model_path: путь для сохранения модели\n",
    "            format: формат сохранения ('cbm' или 'json')\n",
    "            export_parameters: сохранять ли дополнительные параметры модели\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Модель не обучена. Сначала вызовите метод fit()\")\n",
    "\n",
    "        # Создаем директорию если её нет\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "        # Сохраняем модель CatBoost\n",
    "        self.model.save_model(model_path, format=format, pool=None)\n",
    "\n",
    "        if export_parameters:\n",
    "            # Сохраняем дополнительные параметры модели\n",
    "            parameters = {\n",
    "                \"model_params\": self.model_params,\n",
    "                \"feature_names\": self.feature_names,\n",
    "                \"categorical_features\": self.categorical_features,\n",
    "                \"numeric_fill_values\": self.numeric_fill_values_,\n",
    "            }\n",
    "\n",
    "            # Путь для сохранения параметров\n",
    "            params_path = f\"{os.path.splitext(model_path)[0]}_params.json\"\n",
    "\n",
    "            with open(params_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(parameters, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(\n",
    "        cls, model_path: str, load_parameters: bool = True\n",
    "    ) -> \"CatBoostRanker\":\n",
    "        \"\"\"\n",
    "        Загрузка сохранённой модели и её параметров.\n",
    "\n",
    "        Args:\n",
    "            model_path: путь к сохранённой модели\n",
    "            load_parameters: загружать ли дополнительные параметры модели\n",
    "\n",
    "        Returns:\n",
    "            CatBoostRanker: загруженная модель\n",
    "        \"\"\"\n",
    "        # Создаем экземпляр класса\n",
    "        ranker = cls()\n",
    "\n",
    "        # Загружаем модель CatBoost\n",
    "        ranker.model = CatBoost()\n",
    "        ranker.model.load_model(model_path)\n",
    "\n",
    "        if load_parameters:\n",
    "            # Путь к файлу с параметрами\n",
    "            params_path = f\"{os.path.splitext(model_path)[0]}_params.json\"\n",
    "\n",
    "            if os.path.exists(params_path):\n",
    "                with open(params_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    parameters = json.load(f)\n",
    "\n",
    "                # Загружаем параметры\n",
    "                ranker.model_params = parameters[\"model_params\"]\n",
    "                ranker.feature_names = parameters[\"feature_names\"]\n",
    "                ranker.categorical_features = parameters[\"categorical_features\"]\n",
    "                ranker.numeric_fill_values_ = parameters[\"numeric_fill_values\"]\n",
    "            else:\n",
    "                logging.warning(\n",
    "                    f\"Файл с параметрами {params_path} не найден. \"\n",
    "                    \"Загружена только модель без дополнительных параметров.\"\n",
    "                )\n",
    "\n",
    "        return ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_catboost_parameters(\n",
    "    train_data: pd.DataFrame,\n",
    "    test_data: pd.DataFrame,\n",
    "    candidates: pd.DataFrame,\n",
    "    metrics_calculator: MetricsCalculator,\n",
    "    n_trials: int = 10,\n",
    "    timeout: int = 7200,\n",
    ") -> Tuple[Dict, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Оптимизация параметров CatBoostRanker с помощью Optuna.\n",
    "\n",
    "    Args:\n",
    "        train_data: тренировочные данные\n",
    "        test_data: тестовые данные\n",
    "        candidates: кандидаты для ранжирования\n",
    "        n_trials: количество итераций оптимизации\n",
    "        timeout: максимальное время оптимизации в секундах\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict, Dict[str, float]]:\n",
    "            - Лучшие параметры\n",
    "            - Значения метрик на лучших параметрах\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial: Trial) -> float:\n",
    "        # Определяем пространство гиперпараметров\n",
    "        model_params = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "            'iterations': 1000,\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "            'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 10),\n",
    "            'early_stopping_rounds': 50,\n",
    "            'task_type': 'CPU',\n",
    "            'thread_count': -1,\n",
    "            'verbose': False,\n",
    "            'eval_metric': 'NDCG',\n",
    "            'loss_function': 'Logloss'\n",
    "        }\n",
    "\n",
    "        # Создаем и обучаем модель\n",
    "        ranker = CatBoostRanker(\n",
    "            learning_rate=model_params['learning_rate'],\n",
    "            iterations=model_params['iterations'],\n",
    "            depth=model_params['depth'],\n",
    "            l2_leaf_reg=model_params['l2_leaf_reg'],\n",
    "            random_seed=42,\n",
    "            thread_count=model_params['thread_count'],\n",
    "            verbose=model_params['verbose']\n",
    "        )\n",
    "\n",
    "        # Обновляем model_params в объекте\n",
    "        ranker.model_params.update(model_params)\n",
    "\n",
    "        # Обучаем модель\n",
    "        ranker.fit(train_data, candidates)\n",
    "\n",
    "        # Получаем предсказания\n",
    "        recommendations = ranker.rank(\n",
    "            candidates=candidates,\n",
    "            train_data=train_data,\n",
    "            top_n=max([10, 100, 1000]),  # для расчета всех метрик\n",
    "        )\n",
    "\n",
    "        # Преобразуем в формат для расчета метрик\n",
    "        recommendations_dict = (\n",
    "            recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    "        )\n",
    "\n",
    "        # Рассчитываем метрики\n",
    "        metrics = metrics_calculator.calculate(\n",
    "            recommendations=recommendations_dict,\n",
    "            train_data=train_data,\n",
    "            test_data=test_data,\n",
    "            item_categories=item_categories,\n",
    "        )\n",
    "\n",
    "        # Возвращаем метрику для оптимизации (NDCG@K)\n",
    "        target_k = max(metrics_calculator.k_values)\n",
    "        return metrics[f\"ndcg_{target_k}\"]\n",
    "\n",
    "    # Создаем исследование\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=f'catboost_ranker_optimization_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    )\n",
    "\n",
    "    # Запускаем оптимизацию\n",
    "    logging.info(\"Начало оптимизации параметров...\")\n",
    "    study.optimize(\n",
    "        objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # Получаем лучшие параметры\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Добавляем фиксированные параметры\n",
    "    best_model_params = {\n",
    "        'learning_rate': best_params['learning_rate'],\n",
    "        'iterations': 1000,\n",
    "        'depth': best_params['depth'],\n",
    "        'l2_leaf_reg': best_params['l2_leaf_reg'],\n",
    "        'random_seed': 42,\n",
    "        'thread_count': -1,\n",
    "        'verbose': True,\n",
    "        'task_type': 'CPU',\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'NDCG',\n",
    "        'early_stopping_rounds': 50,\n",
    "        'random_strength': best_params['random_strength'],\n",
    "        'bagging_temperature': best_params['bagging_temperature'],\n",
    "        'leaf_estimation_iterations': best_params['leaf_estimation_iterations']\n",
    "    }\n",
    "\n",
    "    # Обучаем модель на лучших параметрах\n",
    "    logging.info(\"Обучение модели на лучших параметрах...\")\n",
    "    best_ranker = CatBoostRanker(\n",
    "        learning_rate=best_model_params['learning_rate'],\n",
    "        iterations=best_model_params['iterations'],\n",
    "        depth=best_model_params['depth'],\n",
    "        l2_leaf_reg=best_model_params['l2_leaf_reg'],\n",
    "        random_seed=42,\n",
    "        thread_count=best_model_params['thread_count'],\n",
    "        verbose=best_model_params['verbose']\n",
    "    )\n",
    "    best_ranker.model_params.update(best_model_params)\n",
    "    best_ranker.fit(train_data, candidates)\n",
    "\n",
    "    # Получаем предсказания\n",
    "    recommendations = best_ranker.rank(\n",
    "        candidates=candidates, train_data=train_data, top_n=max([10, 100, 1000])\n",
    "    )\n",
    "\n",
    "    # Рассчитываем финальные метрики\n",
    "    recommendations_dict = (\n",
    "        recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    "    )\n",
    "\n",
    "    final_metrics = metrics_calculator.calculate(\n",
    "        recommendations=recommendations_dict,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        item_categories=item_categories,\n",
    "    )\n",
    "\n",
    "    # Выводим результаты\n",
    "    logging.info(\"\\nЛучшие параметры:\")\n",
    "    for param_name, param_value in best_params.items():\n",
    "        logging.info(f\"{param_name}: {param_value}\")\n",
    "\n",
    "    logging.info(\"\\nМетрики на лучших параметрах:\")\n",
    "    for metric_name, metric_value in final_metrics.items():\n",
    "        logging.info(f\"{metric_name}: {metric_value:.4f}\")\n",
    "\n",
    "    return best_params, final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-21 12:00:34,560] A new study created in memory with name: catboost_ranker_optimization_20250421_120034\n",
      "2025-04-21 12:00:34 - INFO - Начало оптимизации параметров...\n",
      "Подготовка отрицательных примеров: 100%|██████████| 601/601 [05:16<00:00,  1.90it/s]\n",
      "IOStream.flush timed out\n",
      "  0%|          | 0/10 [1:28:41<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-21 13:29:15,605] Trial 0 finished with value: 0.8317919801478025 and parameters: {'learning_rate': 0.004684283500572371, 'depth': 6, 'l2_leaf_reg': 5.36104374072008e-06, 'random_strength': 0.007652648646042903, 'bagging_temperature': 0.9487648394691982, 'leaf_estimation_iterations': 9}. Best is trial 0 with value: 0.8317919801478025.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Подготовка отрицательных примеров: 100%|██████████| 601/601 [04:52<00:00,  2.05it/s]5s/it, 5321.05/7200 seconds]\n",
      "Best trial: 0. Best value: 0.831792:  10%|█         | 1/10 [2:37:40<13:18:09, 5321.05s/it, 5321.05/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-21 14:38:15,522] Trial 1 finished with value: 0.8317919801478025 and parameters: {'learning_rate': 0.02388179353529231, 'depth': 5, 'l2_leaf_reg': 4.8339398459122034e-08, 'random_strength': 0.0031822125482293437, 'bagging_temperature': 0.5517318159431245, 'leaf_estimation_iterations': 2}. Best is trial 0 with value: 0.8317919801478025.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.831792:  20%|██        | 2/10 [2:37:40<10:30:43, 4730.48s/it, 9460.96/7200 seconds]\n",
      "2025-04-21 14:38:15 - INFO - Обучение модели на лучших параметрах...\n",
      "Подготовка отрицательных примеров: 100%|██████████| 601/601 [04:13<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 6.48s\tremaining: 1h 47m 55s\n",
      "1:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 12.9s\tremaining: 1h 47m 21s\n",
      "2:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 19s\tremaining: 1h 45m 24s\n",
      "3:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 22.2s\tremaining: 1h 32m 9s\n",
      "4:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 25.6s\tremaining: 1h 24m 45s\n",
      "5:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 28.8s\tremaining: 1h 19m 32s\n",
      "6:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 32.1s\tremaining: 1h 15m 53s\n",
      "7:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 35.3s\tremaining: 1h 12m 59s\n",
      "8:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 38.6s\tremaining: 1h 10m 54s\n",
      "9:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 41.9s\tremaining: 1h 9m 12s\n",
      "10:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 45.2s\tremaining: 1h 7m 42s\n",
      "11:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 48.6s\tremaining: 1h 6m 37s\n",
      "12:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 51.8s\tremaining: 1h 5m 34s\n",
      "13:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 55.1s\tremaining: 1h 4m 42s\n",
      "14:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 58.3s\tremaining: 1h 3m 50s\n",
      "15:\ttest: 0.7779675\tbest: 0.7779675 (15)\ttotal: 1m 1s\tremaining: 1h 3m 13s\n",
      "16:\ttest: 0.7779675\tbest: 0.7779675 (15)\ttotal: 1m 4s\tremaining: 1h 2m 35s\n",
      "17:\ttest: 0.7779675\tbest: 0.7779675 (15)\ttotal: 1m 8s\tremaining: 1h 2m 3s\n",
      "18:\ttest: 0.7779353\tbest: 0.7779675 (15)\ttotal: 1m 11s\tremaining: 1h 1m 39s\n",
      "19:\ttest: 0.7779353\tbest: 0.7779675 (15)\ttotal: 1m 14s\tremaining: 1h 1m 7s\n",
      "20:\ttest: 0.7819910\tbest: 0.7819910 (20)\ttotal: 1m 18s\tremaining: 1h 40s\n",
      "21:\ttest: 0.7819920\tbest: 0.7819920 (21)\ttotal: 1m 21s\tremaining: 1h 16s\n",
      "22:\ttest: 0.7819910\tbest: 0.7819920 (21)\ttotal: 1m 24s\tremaining: 59m 56s\n",
      "23:\ttest: 0.7819897\tbest: 0.7819920 (21)\ttotal: 1m 28s\tremaining: 1h 17s\n",
      "24:\ttest: 0.7825440\tbest: 0.7825440 (24)\ttotal: 1m 32s\tremaining: 59m 59s\n",
      "25:\ttest: 0.7825850\tbest: 0.7825850 (25)\ttotal: 1m 36s\tremaining: 1h 22s\n",
      "26:\ttest: 0.7825926\tbest: 0.7825926 (26)\ttotal: 1m 39s\tremaining: 1h 3s\n",
      "27:\ttest: 0.7824892\tbest: 0.7825926 (26)\ttotal: 1m 43s\tremaining: 59m 47s\n",
      "28:\ttest: 0.7825577\tbest: 0.7825926 (26)\ttotal: 1m 47s\tremaining: 1h 4s\n",
      "29:\ttest: 0.7825435\tbest: 0.7825926 (26)\ttotal: 1m 51s\tremaining: 59m 49s\n",
      "30:\ttest: 0.7825916\tbest: 0.7825926 (26)\ttotal: 1m 54s\tremaining: 59m 29s\n",
      "31:\ttest: 0.7826242\tbest: 0.7826242 (31)\ttotal: 1m 57s\tremaining: 59m 10s\n",
      "32:\ttest: 0.7826181\tbest: 0.7826242 (31)\ttotal: 2m\tremaining: 58m 54s\n",
      "33:\ttest: 0.7825956\tbest: 0.7826242 (31)\ttotal: 2m 3s\tremaining: 58m 38s\n",
      "34:\ttest: 0.7826103\tbest: 0.7826242 (31)\ttotal: 2m 7s\tremaining: 58m 23s\n",
      "35:\ttest: 0.7826058\tbest: 0.7826242 (31)\ttotal: 2m 10s\tremaining: 58m 10s\n",
      "36:\ttest: 0.7825954\tbest: 0.7826242 (31)\ttotal: 2m 13s\tremaining: 57m 57s\n",
      "37:\ttest: 0.7826414\tbest: 0.7826414 (37)\ttotal: 2m 16s\tremaining: 57m 45s\n",
      "38:\ttest: 0.7826590\tbest: 0.7826590 (38)\ttotal: 2m 20s\tremaining: 57m 33s\n",
      "39:\ttest: 0.7826511\tbest: 0.7826590 (38)\ttotal: 2m 23s\tremaining: 57m 25s\n",
      "40:\ttest: 0.7826488\tbest: 0.7826590 (38)\ttotal: 2m 26s\tremaining: 57m 15s\n",
      "41:\ttest: 0.7825666\tbest: 0.7826590 (38)\ttotal: 2m 31s\tremaining: 57m 26s\n",
      "42:\ttest: 0.7825827\tbest: 0.7826590 (38)\ttotal: 2m 34s\tremaining: 57m 17s\n",
      "43:\ttest: 0.7825767\tbest: 0.7826590 (38)\ttotal: 2m 37s\tremaining: 57m 7s\n",
      "44:\ttest: 0.7823806\tbest: 0.7826590 (38)\ttotal: 2m 41s\tremaining: 56m 58s\n",
      "45:\ttest: 0.7823715\tbest: 0.7826590 (38)\ttotal: 2m 44s\tremaining: 56m 50s\n",
      "46:\ttest: 0.7823887\tbest: 0.7826590 (38)\ttotal: 2m 47s\tremaining: 56m 41s\n",
      "47:\ttest: 0.7823688\tbest: 0.7826590 (38)\ttotal: 2m 51s\tremaining: 56m 32s\n",
      "48:\ttest: 0.7823496\tbest: 0.7826590 (38)\ttotal: 2m 54s\tremaining: 56m 23s\n",
      "49:\ttest: 0.7823128\tbest: 0.7826590 (38)\ttotal: 2m 57s\tremaining: 56m 15s\n",
      "50:\ttest: 0.7823189\tbest: 0.7826590 (38)\ttotal: 3m\tremaining: 56m 6s\n",
      "51:\ttest: 0.7823808\tbest: 0.7826590 (38)\ttotal: 3m 4s\tremaining: 55m 57s\n",
      "52:\ttest: 0.7823446\tbest: 0.7826590 (38)\ttotal: 3m 7s\tremaining: 55m 47s\n",
      "53:\ttest: 0.7823438\tbest: 0.7826590 (38)\ttotal: 3m 11s\tremaining: 55m 55s\n",
      "54:\ttest: 0.7823438\tbest: 0.7826590 (38)\ttotal: 3m 14s\tremaining: 55m 45s\n",
      "55:\ttest: 0.7823446\tbest: 0.7826590 (38)\ttotal: 3m 17s\tremaining: 55m 37s\n",
      "56:\ttest: 0.7823643\tbest: 0.7826590 (38)\ttotal: 3m 21s\tremaining: 55m 27s\n",
      "57:\ttest: 0.7823656\tbest: 0.7826590 (38)\ttotal: 3m 24s\tremaining: 55m 18s\n",
      "58:\ttest: 0.7823657\tbest: 0.7826590 (38)\ttotal: 3m 27s\tremaining: 55m 11s\n",
      "59:\ttest: 0.7823933\tbest: 0.7826590 (38)\ttotal: 3m 30s\tremaining: 55m 5s\n",
      "60:\ttest: 0.7823933\tbest: 0.7826590 (38)\ttotal: 3m 34s\tremaining: 54m 55s\n",
      "61:\ttest: 0.7823933\tbest: 0.7826590 (38)\ttotal: 3m 37s\tremaining: 54m 46s\n",
      "62:\ttest: 0.7823994\tbest: 0.7826590 (38)\ttotal: 3m 41s\tremaining: 54m 55s\n",
      "63:\ttest: 0.7823994\tbest: 0.7826590 (38)\ttotal: 3m 44s\tremaining: 54m 46s\n",
      "64:\ttest: 0.7824081\tbest: 0.7826590 (38)\ttotal: 3m 47s\tremaining: 54m 37s\n",
      "65:\ttest: 0.7825094\tbest: 0.7826590 (38)\ttotal: 3m 52s\tremaining: 54m 44s\n",
      "66:\ttest: 0.7825360\tbest: 0.7826590 (38)\ttotal: 3m 55s\tremaining: 54m 35s\n",
      "67:\ttest: 0.7825851\tbest: 0.7826590 (38)\ttotal: 3m 58s\tremaining: 54m 26s\n",
      "68:\ttest: 0.7826132\tbest: 0.7826590 (38)\ttotal: 4m 2s\tremaining: 54m 32s\n",
      "69:\ttest: 0.7827076\tbest: 0.7827076 (69)\ttotal: 4m 5s\tremaining: 54m 25s\n",
      "70:\ttest: 0.7826964\tbest: 0.7827076 (69)\ttotal: 4m 8s\tremaining: 54m 17s\n",
      "71:\ttest: 0.7826870\tbest: 0.7827076 (69)\ttotal: 4m 12s\tremaining: 54m 17s\n",
      "72:\ttest: 0.7827072\tbest: 0.7827076 (69)\ttotal: 4m 15s\tremaining: 54m 9s\n",
      "73:\ttest: 0.7826728\tbest: 0.7827076 (69)\ttotal: 4m 18s\tremaining: 53m 59s\n",
      "74:\ttest: 0.7826974\tbest: 0.7827076 (69)\ttotal: 4m 22s\tremaining: 53m 51s\n",
      "75:\ttest: 0.7827105\tbest: 0.7827105 (75)\ttotal: 4m 25s\tremaining: 53m 51s\n",
      "76:\ttest: 0.7827363\tbest: 0.7827363 (76)\ttotal: 4m 28s\tremaining: 53m 43s\n",
      "77:\ttest: 0.7827269\tbest: 0.7827363 (76)\ttotal: 4m 32s\tremaining: 53m 46s\n",
      "78:\ttest: 0.7826481\tbest: 0.7827363 (76)\ttotal: 4m 36s\tremaining: 53m 40s\n",
      "79:\ttest: 0.7826365\tbest: 0.7827363 (76)\ttotal: 4m 39s\tremaining: 53m 34s\n",
      "80:\ttest: 0.7826096\tbest: 0.7827363 (76)\ttotal: 4m 42s\tremaining: 53m 27s\n",
      "81:\ttest: 0.7826312\tbest: 0.7827363 (76)\ttotal: 4m 45s\tremaining: 53m 20s\n",
      "82:\ttest: 0.7827053\tbest: 0.7827363 (76)\ttotal: 4m 49s\tremaining: 53m 14s\n",
      "83:\ttest: 0.7827009\tbest: 0.7827363 (76)\ttotal: 4m 52s\tremaining: 53m 6s\n",
      "84:\ttest: 0.7826987\tbest: 0.7827363 (76)\ttotal: 4m 55s\tremaining: 52m 59s\n",
      "85:\ttest: 0.7826990\tbest: 0.7827363 (76)\ttotal: 4m 58s\tremaining: 52m 53s\n",
      "86:\ttest: 0.7827182\tbest: 0.7827363 (76)\ttotal: 5m 1s\tremaining: 52m 47s\n",
      "87:\ttest: 0.7827181\tbest: 0.7827363 (76)\ttotal: 5m 4s\tremaining: 52m 40s\n",
      "88:\ttest: 0.7827461\tbest: 0.7827461 (88)\ttotal: 5m 8s\tremaining: 52m 32s\n",
      "89:\ttest: 0.7827453\tbest: 0.7827461 (88)\ttotal: 5m 11s\tremaining: 52m 27s\n",
      "90:\ttest: 0.7827324\tbest: 0.7827461 (88)\ttotal: 5m 14s\tremaining: 52m 19s\n",
      "91:\ttest: 0.7827455\tbest: 0.7827461 (88)\ttotal: 5m 17s\tremaining: 52m 14s\n",
      "92:\ttest: 0.7827167\tbest: 0.7827461 (88)\ttotal: 5m 20s\tremaining: 52m 8s\n",
      "93:\ttest: 0.7828773\tbest: 0.7828773 (93)\ttotal: 5m 23s\tremaining: 52m 1s\n",
      "94:\ttest: 0.7829434\tbest: 0.7829434 (94)\ttotal: 5m 27s\tremaining: 51m 55s\n",
      "95:\ttest: 0.7830301\tbest: 0.7830301 (95)\ttotal: 5m 30s\tremaining: 51m 47s\n",
      "96:\ttest: 0.7829966\tbest: 0.7830301 (95)\ttotal: 5m 33s\tremaining: 51m 40s\n",
      "97:\ttest: 0.7829966\tbest: 0.7830301 (95)\ttotal: 5m 36s\tremaining: 51m 33s\n",
      "98:\ttest: 0.7830186\tbest: 0.7830301 (95)\ttotal: 5m 39s\tremaining: 51m 26s\n",
      "99:\ttest: 0.7830111\tbest: 0.7830301 (95)\ttotal: 5m 42s\tremaining: 51m 20s\n",
      "100:\ttest: 0.7830000\tbest: 0.7830301 (95)\ttotal: 5m 45s\tremaining: 51m 14s\n",
      "101:\ttest: 0.7829988\tbest: 0.7830301 (95)\ttotal: 5m 48s\tremaining: 51m 7s\n",
      "102:\ttest: 0.7829988\tbest: 0.7830301 (95)\ttotal: 5m 51s\tremaining: 51m 1s\n",
      "103:\ttest: 0.7830110\tbest: 0.7830301 (95)\ttotal: 5m 54s\tremaining: 50m 54s\n",
      "104:\ttest: 0.7830077\tbest: 0.7830301 (95)\ttotal: 5m 57s\tremaining: 50m 48s\n",
      "105:\ttest: 0.7830393\tbest: 0.7830393 (105)\ttotal: 6m\tremaining: 50m 42s\n",
      "106:\ttest: 0.7830409\tbest: 0.7830409 (106)\ttotal: 6m 4s\tremaining: 50m 40s\n",
      "107:\ttest: 0.7830514\tbest: 0.7830514 (107)\ttotal: 6m 8s\tremaining: 50m 39s\n",
      "108:\ttest: 0.7830980\tbest: 0.7830980 (108)\ttotal: 6m 11s\tremaining: 50m 33s\n",
      "109:\ttest: 0.7830782\tbest: 0.7830980 (108)\ttotal: 6m 14s\tremaining: 50m 26s\n",
      "110:\ttest: 0.7830887\tbest: 0.7830980 (108)\ttotal: 6m 17s\tremaining: 50m 24s\n",
      "111:\ttest: 0.7830415\tbest: 0.7830980 (108)\ttotal: 6m 20s\tremaining: 50m 18s\n",
      "112:\ttest: 0.7830948\tbest: 0.7830980 (108)\ttotal: 6m 23s\tremaining: 50m 12s\n",
      "113:\ttest: 0.7831506\tbest: 0.7831506 (113)\ttotal: 6m 26s\tremaining: 50m 6s\n",
      "114:\ttest: 0.7831525\tbest: 0.7831525 (114)\ttotal: 6m 29s\tremaining: 50m\n",
      "115:\ttest: 0.7831531\tbest: 0.7831531 (115)\ttotal: 6m 33s\tremaining: 49m 55s\n",
      "116:\ttest: 0.7831610\tbest: 0.7831610 (116)\ttotal: 6m 36s\tremaining: 49m 50s\n",
      "117:\ttest: 0.7831709\tbest: 0.7831709 (117)\ttotal: 6m 39s\tremaining: 49m 44s\n",
      "118:\ttest: 0.7831777\tbest: 0.7831777 (118)\ttotal: 6m 42s\tremaining: 49m 38s\n",
      "119:\ttest: 0.7831735\tbest: 0.7831777 (118)\ttotal: 6m 45s\tremaining: 49m 36s\n",
      "120:\ttest: 0.7833201\tbest: 0.7833201 (120)\ttotal: 6m 48s\tremaining: 49m 29s\n",
      "121:\ttest: 0.7832540\tbest: 0.7833201 (120)\ttotal: 6m 51s\tremaining: 49m 24s\n",
      "122:\ttest: 0.7833260\tbest: 0.7833260 (122)\ttotal: 6m 55s\tremaining: 49m 19s\n",
      "123:\ttest: 0.7833241\tbest: 0.7833260 (122)\ttotal: 6m 58s\tremaining: 49m 14s\n",
      "124:\ttest: 0.7832874\tbest: 0.7833260 (122)\ttotal: 7m 1s\tremaining: 49m 12s\n",
      "125:\ttest: 0.7832881\tbest: 0.7833260 (122)\ttotal: 7m 6s\tremaining: 49m 18s\n",
      "126:\ttest: 0.7951483\tbest: 0.7951483 (126)\ttotal: 7m 11s\tremaining: 49m 25s\n",
      "127:\ttest: 0.8053362\tbest: 0.8053362 (127)\ttotal: 7m 14s\tremaining: 49m 20s\n",
      "128:\ttest: 0.8060745\tbest: 0.8060745 (128)\ttotal: 7m 17s\tremaining: 49m 13s\n",
      "129:\ttest: 0.8067180\tbest: 0.8067180 (129)\ttotal: 7m 20s\tremaining: 49m 8s\n",
      "130:\ttest: 0.8069621\tbest: 0.8069621 (130)\ttotal: 7m 23s\tremaining: 49m 3s\n",
      "131:\ttest: 0.8070977\tbest: 0.8070977 (131)\ttotal: 7m 26s\tremaining: 48m 58s\n",
      "132:\ttest: 0.8073467\tbest: 0.8073467 (132)\ttotal: 7m 29s\tremaining: 48m 52s\n",
      "133:\ttest: 0.8074564\tbest: 0.8074564 (133)\ttotal: 7m 33s\tremaining: 48m 47s\n",
      "134:\ttest: 0.8075805\tbest: 0.8075805 (134)\ttotal: 7m 36s\tremaining: 48m 43s\n",
      "135:\ttest: 0.8077673\tbest: 0.8077673 (135)\ttotal: 7m 39s\tremaining: 48m 38s\n",
      "136:\ttest: 0.8078937\tbest: 0.8078937 (136)\ttotal: 7m 42s\tremaining: 48m 33s\n",
      "137:\ttest: 0.8081131\tbest: 0.8081131 (137)\ttotal: 7m 45s\tremaining: 48m 27s\n",
      "138:\ttest: 0.8083270\tbest: 0.8083270 (138)\ttotal: 7m 48s\tremaining: 48m 21s\n",
      "139:\ttest: 0.8084985\tbest: 0.8084985 (139)\ttotal: 7m 51s\tremaining: 48m 15s\n",
      "140:\ttest: 0.8087326\tbest: 0.8087326 (140)\ttotal: 7m 54s\tremaining: 48m 9s\n",
      "141:\ttest: 0.8090104\tbest: 0.8090104 (141)\ttotal: 7m 57s\tremaining: 48m 3s\n",
      "142:\ttest: 0.8094966\tbest: 0.8094966 (142)\ttotal: 8m\tremaining: 47m 57s\n",
      "143:\ttest: 0.8098249\tbest: 0.8098249 (143)\ttotal: 8m 3s\tremaining: 47m 52s\n",
      "144:\ttest: 0.8100758\tbest: 0.8100758 (144)\ttotal: 8m 6s\tremaining: 47m 48s\n",
      "145:\ttest: 0.8102108\tbest: 0.8102108 (145)\ttotal: 8m 9s\tremaining: 47m 43s\n",
      "146:\ttest: 0.8105237\tbest: 0.8105237 (146)\ttotal: 8m 12s\tremaining: 47m 39s\n",
      "147:\ttest: 0.8108824\tbest: 0.8108824 (147)\ttotal: 8m 15s\tremaining: 47m 34s\n",
      "148:\ttest: 0.8114435\tbest: 0.8114435 (148)\ttotal: 8m 19s\tremaining: 47m 30s\n",
      "149:\ttest: 0.8116759\tbest: 0.8116759 (149)\ttotal: 8m 21s\tremaining: 47m 24s\n",
      "150:\ttest: 0.8120468\tbest: 0.8120468 (150)\ttotal: 8m 25s\tremaining: 47m 20s\n",
      "151:\ttest: 0.8123611\tbest: 0.8123611 (151)\ttotal: 8m 28s\tremaining: 47m 16s\n",
      "152:\ttest: 0.8127372\tbest: 0.8127372 (152)\ttotal: 8m 31s\tremaining: 47m 12s\n",
      "153:\ttest: 0.8133548\tbest: 0.8133548 (153)\ttotal: 8m 34s\tremaining: 47m 8s\n",
      "154:\ttest: 0.8137173\tbest: 0.8137173 (154)\ttotal: 8m 37s\tremaining: 47m 2s\n",
      "155:\ttest: 0.8139056\tbest: 0.8139056 (155)\ttotal: 8m 40s\tremaining: 46m 58s\n",
      "156:\ttest: 0.8140177\tbest: 0.8140177 (156)\ttotal: 8m 44s\tremaining: 46m 53s\n",
      "157:\ttest: 0.8140724\tbest: 0.8140724 (157)\ttotal: 8m 46s\tremaining: 46m 47s\n",
      "158:\ttest: 0.8142113\tbest: 0.8142113 (158)\ttotal: 8m 50s\tremaining: 46m 43s\n",
      "159:\ttest: 0.8142724\tbest: 0.8142724 (159)\ttotal: 8m 52s\tremaining: 46m 36s\n",
      "160:\ttest: 0.8143983\tbest: 0.8143983 (160)\ttotal: 8m 55s\tremaining: 46m 32s\n",
      "161:\ttest: 0.8154798\tbest: 0.8154798 (161)\ttotal: 8m 58s\tremaining: 46m 28s\n",
      "162:\ttest: 0.8156186\tbest: 0.8156186 (162)\ttotal: 9m 2s\tremaining: 46m 23s\n",
      "163:\ttest: 0.8157127\tbest: 0.8157127 (163)\ttotal: 9m 5s\tremaining: 46m 18s\n",
      "164:\ttest: 0.8159110\tbest: 0.8159110 (164)\ttotal: 9m 7s\tremaining: 46m 12s\n",
      "165:\ttest: 0.8159340\tbest: 0.8159340 (165)\ttotal: 9m 11s\tremaining: 46m 8s\n",
      "166:\ttest: 0.8160455\tbest: 0.8160455 (166)\ttotal: 9m 14s\tremaining: 46m 4s\n",
      "167:\ttest: 0.8161053\tbest: 0.8161053 (167)\ttotal: 9m 17s\tremaining: 45m 59s\n",
      "168:\ttest: 0.8162427\tbest: 0.8162427 (168)\ttotal: 9m 20s\tremaining: 45m 55s\n",
      "169:\ttest: 0.8163462\tbest: 0.8163462 (169)\ttotal: 9m 23s\tremaining: 45m 51s\n",
      "170:\ttest: 0.8164525\tbest: 0.8164525 (170)\ttotal: 9m 26s\tremaining: 45m 44s\n",
      "171:\ttest: 0.8165453\tbest: 0.8165453 (171)\ttotal: 9m 29s\tremaining: 45m 41s\n",
      "172:\ttest: 0.8166499\tbest: 0.8166499 (172)\ttotal: 9m 32s\tremaining: 45m 36s\n",
      "173:\ttest: 0.8167769\tbest: 0.8167769 (173)\ttotal: 9m 35s\tremaining: 45m 32s\n",
      "174:\ttest: 0.8168449\tbest: 0.8168449 (174)\ttotal: 9m 38s\tremaining: 45m 26s\n",
      "175:\ttest: 0.8169987\tbest: 0.8169987 (175)\ttotal: 9m 41s\tremaining: 45m 21s\n",
      "176:\ttest: 0.8171020\tbest: 0.8171020 (176)\ttotal: 9m 44s\tremaining: 45m 18s\n",
      "177:\ttest: 0.8172481\tbest: 0.8172481 (177)\ttotal: 9m 47s\tremaining: 45m 14s\n",
      "178:\ttest: 0.8174576\tbest: 0.8174576 (178)\ttotal: 9m 50s\tremaining: 45m 10s\n",
      "179:\ttest: 0.8175576\tbest: 0.8175576 (179)\ttotal: 9m 54s\tremaining: 45m 6s\n",
      "180:\ttest: 0.8176320\tbest: 0.8176320 (180)\ttotal: 9m 56s\tremaining: 44m 59s\n",
      "181:\ttest: 0.8177407\tbest: 0.8177407 (181)\ttotal: 9m 59s\tremaining: 44m 55s\n",
      "182:\ttest: 0.8178204\tbest: 0.8178204 (182)\ttotal: 10m 2s\tremaining: 44m 51s\n",
      "183:\ttest: 0.8178397\tbest: 0.8178397 (183)\ttotal: 10m 6s\tremaining: 44m 47s\n",
      "184:\ttest: 0.8179257\tbest: 0.8179257 (184)\ttotal: 10m 9s\tremaining: 44m 44s\n",
      "185:\ttest: 0.8180383\tbest: 0.8180383 (185)\ttotal: 10m 12s\tremaining: 44m 38s\n",
      "186:\ttest: 0.8180437\tbest: 0.8180437 (186)\ttotal: 10m 15s\tremaining: 44m 34s\n",
      "187:\ttest: 0.8181292\tbest: 0.8181292 (187)\ttotal: 10m 18s\tremaining: 44m 30s\n",
      "188:\ttest: 0.8181941\tbest: 0.8181941 (188)\ttotal: 10m 21s\tremaining: 44m 28s\n",
      "189:\ttest: 0.8182748\tbest: 0.8182748 (189)\ttotal: 10m 24s\tremaining: 44m 22s\n",
      "190:\ttest: 0.8182869\tbest: 0.8182869 (190)\ttotal: 10m 27s\tremaining: 44m 17s\n",
      "191:\ttest: 0.8183445\tbest: 0.8183445 (191)\ttotal: 10m 30s\tremaining: 44m 13s\n",
      "192:\ttest: 0.8184491\tbest: 0.8184491 (192)\ttotal: 10m 33s\tremaining: 44m 8s\n",
      "193:\ttest: 0.8184630\tbest: 0.8184630 (193)\ttotal: 10m 36s\tremaining: 44m 4s\n",
      "194:\ttest: 0.8184852\tbest: 0.8184852 (194)\ttotal: 10m 39s\tremaining: 44m\n",
      "195:\ttest: 0.8185108\tbest: 0.8185108 (195)\ttotal: 10m 42s\tremaining: 43m 55s\n",
      "196:\ttest: 0.8186484\tbest: 0.8186484 (196)\ttotal: 10m 45s\tremaining: 43m 51s\n",
      "197:\ttest: 0.8187010\tbest: 0.8187010 (197)\ttotal: 10m 48s\tremaining: 43m 47s\n",
      "198:\ttest: 0.8187377\tbest: 0.8187377 (198)\ttotal: 10m 51s\tremaining: 43m 42s\n",
      "199:\ttest: 0.8188906\tbest: 0.8188906 (199)\ttotal: 10m 54s\tremaining: 43m 38s\n",
      "200:\ttest: 0.8189023\tbest: 0.8189023 (200)\ttotal: 10m 57s\tremaining: 43m 34s\n",
      "201:\ttest: 0.8189596\tbest: 0.8189596 (201)\ttotal: 11m\tremaining: 43m 30s\n",
      "202:\ttest: 0.8190209\tbest: 0.8190209 (202)\ttotal: 11m 3s\tremaining: 43m 25s\n",
      "203:\ttest: 0.8191111\tbest: 0.8191111 (203)\ttotal: 11m 6s\tremaining: 43m 21s\n",
      "204:\ttest: 0.8191349\tbest: 0.8191349 (204)\ttotal: 11m 9s\tremaining: 43m 17s\n",
      "205:\ttest: 0.8191752\tbest: 0.8191752 (205)\ttotal: 11m 12s\tremaining: 43m 10s\n",
      "206:\ttest: 0.8191691\tbest: 0.8191752 (205)\ttotal: 11m 15s\tremaining: 43m 6s\n",
      "207:\ttest: 0.8192374\tbest: 0.8192374 (207)\ttotal: 11m 18s\tremaining: 43m 2s\n",
      "208:\ttest: 0.8192887\tbest: 0.8192887 (208)\ttotal: 11m 21s\tremaining: 42m 58s\n",
      "209:\ttest: 0.8193242\tbest: 0.8193242 (209)\ttotal: 11m 24s\tremaining: 42m 54s\n",
      "210:\ttest: 0.8193985\tbest: 0.8193985 (210)\ttotal: 11m 27s\tremaining: 42m 50s\n",
      "211:\ttest: 0.8194247\tbest: 0.8194247 (211)\ttotal: 11m 30s\tremaining: 42m 45s\n",
      "212:\ttest: 0.8195004\tbest: 0.8195004 (212)\ttotal: 11m 33s\tremaining: 42m 42s\n",
      "213:\ttest: 0.8195560\tbest: 0.8195560 (213)\ttotal: 11m 36s\tremaining: 42m 36s\n",
      "214:\ttest: 0.8195703\tbest: 0.8195703 (214)\ttotal: 11m 38s\tremaining: 42m 31s\n",
      "215:\ttest: 0.8195939\tbest: 0.8195939 (215)\ttotal: 11m 41s\tremaining: 42m 26s\n",
      "216:\ttest: 0.8196091\tbest: 0.8196091 (216)\ttotal: 11m 44s\tremaining: 42m 22s\n",
      "217:\ttest: 0.8195949\tbest: 0.8196091 (216)\ttotal: 11m 47s\tremaining: 42m 18s\n",
      "218:\ttest: 0.8195859\tbest: 0.8196091 (216)\ttotal: 11m 50s\tremaining: 42m 14s\n",
      "219:\ttest: 0.8196122\tbest: 0.8196122 (219)\ttotal: 11m 53s\tremaining: 42m 10s\n",
      "220:\ttest: 0.8196201\tbest: 0.8196201 (220)\ttotal: 11m 56s\tremaining: 42m 6s\n",
      "221:\ttest: 0.8196461\tbest: 0.8196461 (221)\ttotal: 11m 59s\tremaining: 42m\n",
      "222:\ttest: 0.8196497\tbest: 0.8196497 (222)\ttotal: 12m 2s\tremaining: 41m 56s\n",
      "223:\ttest: 0.8196398\tbest: 0.8196497 (222)\ttotal: 12m 5s\tremaining: 41m 52s\n",
      "224:\ttest: 0.8196947\tbest: 0.8196947 (224)\ttotal: 12m 7s\tremaining: 41m 46s\n",
      "225:\ttest: 0.8196903\tbest: 0.8196947 (224)\ttotal: 12m 10s\tremaining: 41m 41s\n",
      "226:\ttest: 0.8197330\tbest: 0.8197330 (226)\ttotal: 12m 13s\tremaining: 41m 37s\n",
      "227:\ttest: 0.8197310\tbest: 0.8197330 (226)\ttotal: 12m 16s\tremaining: 41m 33s\n",
      "228:\ttest: 0.8197031\tbest: 0.8197330 (226)\ttotal: 12m 19s\tremaining: 41m 29s\n",
      "229:\ttest: 0.8197258\tbest: 0.8197330 (226)\ttotal: 12m 22s\tremaining: 41m 25s\n",
      "230:\ttest: 0.8197255\tbest: 0.8197330 (226)\ttotal: 12m 25s\tremaining: 41m 21s\n",
      "231:\ttest: 0.8197503\tbest: 0.8197503 (231)\ttotal: 12m 28s\tremaining: 41m 16s\n",
      "232:\ttest: 0.8197462\tbest: 0.8197503 (231)\ttotal: 12m 30s\tremaining: 41m 12s\n",
      "233:\ttest: 0.8197899\tbest: 0.8197899 (233)\ttotal: 12m 33s\tremaining: 41m 6s\n",
      "234:\ttest: 0.8197926\tbest: 0.8197926 (234)\ttotal: 12m 36s\tremaining: 41m 2s\n",
      "235:\ttest: 0.8198308\tbest: 0.8198308 (235)\ttotal: 12m 39s\tremaining: 40m 57s\n",
      "236:\ttest: 0.8198741\tbest: 0.8198741 (236)\ttotal: 12m 42s\tremaining: 40m 53s\n",
      "237:\ttest: 0.8199164\tbest: 0.8199164 (237)\ttotal: 12m 44s\tremaining: 40m 48s\n",
      "238:\ttest: 0.8199510\tbest: 0.8199510 (238)\ttotal: 12m 47s\tremaining: 40m 45s\n",
      "239:\ttest: 0.8199720\tbest: 0.8199720 (239)\ttotal: 12m 50s\tremaining: 40m 41s\n",
      "240:\ttest: 0.8199847\tbest: 0.8199847 (240)\ttotal: 12m 53s\tremaining: 40m 36s\n",
      "241:\ttest: 0.8200007\tbest: 0.8200007 (241)\ttotal: 12m 56s\tremaining: 40m 32s\n",
      "242:\ttest: 0.8199932\tbest: 0.8200007 (241)\ttotal: 12m 59s\tremaining: 40m 27s\n",
      "243:\ttest: 0.8200788\tbest: 0.8200788 (243)\ttotal: 13m 2s\tremaining: 40m 24s\n",
      "244:\ttest: 0.8201487\tbest: 0.8201487 (244)\ttotal: 13m 5s\tremaining: 40m 20s\n",
      "245:\ttest: 0.8201623\tbest: 0.8201623 (245)\ttotal: 13m 8s\tremaining: 40m 15s\n",
      "246:\ttest: 0.8201224\tbest: 0.8201623 (245)\ttotal: 13m 10s\tremaining: 40m 11s\n",
      "247:\ttest: 0.8201393\tbest: 0.8201623 (245)\ttotal: 13m 13s\tremaining: 40m 6s\n",
      "248:\ttest: 0.8201597\tbest: 0.8201623 (245)\ttotal: 13m 16s\tremaining: 40m 2s\n",
      "249:\ttest: 0.8201729\tbest: 0.8201729 (249)\ttotal: 13m 19s\tremaining: 39m 57s\n",
      "250:\ttest: 0.8201559\tbest: 0.8201729 (249)\ttotal: 13m 21s\tremaining: 39m 52s\n",
      "251:\ttest: 0.8201815\tbest: 0.8201815 (251)\ttotal: 13m 24s\tremaining: 39m 48s\n",
      "252:\ttest: 0.8201621\tbest: 0.8201815 (251)\ttotal: 13m 27s\tremaining: 39m 44s\n",
      "253:\ttest: 0.8201841\tbest: 0.8201841 (253)\ttotal: 13m 30s\tremaining: 39m 39s\n",
      "254:\ttest: 0.8202119\tbest: 0.8202119 (254)\ttotal: 13m 32s\tremaining: 39m 34s\n",
      "255:\ttest: 0.8201999\tbest: 0.8202119 (254)\ttotal: 13m 35s\tremaining: 39m 30s\n",
      "256:\ttest: 0.8201959\tbest: 0.8202119 (254)\ttotal: 13m 38s\tremaining: 39m 26s\n",
      "257:\ttest: 0.8202079\tbest: 0.8202119 (254)\ttotal: 13m 41s\tremaining: 39m 22s\n",
      "258:\ttest: 0.8202347\tbest: 0.8202347 (258)\ttotal: 13m 44s\tremaining: 39m 18s\n",
      "259:\ttest: 0.8203119\tbest: 0.8203119 (259)\ttotal: 13m 47s\tremaining: 39m 14s\n",
      "260:\ttest: 0.8203297\tbest: 0.8203297 (260)\ttotal: 13m 50s\tremaining: 39m 10s\n",
      "261:\ttest: 0.8203411\tbest: 0.8203411 (261)\ttotal: 13m 52s\tremaining: 39m 5s\n",
      "262:\ttest: 0.8203390\tbest: 0.8203411 (261)\ttotal: 13m 55s\tremaining: 39m 2s\n",
      "263:\ttest: 0.8203056\tbest: 0.8203411 (261)\ttotal: 13m 58s\tremaining: 38m 56s\n",
      "264:\ttest: 0.8203233\tbest: 0.8203411 (261)\ttotal: 14m\tremaining: 38m 51s\n",
      "265:\ttest: 0.8203390\tbest: 0.8203411 (261)\ttotal: 14m 3s\tremaining: 38m 46s\n",
      "266:\ttest: 0.8203592\tbest: 0.8203592 (266)\ttotal: 14m 6s\tremaining: 38m 42s\n",
      "267:\ttest: 0.8203994\tbest: 0.8203994 (267)\ttotal: 14m 8s\tremaining: 38m 37s\n",
      "268:\ttest: 0.8204338\tbest: 0.8204338 (268)\ttotal: 14m 11s\tremaining: 38m 33s\n",
      "269:\ttest: 0.8204486\tbest: 0.8204486 (269)\ttotal: 14m 13s\tremaining: 38m 28s\n",
      "270:\ttest: 0.8204523\tbest: 0.8204523 (270)\ttotal: 14m 16s\tremaining: 38m 23s\n",
      "271:\ttest: 0.8204410\tbest: 0.8204523 (270)\ttotal: 14m 19s\tremaining: 38m 19s\n",
      "272:\ttest: 0.8204588\tbest: 0.8204588 (272)\ttotal: 14m 21s\tremaining: 38m 14s\n",
      "273:\ttest: 0.8204761\tbest: 0.8204761 (273)\ttotal: 14m 24s\tremaining: 38m 9s\n",
      "274:\ttest: 0.8204659\tbest: 0.8204761 (273)\ttotal: 14m 26s\tremaining: 38m 5s\n",
      "275:\ttest: 0.8204984\tbest: 0.8204984 (275)\ttotal: 14m 29s\tremaining: 38m 1s\n",
      "276:\ttest: 0.8204923\tbest: 0.8204984 (275)\ttotal: 14m 32s\tremaining: 37m 56s\n",
      "277:\ttest: 0.8205047\tbest: 0.8205047 (277)\ttotal: 14m 35s\tremaining: 37m 53s\n",
      "278:\ttest: 0.8205147\tbest: 0.8205147 (278)\ttotal: 14m 38s\tremaining: 37m 49s\n",
      "279:\ttest: 0.8205163\tbest: 0.8205163 (279)\ttotal: 14m 40s\tremaining: 37m 44s\n",
      "280:\ttest: 0.8205663\tbest: 0.8205663 (280)\ttotal: 14m 43s\tremaining: 37m 40s\n",
      "281:\ttest: 0.8205664\tbest: 0.8205664 (281)\ttotal: 14m 46s\tremaining: 37m 36s\n",
      "282:\ttest: 0.8205861\tbest: 0.8205861 (282)\ttotal: 14m 48s\tremaining: 37m 31s\n",
      "283:\ttest: 0.8206247\tbest: 0.8206247 (283)\ttotal: 14m 51s\tremaining: 37m 28s\n",
      "284:\ttest: 0.8206446\tbest: 0.8206446 (284)\ttotal: 14m 54s\tremaining: 37m 23s\n",
      "285:\ttest: 0.8206522\tbest: 0.8206522 (285)\ttotal: 14m 56s\tremaining: 37m 18s\n",
      "286:\ttest: 0.8206435\tbest: 0.8206522 (285)\ttotal: 14m 59s\tremaining: 37m 14s\n",
      "287:\ttest: 0.8206620\tbest: 0.8206620 (287)\ttotal: 15m 2s\tremaining: 37m 11s\n",
      "288:\ttest: 0.8206419\tbest: 0.8206620 (287)\ttotal: 15m 5s\tremaining: 37m 7s\n",
      "289:\ttest: 0.8206744\tbest: 0.8206744 (289)\ttotal: 15m 7s\tremaining: 37m 2s\n",
      "290:\ttest: 0.8206980\tbest: 0.8206980 (290)\ttotal: 15m 10s\tremaining: 36m 57s\n",
      "291:\ttest: 0.8207292\tbest: 0.8207292 (291)\ttotal: 15m 12s\tremaining: 36m 52s\n",
      "292:\ttest: 0.8207181\tbest: 0.8207292 (291)\ttotal: 15m 15s\tremaining: 36m 49s\n",
      "293:\ttest: 0.8207442\tbest: 0.8207442 (293)\ttotal: 15m 18s\tremaining: 36m 44s\n",
      "294:\ttest: 0.8207285\tbest: 0.8207442 (293)\ttotal: 15m 21s\tremaining: 36m 42s\n",
      "295:\ttest: 0.8207455\tbest: 0.8207455 (295)\ttotal: 15m 23s\tremaining: 36m 37s\n",
      "296:\ttest: 0.8207281\tbest: 0.8207455 (295)\ttotal: 15m 26s\tremaining: 36m 33s\n",
      "297:\ttest: 0.8207399\tbest: 0.8207455 (295)\ttotal: 15m 28s\tremaining: 36m 28s\n",
      "298:\ttest: 0.8207594\tbest: 0.8207594 (298)\ttotal: 15m 31s\tremaining: 36m 23s\n",
      "299:\ttest: 0.8207692\tbest: 0.8207692 (299)\ttotal: 15m 34s\tremaining: 36m 19s\n",
      "300:\ttest: 0.8207734\tbest: 0.8207734 (300)\ttotal: 15m 37s\tremaining: 36m 16s\n",
      "301:\ttest: 0.8207990\tbest: 0.8207990 (301)\ttotal: 15m 39s\tremaining: 36m 12s\n",
      "302:\ttest: 0.8208109\tbest: 0.8208109 (302)\ttotal: 15m 42s\tremaining: 36m 8s\n",
      "303:\ttest: 0.8208039\tbest: 0.8208109 (302)\ttotal: 15m 45s\tremaining: 36m 4s\n",
      "304:\ttest: 0.8207946\tbest: 0.8208109 (302)\ttotal: 15m 47s\tremaining: 35m 59s\n",
      "305:\ttest: 0.8208066\tbest: 0.8208109 (302)\ttotal: 15m 50s\tremaining: 35m 55s\n",
      "306:\ttest: 0.8208387\tbest: 0.8208387 (306)\ttotal: 15m 52s\tremaining: 35m 50s\n",
      "307:\ttest: 0.8208316\tbest: 0.8208387 (306)\ttotal: 15m 55s\tremaining: 35m 47s\n",
      "308:\ttest: 0.8215405\tbest: 0.8215405 (308)\ttotal: 15m 59s\tremaining: 35m 45s\n",
      "309:\ttest: 0.8215373\tbest: 0.8215405 (308)\ttotal: 16m 2s\tremaining: 35m 41s\n",
      "310:\ttest: 0.8214555\tbest: 0.8215405 (308)\ttotal: 16m 4s\tremaining: 35m 37s\n",
      "311:\ttest: 0.8214712\tbest: 0.8215405 (308)\ttotal: 16m 7s\tremaining: 35m 33s\n",
      "312:\ttest: 0.8214719\tbest: 0.8215405 (308)\ttotal: 16m 10s\tremaining: 35m 29s\n",
      "313:\ttest: 0.8214779\tbest: 0.8215405 (308)\ttotal: 16m 13s\tremaining: 35m 25s\n",
      "314:\ttest: 0.8214782\tbest: 0.8215405 (308)\ttotal: 16m 15s\tremaining: 35m 20s\n",
      "315:\ttest: 0.8214805\tbest: 0.8215405 (308)\ttotal: 16m 17s\tremaining: 35m 16s\n",
      "316:\ttest: 0.8214914\tbest: 0.8215405 (308)\ttotal: 16m 20s\tremaining: 35m 13s\n",
      "317:\ttest: 0.8214888\tbest: 0.8215405 (308)\ttotal: 16m 23s\tremaining: 35m 9s\n",
      "318:\ttest: 0.8214864\tbest: 0.8215405 (308)\ttotal: 16m 26s\tremaining: 35m 5s\n",
      "319:\ttest: 0.8214899\tbest: 0.8215405 (308)\ttotal: 16m 28s\tremaining: 35m 1s\n",
      "320:\ttest: 0.8215646\tbest: 0.8215646 (320)\ttotal: 16m 31s\tremaining: 34m 57s\n",
      "321:\ttest: 0.8215667\tbest: 0.8215667 (321)\ttotal: 16m 34s\tremaining: 34m 53s\n",
      "322:\ttest: 0.8215673\tbest: 0.8215673 (322)\ttotal: 16m 37s\tremaining: 34m 50s\n",
      "323:\ttest: 0.8215817\tbest: 0.8215817 (323)\ttotal: 16m 40s\tremaining: 34m 46s\n",
      "324:\ttest: 0.8215473\tbest: 0.8215817 (323)\ttotal: 16m 42s\tremaining: 34m 42s\n",
      "325:\ttest: 0.8215588\tbest: 0.8215817 (323)\ttotal: 16m 45s\tremaining: 34m 39s\n",
      "326:\ttest: 0.8215521\tbest: 0.8215817 (323)\ttotal: 16m 48s\tremaining: 34m 35s\n",
      "327:\ttest: 0.8215549\tbest: 0.8215817 (323)\ttotal: 16m 51s\tremaining: 34m 31s\n",
      "328:\ttest: 0.8215307\tbest: 0.8215817 (323)\ttotal: 16m 54s\tremaining: 34m 28s\n",
      "329:\ttest: 0.8215518\tbest: 0.8215817 (323)\ttotal: 16m 56s\tremaining: 34m 24s\n",
      "330:\ttest: 0.8215571\tbest: 0.8215817 (323)\ttotal: 16m 59s\tremaining: 34m 20s\n",
      "331:\ttest: 0.8215558\tbest: 0.8215817 (323)\ttotal: 17m 2s\tremaining: 34m 17s\n",
      "332:\ttest: 0.8216039\tbest: 0.8216039 (332)\ttotal: 17m 5s\tremaining: 34m 14s\n",
      "333:\ttest: 0.8216116\tbest: 0.8216116 (333)\ttotal: 17m 8s\tremaining: 34m 10s\n",
      "334:\ttest: 0.8216228\tbest: 0.8216228 (334)\ttotal: 17m 11s\tremaining: 34m 7s\n",
      "335:\ttest: 0.8220531\tbest: 0.8220531 (335)\ttotal: 17m 14s\tremaining: 34m 5s\n",
      "336:\ttest: 0.8220340\tbest: 0.8220531 (335)\ttotal: 17m 17s\tremaining: 34m\n",
      "337:\ttest: 0.8220323\tbest: 0.8220531 (335)\ttotal: 17m 19s\tremaining: 33m 56s\n",
      "338:\ttest: 0.8220441\tbest: 0.8220531 (335)\ttotal: 17m 22s\tremaining: 33m 52s\n",
      "339:\ttest: 0.8220362\tbest: 0.8220531 (335)\ttotal: 17m 24s\tremaining: 33m 48s\n",
      "340:\ttest: 0.8220577\tbest: 0.8220577 (340)\ttotal: 17m 27s\tremaining: 33m 44s\n",
      "341:\ttest: 0.8220546\tbest: 0.8220577 (340)\ttotal: 17m 29s\tremaining: 33m 40s\n",
      "342:\ttest: 0.8220603\tbest: 0.8220603 (342)\ttotal: 17m 32s\tremaining: 33m 36s\n",
      "343:\ttest: 0.8220559\tbest: 0.8220603 (342)\ttotal: 17m 35s\tremaining: 33m 31s\n",
      "344:\ttest: 0.8220286\tbest: 0.8220603 (342)\ttotal: 17m 37s\tremaining: 33m 27s\n",
      "345:\ttest: 0.8220604\tbest: 0.8220604 (345)\ttotal: 17m 40s\tremaining: 33m 24s\n",
      "346:\ttest: 0.8220527\tbest: 0.8220604 (345)\ttotal: 17m 43s\tremaining: 33m 20s\n",
      "347:\ttest: 0.8220605\tbest: 0.8220605 (347)\ttotal: 17m 45s\tremaining: 33m 16s\n",
      "348:\ttest: 0.8220684\tbest: 0.8220684 (348)\ttotal: 17m 48s\tremaining: 33m 13s\n",
      "349:\ttest: 0.8220744\tbest: 0.8220744 (349)\ttotal: 17m 51s\tremaining: 33m 9s\n",
      "350:\ttest: 0.8220567\tbest: 0.8220744 (349)\ttotal: 17m 53s\tremaining: 33m 5s\n",
      "351:\ttest: 0.8220690\tbest: 0.8220744 (349)\ttotal: 17m 56s\tremaining: 33m 1s\n",
      "352:\ttest: 0.8220669\tbest: 0.8220744 (349)\ttotal: 17m 59s\tremaining: 32m 58s\n",
      "353:\ttest: 0.8220837\tbest: 0.8220837 (353)\ttotal: 18m 1s\tremaining: 32m 54s\n",
      "354:\ttest: 0.8227408\tbest: 0.8227408 (354)\ttotal: 18m 4s\tremaining: 32m 50s\n",
      "355:\ttest: 0.8227402\tbest: 0.8227408 (354)\ttotal: 18m 7s\tremaining: 32m 46s\n",
      "356:\ttest: 0.8227368\tbest: 0.8227408 (354)\ttotal: 18m 9s\tremaining: 32m 42s\n",
      "357:\ttest: 0.8227583\tbest: 0.8227583 (357)\ttotal: 18m 12s\tremaining: 32m 38s\n",
      "358:\ttest: 0.8227716\tbest: 0.8227716 (358)\ttotal: 18m 14s\tremaining: 32m 35s\n",
      "359:\ttest: 0.8227812\tbest: 0.8227812 (359)\ttotal: 18m 17s\tremaining: 32m 31s\n",
      "360:\ttest: 0.8227935\tbest: 0.8227935 (360)\ttotal: 18m 20s\tremaining: 32m 27s\n",
      "361:\ttest: 0.8229874\tbest: 0.8229874 (361)\ttotal: 18m 23s\tremaining: 32m 24s\n",
      "362:\ttest: 0.8230102\tbest: 0.8230102 (362)\ttotal: 18m 25s\tremaining: 32m 20s\n",
      "363:\ttest: 0.8230008\tbest: 0.8230102 (362)\ttotal: 18m 28s\tremaining: 32m 16s\n",
      "364:\ttest: 0.8229885\tbest: 0.8230102 (362)\ttotal: 18m 30s\tremaining: 32m 12s\n",
      "365:\ttest: 0.8230199\tbest: 0.8230199 (365)\ttotal: 18m 33s\tremaining: 32m 9s\n",
      "366:\ttest: 0.8230161\tbest: 0.8230199 (365)\ttotal: 18m 36s\tremaining: 32m 5s\n",
      "367:\ttest: 0.8230177\tbest: 0.8230199 (365)\ttotal: 18m 38s\tremaining: 32m 1s\n",
      "368:\ttest: 0.8230314\tbest: 0.8230314 (368)\ttotal: 18m 42s\tremaining: 31m 59s\n",
      "369:\ttest: 0.8230220\tbest: 0.8230314 (368)\ttotal: 18m 45s\tremaining: 31m 56s\n",
      "370:\ttest: 0.8230183\tbest: 0.8230314 (368)\ttotal: 18m 47s\tremaining: 31m 52s\n",
      "371:\ttest: 0.8230049\tbest: 0.8230314 (368)\ttotal: 18m 51s\tremaining: 31m 49s\n",
      "372:\ttest: 0.8230046\tbest: 0.8230314 (368)\ttotal: 18m 53s\tremaining: 31m 46s\n",
      "373:\ttest: 0.8230068\tbest: 0.8230314 (368)\ttotal: 18m 56s\tremaining: 31m 41s\n",
      "374:\ttest: 0.8230016\tbest: 0.8230314 (368)\ttotal: 18m 59s\tremaining: 31m 38s\n",
      "375:\ttest: 0.8229703\tbest: 0.8230314 (368)\ttotal: 19m 2s\tremaining: 31m 35s\n",
      "376:\ttest: 0.8229956\tbest: 0.8230314 (368)\ttotal: 19m 5s\tremaining: 31m 32s\n",
      "377:\ttest: 0.8229850\tbest: 0.8230314 (368)\ttotal: 19m 8s\tremaining: 31m 29s\n",
      "378:\ttest: 0.8229838\tbest: 0.8230314 (368)\ttotal: 19m 10s\tremaining: 31m 25s\n",
      "379:\ttest: 0.8229689\tbest: 0.8230314 (368)\ttotal: 19m 13s\tremaining: 31m 21s\n",
      "380:\ttest: 0.8229469\tbest: 0.8230314 (368)\ttotal: 19m 16s\tremaining: 31m 18s\n",
      "381:\ttest: 0.8229687\tbest: 0.8230314 (368)\ttotal: 19m 18s\tremaining: 31m 14s\n",
      "382:\ttest: 0.8229462\tbest: 0.8230314 (368)\ttotal: 19m 22s\tremaining: 31m 12s\n",
      "383:\ttest: 0.8229426\tbest: 0.8230314 (368)\ttotal: 19m 24s\tremaining: 31m 8s\n",
      "384:\ttest: 0.8229287\tbest: 0.8230314 (368)\ttotal: 19m 27s\tremaining: 31m 4s\n",
      "385:\ttest: 0.8228868\tbest: 0.8230314 (368)\ttotal: 19m 30s\tremaining: 31m 1s\n",
      "386:\ttest: 0.8228622\tbest: 0.8230314 (368)\ttotal: 19m 32s\tremaining: 30m 57s\n",
      "387:\ttest: 0.8228741\tbest: 0.8230314 (368)\ttotal: 19m 35s\tremaining: 30m 54s\n",
      "388:\ttest: 0.8228993\tbest: 0.8230314 (368)\ttotal: 19m 38s\tremaining: 30m 51s\n",
      "389:\ttest: 0.8228973\tbest: 0.8230314 (368)\ttotal: 19m 41s\tremaining: 30m 48s\n",
      "390:\ttest: 0.8228642\tbest: 0.8230314 (368)\ttotal: 19m 44s\tremaining: 30m 44s\n",
      "391:\ttest: 0.8228654\tbest: 0.8230314 (368)\ttotal: 19m 46s\tremaining: 30m 40s\n",
      "392:\ttest: 0.8228658\tbest: 0.8230314 (368)\ttotal: 19m 49s\tremaining: 30m 37s\n",
      "393:\ttest: 0.8228816\tbest: 0.8230314 (368)\ttotal: 19m 52s\tremaining: 30m 33s\n",
      "394:\ttest: 0.8228677\tbest: 0.8230314 (368)\ttotal: 19m 54s\tremaining: 30m 30s\n",
      "395:\ttest: 0.8229042\tbest: 0.8230314 (368)\ttotal: 19m 57s\tremaining: 30m 26s\n",
      "396:\ttest: 0.8228999\tbest: 0.8230314 (368)\ttotal: 19m 59s\tremaining: 30m 22s\n",
      "397:\ttest: 0.8228875\tbest: 0.8230314 (368)\ttotal: 20m 2s\tremaining: 30m 19s\n",
      "398:\ttest: 0.8228987\tbest: 0.8230314 (368)\ttotal: 20m 5s\tremaining: 30m 15s\n",
      "399:\ttest: 0.8228912\tbest: 0.8230314 (368)\ttotal: 20m 7s\tremaining: 30m 11s\n",
      "400:\ttest: 0.8228975\tbest: 0.8230314 (368)\ttotal: 20m 10s\tremaining: 30m 8s\n",
      "401:\ttest: 0.8228796\tbest: 0.8230314 (368)\ttotal: 20m 12s\tremaining: 30m 4s\n",
      "402:\ttest: 0.8228636\tbest: 0.8230314 (368)\ttotal: 20m 15s\tremaining: 30m\n",
      "403:\ttest: 0.8228667\tbest: 0.8230314 (368)\ttotal: 20m 17s\tremaining: 29m 56s\n",
      "404:\ttest: 0.8228585\tbest: 0.8230314 (368)\ttotal: 20m 20s\tremaining: 29m 53s\n",
      "405:\ttest: 0.8228564\tbest: 0.8230314 (368)\ttotal: 20m 23s\tremaining: 29m 49s\n",
      "406:\ttest: 0.8228711\tbest: 0.8230314 (368)\ttotal: 20m 25s\tremaining: 29m 46s\n",
      "407:\ttest: 0.8228401\tbest: 0.8230314 (368)\ttotal: 20m 28s\tremaining: 29m 42s\n",
      "408:\ttest: 0.8228609\tbest: 0.8230314 (368)\ttotal: 20m 31s\tremaining: 29m 39s\n",
      "409:\ttest: 0.8228824\tbest: 0.8230314 (368)\ttotal: 20m 34s\tremaining: 29m 35s\n",
      "410:\ttest: 0.8228921\tbest: 0.8230314 (368)\ttotal: 20m 36s\tremaining: 29m 32s\n",
      "411:\ttest: 0.8228957\tbest: 0.8230314 (368)\ttotal: 20m 39s\tremaining: 29m 28s\n",
      "412:\ttest: 0.8228928\tbest: 0.8230314 (368)\ttotal: 20m 41s\tremaining: 29m 25s\n",
      "413:\ttest: 0.8229052\tbest: 0.8230314 (368)\ttotal: 20m 44s\tremaining: 29m 21s\n",
      "414:\ttest: 0.8228878\tbest: 0.8230314 (368)\ttotal: 20m 47s\tremaining: 29m 18s\n",
      "415:\ttest: 0.8228744\tbest: 0.8230314 (368)\ttotal: 20m 49s\tremaining: 29m 14s\n",
      "416:\ttest: 0.8228748\tbest: 0.8230314 (368)\ttotal: 20m 52s\tremaining: 29m 11s\n",
      "417:\ttest: 0.8229083\tbest: 0.8230314 (368)\ttotal: 20m 54s\tremaining: 29m 7s\n",
      "418:\ttest: 0.8228976\tbest: 0.8230314 (368)\ttotal: 20m 57s\tremaining: 29m 3s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.8230314322\n",
      "bestIteration = 368\n",
      "\n",
      "Shrink model to first 369 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 15:27:34 - INFO - \n",
      "Лучшие параметры:\n",
      "2025-04-21 15:27:34 - INFO - learning_rate: 0.004684283500572371\n",
      "2025-04-21 15:27:34 - INFO - depth: 6\n",
      "2025-04-21 15:27:34 - INFO - l2_leaf_reg: 5.36104374072008e-06\n",
      "2025-04-21 15:27:34 - INFO - random_strength: 0.007652648646042903\n",
      "2025-04-21 15:27:34 - INFO - bagging_temperature: 0.9487648394691982\n",
      "2025-04-21 15:27:34 - INFO - leaf_estimation_iterations: 9\n",
      "2025-04-21 15:27:34 - INFO - \n",
      "Метрики на лучших параметрах:\n",
      "2025-04-21 15:27:34 - INFO - ndcg_10: 0.8393\n",
      "2025-04-21 15:27:34 - INFO - precision_10: 0.1210\n",
      "2025-04-21 15:27:34 - INFO - recall_10: 0.3156\n",
      "2025-04-21 15:27:34 - INFO - diversity_10: 0.0052\n",
      "2025-04-21 15:27:34 - INFO - novelty_10: 0.3672\n",
      "2025-04-21 15:27:34 - INFO - serendipity_10: 0.0039\n",
      "2025-04-21 15:27:34 - INFO - ndcg_100: 0.8272\n",
      "2025-04-21 15:27:34 - INFO - precision_100: 0.0697\n",
      "2025-04-21 15:27:34 - INFO - recall_100: 0.3538\n",
      "2025-04-21 15:27:34 - INFO - diversity_100: 0.0225\n",
      "2025-04-21 15:27:34 - INFO - novelty_100: 0.6363\n",
      "2025-04-21 15:27:34 - INFO - serendipity_100: 0.0022\n",
      "2025-04-21 15:27:34 - INFO - ndcg_1000: 0.8318\n",
      "2025-04-21 15:27:34 - INFO - precision_1000: 0.0477\n",
      "2025-04-21 15:27:34 - INFO - recall_1000: 0.4952\n",
      "2025-04-21 15:27:34 - INFO - diversity_1000: 0.0777\n",
      "2025-04-21 15:27:34 - INFO - novelty_1000: 0.7544\n",
      "2025-04-21 15:27:34 - INFO - serendipity_1000: 0.0015\n",
      "2025-04-21 15:27:37 - INFO - Лучшие параметры: {'learning_rate': 0.004684283500572371, 'depth': 6, 'l2_leaf_reg': 5.36104374072008e-06, 'random_strength': 0.007652648646042903, 'bagging_temperature': 0.9487648394691982, 'leaf_estimation_iterations': 9}\n",
      "2025-04-21 15:27:37 - INFO - Результаты:\n",
      "2025-04-21 15:27:37 - INFO - Метрики для K=10:\n",
      "2025-04-21 15:27:37 - INFO - NDCG@10: 0.8393\n",
      "2025-04-21 15:27:37 - INFO - Precision@10: 0.1210\n",
      "2025-04-21 15:27:37 - INFO - Recall@10: 0.3156\n",
      "2025-04-21 15:27:37 - INFO - Diversity@10: 0.0052\n",
      "2025-04-21 15:27:37 - INFO - Novelty@10: 0.3672\n",
      "2025-04-21 15:27:37 - INFO - Serendipity@10: 0.0039\n",
      "2025-04-21 15:27:37 - INFO - --------------------------------\n",
      "2025-04-21 15:27:37 - INFO - Метрики для K=100:\n",
      "2025-04-21 15:27:37 - INFO - NDCG@100: 0.8272\n",
      "2025-04-21 15:27:37 - INFO - Precision@100: 0.0697\n",
      "2025-04-21 15:27:37 - INFO - Recall@100: 0.3538\n",
      "2025-04-21 15:27:37 - INFO - Diversity@100: 0.0225\n",
      "2025-04-21 15:27:37 - INFO - Novelty@100: 0.6363\n",
      "2025-04-21 15:27:37 - INFO - Serendipity@100: 0.0022\n",
      "2025-04-21 15:27:37 - INFO - --------------------------------\n",
      "2025-04-21 15:27:37 - INFO - Метрики для K=1000:\n",
      "2025-04-21 15:27:37 - INFO - NDCG@1000: 0.8318\n",
      "2025-04-21 15:27:37 - INFO - Precision@1000: 0.0477\n",
      "2025-04-21 15:27:37 - INFO - Recall@1000: 0.4952\n",
      "2025-04-21 15:27:37 - INFO - Diversity@1000: 0.0777\n",
      "2025-04-21 15:27:37 - INFO - Novelty@1000: 0.7544\n",
      "2025-04-21 15:27:37 - INFO - Serendipity@1000: 0.0015\n",
      "2025-04-21 15:27:37 - INFO - --------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    n_samples = 100\n",
    "    # buyer_ids = test_data[\"buyer_id\"].unique()[:n_samples]\n",
    "    buyer_ids = test_data[\"buyer_id\"].unique()\n",
    "\n",
    "    temp_candidates = candidates[candidates[\"buyer_id\"].isin(buyer_ids)].copy()\n",
    "\n",
    "    product_ids = temp_candidates[\"product_id\"].unique()\n",
    "    temp_train_data = train_data[\n",
    "        (train_data[\"buyer_id\"].isin(buyer_ids))\n",
    "        & (train_data[\"product_id\"].isin(product_ids))\n",
    "    ].copy()\n",
    "\n",
    "    temp_test_data = test_data[\n",
    "        (test_data[\"buyer_id\"].isin(buyer_ids))\n",
    "        & (test_data[\"product_id\"].isin(product_ids))\n",
    "    ].copy()\n",
    "\n",
    "    metrics_calculator = MetricsCalculator([10, 100, 1000])\n",
    "\n",
    "    best_params, best_metrics = optimize_catboost_parameters(\n",
    "        train_data=temp_train_data,\n",
    "        test_data=temp_test_data,\n",
    "        candidates=temp_candidates,\n",
    "        metrics_calculator=metrics_calculator,\n",
    "        n_trials=10,\n",
    "        timeout=7200,\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Лучшие параметры: {best_params}\")\n",
    "    logging.info(\"Результаты:\")\n",
    "    for k in metrics_calculator.k_values:\n",
    "        logging.info(f\"Метрики для K={k}:\")\n",
    "        logging.info(f\"NDCG@{k}: {best_metrics[f'ndcg_{k}']:.4f}\")\n",
    "        logging.info(f\"Precision@{k}: {best_metrics[f'precision_{k}']:.4f}\")\n",
    "        logging.info(f\"Recall@{k}: {best_metrics[f'recall_{k}']:.4f}\")\n",
    "        logging.info(f\"Diversity@{k}: {best_metrics[f'diversity_{k}']:.4f}\")\n",
    "        logging.info(f\"Novelty@{k}: {best_metrics[f'novelty_{k}']:.4f}\")\n",
    "        logging.info(f\"Serendipity@{k}: {best_metrics[f'serendipity_{k}']:.4f}\")\n",
    "        logging.info(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель 3: XGBoost (Градиентный бустинг над решающими деревьями)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_train.csv\"\n",
    ")\n",
    "test_data = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_test.csv\"\n",
    ")\n",
    "candidates = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_als_recommendations.csv\"\n",
    ")\n",
    "\n",
    "item_categories = {\n",
    "    row[\"product_id\"]: row[\"product_group\"] for _, row in train_data.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostRanker:\n",
    "    \"\"\"\n",
    "    Ранжировщик на основе XGBoost.\n",
    "\n",
    "    Модель использует градиентный бустинг для ранжирования товаров-кандидатов\n",
    "    на основе исторических данных о покупках и характеристик пользователей.\n",
    "\n",
    "    Attrs:\n",
    "    ----------\n",
    "    model : xgb.XGBRanker\n",
    "        Модель XGBoost для ранжирования\n",
    "    user_encoder : LabelEncoder\n",
    "        Энкодер для преобразования ID пользователей в индексы\n",
    "    item_encoder : LabelEncoder\n",
    "        Энкодер для преобразования ID товаров в индексы\n",
    "    product_group_encoder : LabelEncoder\n",
    "        Энкодер для преобразования групп товаров в индексы\n",
    "    feature_columns : List[str]\n",
    "        Список колонок, используемых как признаки для обучения\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        max_depth: int = 6,\n",
    "        n_estimators: int = 100,\n",
    "        min_child_weight: float = 1.0,\n",
    "        subsample: float = 1.0,\n",
    "        colsample_bytree: float = 1.0,\n",
    "        gamma: float = 0.0,\n",
    "        random_state: int = 42,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация ранжировщика.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        learning_rate : float, по умолчанию 0.1\n",
    "            Скорость обучения модели\n",
    "        max_depth : int, по умолчанию 6\n",
    "            Максимальная глубина деревьев\n",
    "        n_estimators : int, по умолчанию 100\n",
    "            Количество деревьев в ансамбле\n",
    "        min_child_weight : float, по умолчанию 1.0\n",
    "            Минимальная сумма весов всех наблюдений, необходимая для дочернего узла\n",
    "        subsample : float, по умолчанию 1.0\n",
    "            Доля выборки для обучения каждого дерева\n",
    "        colsample_bytree : float, по умолчанию 1.0\n",
    "            Доля признаков для обучения каждого дерева\n",
    "        gamma : float, по умолчанию 0.0\n",
    "            Минимальное уменьшение потерь, необходимое для создания нового разбиения\n",
    "        random_state : int, по умолчанию 42\n",
    "            Seed для воспроизводимости результатов\n",
    "        \"\"\"\n",
    "        self.model = xgb.XGBRanker(\n",
    "            objective='rank:ndcg',\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            n_estimators=n_estimators,\n",
    "            min_child_weight=min_child_weight,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            gamma=gamma,\n",
    "            random_state=random_state,\n",
    "            tree_method='hist',\n",
    "            ndcg_exp_gain=False,\n",
    "        )\n",
    "\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        self.product_group_encoder = LabelEncoder()\n",
    "\n",
    "        # Базовые признаки пользователя\n",
    "        self.user_features = [\n",
    "            \"buyer_sms_allowed\",\n",
    "            \"buyer_emails_allowed\",\n",
    "            \"buyer_account_status_loyal\",\n",
    "            \"buyer_account_status_unregistered\",\n",
    "            \"buyer_account_status_new\",\n",
    "            \"buyer_account_status_potential\",\n",
    "            \"buyer_account_status_lost\",\n",
    "            \"buyer_account_status_sleeping\",\n",
    "            \"buyer_is_female\",\n",
    "            \"buyer_age\",\n",
    "        ]\n",
    "\n",
    "        # Признаки для обучения будут дополнены в методе fit\n",
    "        self.feature_columns = None\n",
    "\n",
    "    def _prepare_user_features(\n",
    "        self, data: pd.DataFrame, user_col: str = \"buyer_id\"\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Подготовка признаков пользователей.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            Датафрейм с данными пользователей\n",
    "        user_col : str, по умолчанию 'buyer_id'\n",
    "            Название столбца с идентификаторами пользователей\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        pd.DataFrame\n",
    "            Датафрейм с подготовленными признаками пользователей\n",
    "        \"\"\"\n",
    "        return data.groupby(user_col)[self.user_features].first().reset_index()\n",
    "\n",
    "    def _prepare_item_features(\n",
    "        self, data: pd.DataFrame, item_col: str = \"product_id\"\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Подготовка признаков товаров.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            Датафрейм с данными о товарах\n",
    "        item_col : str, по умолчанию 'product_id'\n",
    "            Название столбца с идентификаторами товаров\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        pd.DataFrame\n",
    "            Датафрейм с подготовленными признаками товаров\n",
    "        \"\"\"\n",
    "        # Агрегируем статистики по товарам\n",
    "        item_stats = (\n",
    "            data.groupby(item_col)\n",
    "            .agg(\n",
    "                {\n",
    "                    \"product_count\": [\"mean\", \"sum\"],\n",
    "                    \"product_sum\": [\"mean\", \"sum\"],\n",
    "                    \"product_group\": \"first\",\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Переименовываем колонки\n",
    "        item_stats.columns = [\n",
    "            item_col,\n",
    "            \"avg_product_count\",\n",
    "            \"total_product_count\",\n",
    "            \"avg_product_sum\",\n",
    "            \"total_product_sum\",\n",
    "            \"product_group\",\n",
    "        ]\n",
    "\n",
    "        # Кодируем группы товаров\n",
    "        item_stats[\"product_group_encoded\"] = self.product_group_encoder.transform(\n",
    "            item_stats[\"product_group\"]\n",
    "        )\n",
    "\n",
    "        return item_stats\n",
    "\n",
    "    def _prepare_user_item_features(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        user_col: str = \"buyer_id\",\n",
    "        item_col: str = \"product_id\",\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Подготовка признаков взаимодействия пользователь-товар.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            Датафрейм с данными о взаимодействиях\n",
    "        user_col : str, по умолчанию 'buyer_id'\n",
    "            Название столбца с идентификаторами пользователей\n",
    "        item_col : str, по умолчанию 'product_id'\n",
    "            Название столбца с идентификаторами товаров\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        pd.DataFrame\n",
    "            Датафрейм с признаками взаимодействий\n",
    "        \"\"\"\n",
    "        # Агрегируем статистики по парам пользователь-товар\n",
    "        user_item_stats = (\n",
    "            data.groupby([user_col, item_col])\n",
    "            .agg(\n",
    "                {\n",
    "                    \"product_count\": [\"count\", \"mean\", \"sum\"],\n",
    "                    \"product_sum\": [\"mean\", \"sum\"],\n",
    "                    \"order_date\": \"max\",  # последняя дата покупки\n",
    "                }\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Переименовываем колонки\n",
    "        user_item_stats.columns = [\n",
    "            user_col,\n",
    "            item_col,\n",
    "            \"interaction_count\",\n",
    "            \"avg_purchase_count\",\n",
    "            \"total_purchase_count\",\n",
    "            \"avg_purchase_sum\",\n",
    "            \"total_purchase_sum\",\n",
    "            \"last_purchase_date\",\n",
    "        ]\n",
    "\n",
    "        # Преобразуем даты в datetime\n",
    "        user_item_stats[\"last_purchase_date\"] = pd.to_datetime(\n",
    "            user_item_stats[\"last_purchase_date\"]\n",
    "        )\n",
    "\n",
    "        # Добавляем признак времени с последней покупки\n",
    "        current_date = user_item_stats[\"last_purchase_date\"].max()\n",
    "        user_item_stats[\"days_since_last_purchase\"] = (\n",
    "            current_date - user_item_stats[\"last_purchase_date\"]\n",
    "        ).dt.days\n",
    "\n",
    "        return user_item_stats\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        user_col: str = \"buyer_id\",\n",
    "        item_col: str = \"product_id\",\n",
    "    ) -> \"XGBoostRanker\":\n",
    "        \"\"\"\n",
    "        Обучение ранжировщика на исторических данных.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            Датафрейм с историческими данными о покупках\n",
    "        user_col : str, по умолчанию 'buyer_id'\n",
    "            Название столбца с идентификаторами пользователей\n",
    "        item_col : str, по умолчанию 'product_id'\n",
    "            Название столбца с идентификаторами товаров\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self : XGBoostRanker\n",
    "            Обученный объект\n",
    "        \"\"\"\n",
    "        # Кодируем ID пользователей и товаров\n",
    "        self.user_encoder.fit(data[user_col].unique())\n",
    "        self.item_encoder.fit(data[item_col].unique())\n",
    "        self.product_group_encoder.fit(data[\"product_group\"].unique())\n",
    "\n",
    "        # Подготавливаем признаки\n",
    "        user_features = self._prepare_user_features(data, user_col)\n",
    "        item_features = self._prepare_item_features(data, item_col)\n",
    "        user_item_features = self._prepare_user_item_features(data, user_col, item_col)\n",
    "\n",
    "        # Объединяем все признаки\n",
    "        features = user_item_features.merge(\n",
    "            user_features, on=user_col, how=\"left\"\n",
    "        ).merge(item_features, on=item_col, how=\"left\")\n",
    "\n",
    "        # Добавляем закодированные ID\n",
    "        features[\"user_encoded\"] = self.user_encoder.transform(features[user_col])\n",
    "        features[\"item_encoded\"] = self.item_encoder.transform(features[item_col])\n",
    "\n",
    "        # Определяем список признаков для обучения\n",
    "        self.feature_columns = [\n",
    "            \"user_encoded\",\n",
    "            \"item_encoded\",\n",
    "            \"interaction_count\",\n",
    "            \"avg_purchase_count\",\n",
    "            \"total_purchase_count\",\n",
    "            \"avg_purchase_sum\",\n",
    "            \"total_purchase_sum\",\n",
    "            \"days_since_last_purchase\",\n",
    "            \"avg_product_count\",\n",
    "            \"total_product_count\",\n",
    "            \"avg_product_sum\",\n",
    "            \"total_product_sum\",\n",
    "            \"product_group_encoded\",\n",
    "        ] + self.user_features\n",
    "\n",
    "        # Готовим данные для обучения\n",
    "        X = features[self.feature_columns]\n",
    "        y = features[\n",
    "            \"interaction_count\"\n",
    "        ]  # используем количество взаимодействий как целевую переменную\n",
    "        groups = features.groupby(user_col).size().values\n",
    "\n",
    "        # Обучаем модель\n",
    "        self.model.fit(X, y, group=groups, verbose=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def rank(\n",
    "        self,\n",
    "        candidates: pd.DataFrame,\n",
    "        train_data: pd.DataFrame,\n",
    "        top_n: int = 10,\n",
    "        user_col: str = \"buyer_id\",\n",
    "        item_col: str = \"product_id\",\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ранжирование кандидатов на основе XGBoost.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        candidates : pd.DataFrame\n",
    "            Датафрейм с парами пользователь-товар для ранжирования\n",
    "        train_data : pd.DataFrame\n",
    "            Датафрейм с историческими данными для расчета признаков\n",
    "        top_n : int, по умолчанию 10\n",
    "            Количество рекомендуемых товаров для каждого пользователя\n",
    "        user_col : str, по умолчанию 'buyer_id'\n",
    "            Название столбца с идентификаторами пользователей\n",
    "        item_col : str, по умолчанию 'product_id'\n",
    "            Название столбца с идентификаторами товаров\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        pd.DataFrame\n",
    "            Датафрейм с ранжированными рекомендациями и их скорами\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\") or self.model is None:\n",
    "            raise ValueError(\"Модель не обучена. Сначала вызовите метод fit()\")\n",
    "\n",
    "        # Фильтруем только известные пользователи и товары\n",
    "        known_users_mask = candidates[user_col].isin(self.user_encoder.classes_)\n",
    "        known_items_mask = candidates[item_col].isin(self.item_encoder.classes_)\n",
    "        valid_candidates = candidates[known_users_mask & known_items_mask].copy()\n",
    "\n",
    "        if valid_candidates.empty:\n",
    "            return pd.DataFrame(columns=[user_col, item_col, \"score\"])\n",
    "\n",
    "        # Подготавливаем признаки\n",
    "        user_features = self._prepare_user_features(train_data, user_col)\n",
    "        item_features = self._prepare_item_features(train_data, item_col)\n",
    "        user_item_features = self._prepare_user_item_features(\n",
    "            train_data, user_col, item_col\n",
    "        )\n",
    "\n",
    "        # Объединяем признаки с кандидатами\n",
    "        features = (\n",
    "            valid_candidates.merge(user_features, on=user_col, how=\"left\")\n",
    "            .merge(item_features, on=item_col, how=\"left\")\n",
    "            .merge(user_item_features, on=[user_col, item_col], how=\"left\")\n",
    "        )\n",
    "\n",
    "        # Заполняем пропущенные значения\n",
    "        features = features.fillna(\n",
    "            {\n",
    "                \"interaction_count\": 0,\n",
    "                \"avg_purchase_count\": 0,\n",
    "                \"total_purchase_count\": 0,\n",
    "                \"avg_purchase_sum\": 0,\n",
    "                \"total_purchase_sum\": 0,\n",
    "                \"days_since_last_purchase\": (\n",
    "                    features[\"days_since_last_purchase\"].max()\n",
    "                    if not pd.isna(features[\"days_since_last_purchase\"]).all()\n",
    "                    else 365\n",
    "                ),  # если нет данных, предполагаем год\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Кодируем ID\n",
    "        features[\"user_encoded\"] = self.user_encoder.transform(features[user_col])\n",
    "        features[\"item_encoded\"] = self.item_encoder.transform(features[item_col])\n",
    "\n",
    "        # Получаем предсказания\n",
    "        X = features[self.feature_columns]\n",
    "        scores = self.model.predict(X)\n",
    "\n",
    "        # Формируем результат\n",
    "        recommendations = features[[user_col, item_col]].copy()\n",
    "        recommendations[\"score\"] = scores\n",
    "\n",
    "        # Сортируем и отбираем top_n\n",
    "        recommendations = recommendations.sort_values(\n",
    "            by=[user_col, \"score\"], ascending=[True, False]\n",
    "        )\n",
    "        recommendations = recommendations.groupby(user_col).head(top_n)\n",
    "\n",
    "        return recommendations\n",
    "    \n",
    "    def save_model(\n",
    "        self, \n",
    "        model_path: str, \n",
    "        export_parameters: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Сохранение модели и её параметров.\n",
    "\n",
    "        Args:\n",
    "            model_path: путь для сохранения модели\n",
    "            export_parameters: сохранять ли дополнительные параметры модели\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Модель не обучена. Сначала вызовите метод fit()\")\n",
    "\n",
    "        # Создаем директорию если её нет\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "        # Сохраняем модель XGBoost\n",
    "        self.model.save_model(model_path)\n",
    "\n",
    "        if export_parameters:\n",
    "            # Сохраняем дополнительные параметры модели\n",
    "            parameters = {\n",
    "                'user_encoder': self.user_encoder,\n",
    "                'item_encoder': self.item_encoder,\n",
    "                'product_group_encoder': self.product_group_encoder,\n",
    "                'user_features': self.user_features,\n",
    "                'feature_columns': self.feature_columns\n",
    "            }\n",
    "\n",
    "            # Путь для сохранения параметров\n",
    "            params_path = f\"{os.path.splitext(model_path)[0]}_params.joblib\"\n",
    "\n",
    "            joblib.dump(parameters, params_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(\n",
    "        cls, \n",
    "        model_path: str, \n",
    "        load_parameters: bool = True\n",
    "    ) -> \"XGBoostRanker\":\n",
    "        \"\"\"\n",
    "        Загрузка сохранённой модели и её параметров.\n",
    "\n",
    "        Args:\n",
    "            model_path: путь к сохранённой модели\n",
    "            load_parameters: загружать ли дополнительные параметры модели\n",
    "\n",
    "        Returns:\n",
    "            XGBoostRanker: загруженная модель\n",
    "        \"\"\"\n",
    "        # Создаем экземпляр класса\n",
    "        ranker = cls()\n",
    "\n",
    "        # Загружаем модель XGBoost\n",
    "        ranker.model = xgb.XGBRanker()\n",
    "        ranker.model.load_model(model_path)\n",
    "\n",
    "        if load_parameters:\n",
    "            # Путь к файлу с параметрами\n",
    "            params_path = f\"{os.path.splitext(model_path)[0]}_params.joblib\"\n",
    "\n",
    "            if os.path.exists(params_path):\n",
    "                parameters = joblib.load(params_path)\n",
    "\n",
    "                # Загружаем параметры\n",
    "                ranker.user_encoder = parameters['user_encoder']\n",
    "                ranker.item_encoder = parameters['item_encoder']\n",
    "                ranker.product_group_encoder = parameters['product_group_encoder']\n",
    "                ranker.user_features = parameters['user_features']\n",
    "                ranker.feature_columns = parameters['feature_columns']\n",
    "            else:\n",
    "                logging.warning(\n",
    "                    f\"Файл с параметрами {params_path} не найден. \"\n",
    "                    \"Загружена только модель без дополнительных параметров.\"\n",
    "                )\n",
    "\n",
    "        return ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgboost_parameters(\n",
    "    train_data: pd.DataFrame,\n",
    "    test_data: pd.DataFrame,\n",
    "    candidates: pd.DataFrame,\n",
    "    metrics_calculator: MetricsCalculator,\n",
    "    item_categories: Dict[str, str] = None,\n",
    "    n_trials: int = 10,\n",
    "    k_values: List[int] = [10, 100, 1000],\n",
    "    random_state: int = 42,\n",
    "    timeout: int = 7200,\n",
    ") -> Tuple[Dict[str, float], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Оптимизация гиперпараметров XGBoostRanker с помощью Optuna.\n",
    "\n",
    "    Args:\n",
    "    ----------\n",
    "    train_data : pd.DataFrame\n",
    "        Датафрейм с историческими данными о покупках для обучения\n",
    "    test_data : pd.DataFrame\n",
    "        Датафрейм с историческими данными о покупках для тестирования\n",
    "    candidates : pd.DataFrame\n",
    "        Датафрейм с парами пользователь-товар для ранжирования\n",
    "    metrics_calculator : MetricsCalculator\n",
    "        Калькулятор метрик\n",
    "    item_categories : Dict[str, str], по умолчанию None\n",
    "        Словарь соответствия товаров и их категорий {item_id: category_id}\n",
    "    n_trials : int, по умолчанию 10\n",
    "        Количество итераций оптимизации\n",
    "    k_values : List[int], по умолчанию [10, 100, 1000]\n",
    "        Список значений K для расчета метрик\n",
    "    random_state : int, по умолчанию 42\n",
    "        Seed для воспроизводимости результатов\n",
    "    timeout : int, по умолчанию 42\n",
    "        Максимальное время оптимизации в секундах\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    Tuple[Dict[str, float], Dict[str, float]]\n",
    "        Кортеж из двух словарей:\n",
    "        1. Лучшие найденные гиперпараметры\n",
    "        2. Значения метрик на тестовой выборке для лучших параметров\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial: Trial) -> float:\n",
    "        \"\"\"\n",
    "        Целевая функция для оптимизации.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        trial : Trial\n",
    "            Объект Trial из Optuna для предложения параметров\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        float\n",
    "            Среднее значение NDCG@K на тестовой выборке\n",
    "        \"\"\"\n",
    "        # Определяем пространство поиска гиперпараметров\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "            \"min_child_weight\": trial.suggest_float(\n",
    "                \"min_child_weight\", 1e-3, 10.0, log=True\n",
    "            ),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 1e-3, 10.0, log=True),\n",
    "            \"random_state\": random_state,\n",
    "        }\n",
    "\n",
    "        # Создаем и обучаем модель\n",
    "        model = XGBoostRanker(**params)\n",
    "        model.fit(train_data)\n",
    "\n",
    "        # Получаем рекомендации\n",
    "        recommendations = model.rank(\n",
    "            candidates=candidates, train_data=train_data, top_n=max(k_values)\n",
    "        )\n",
    "\n",
    "        # Преобразуем рекомендации в словарь для расчета метрик\n",
    "        recommendations_dict = (\n",
    "            recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    "        )\n",
    "\n",
    "        # Рассчитываем метрики\n",
    "        metrics = metrics_calculator.calculate(\n",
    "            recommendations=recommendations_dict,\n",
    "            train_data=train_data,\n",
    "            test_data=test_data,\n",
    "        )\n",
    "\n",
    "        return metrics[f\"ndcg_{max(k_values)}\"]\n",
    "\n",
    "    # Создаем исследование Optuna\n",
    "    study_name = f\"xgboost_optimization_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=study_name,\n",
    "        sampler=optuna.samplers.TPESampler(seed=random_state),\n",
    "    )\n",
    "\n",
    "    # Запускаем оптимизацию\n",
    "    study.optimize(\n",
    "        objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # Получаем лучшие параметры\n",
    "    best_params = study.best_params\n",
    "\n",
    "    # Обучаем модель на полных данных с лучшими параметрами\n",
    "    best_model = XGBoostRanker(\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        max_depth=best_params[\"max_depth\"],\n",
    "        n_estimators=best_params[\"n_estimators\"],\n",
    "        min_child_weight=best_params[\"min_child_weight\"],\n",
    "        subsample=best_params[\"subsample\"],\n",
    "        colsample_bytree=best_params[\"colsample_bytree\"],\n",
    "        gamma=best_params[\"gamma\"],\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    best_model.fit(train_data)\n",
    "\n",
    "    # Получаем рекомендации\n",
    "    recommendations = best_model.rank(\n",
    "        candidates=candidates, train_data=train_data, top_n=max(k_values)\n",
    "    )\n",
    "\n",
    "    recommendations_dict = (\n",
    "        recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    "    )\n",
    "\n",
    "    # Рассчитываем финальные метрики\n",
    "    final_metrics = metrics_calculator.calculate(\n",
    "        recommendations=recommendations_dict,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,  # используем тестовые данные для финальной оценки\n",
    "        item_categories=item_categories,\n",
    "    )\n",
    "\n",
    "    return best_params, final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 08:25:01,049] A new study created in memory with name: xgboost_optimization_20250426_082501\n",
      "Best trial: 0. Best value: 0.832345:  10%|█         | 1/10 [05:48<52:13, 348.11s/it, 348.11/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 08:30:49,160] Trial 0 finished with value: 0.8323453801529145 and parameters: {'learning_rate': 0.008468008575248327, 'max_depth': 10, 'n_estimators': 233, 'min_child_weight': 0.24810409748678125, 'subsample': 0.5780093202212182, 'colsample_bytree': 0.5779972601681014, 'gamma': 0.0017073967431528124}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.832345:  20%|██        | 2/10 [11:14<44:44, 335.55s/it, 674.88/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 08:36:15,923] Trial 1 finished with value: 0.8323453801529145 and parameters: {'learning_rate': 0.13983740016490973, 'max_depth': 7, 'n_estimators': 227, 'min_child_weight': 0.0012087541473056963, 'subsample': 0.9849549260809971, 'colsample_bytree': 0.9162213204002109, 'gamma': 0.0070689749506246055}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.832345:  30%|███       | 3/10 [16:38<38:30, 330.10s/it, 998.49/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 08:41:39,533] Trial 2 finished with value: 0.8323453801529145 and parameters: {'learning_rate': 0.002820996133514492, 'max_depth': 4, 'n_estimators': 126, 'min_child_weight': 0.12561043700013558, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021, 'gamma': 0.2801635158716261}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.832345:  40%|████      | 4/10 [22:13<33:13, 332.18s/it, 1333.85/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 08:47:14,900] Trial 3 finished with value: 0.8323453801529145 and parameters: {'learning_rate': 0.0022158645374549917, 'max_depth': 5, 'n_estimators': 141, 'min_child_weight': 0.06672367170464207, 'subsample': 0.8925879806965068, 'colsample_bytree': 0.5998368910791798, 'gamma': 0.11400863701127326}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.832345:  50%|█████     | 5/10 [27:39<27:29, 329.96s/it, 1659.89/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 08:52:40,934] Trial 4 finished with value: 0.8323453801529145 and parameters: {'learning_rate': 0.029341527565000736, 'max_depth': 3, 'n_estimators': 202, 'min_child_weight': 0.004809461967501573, 'subsample': 0.5325257964926398, 'colsample_bytree': 0.9744427686266666, 'gamma': 7.2866537374910445}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.832345:  60%|██████    | 6/10 [33:14<22:06, 331.61s/it, 1994.69/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 08:58:15,739] Trial 5 finished with value: 0.8323453801529145 and parameters: {'learning_rate': 0.10057690178153984, 'max_depth': 5, 'n_estimators': 74, 'min_child_weight': 0.5456725485601477, 'subsample': 0.7200762468698007, 'colsample_bytree': 0.5610191174223894, 'gamma': 0.09565499215943825}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.832345:  70%|███████   | 7/10 [38:48<16:36, 332.24s/it, 2328.23/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 09:03:49,274] Trial 6 finished with value: 0.8323453801529145 and parameters: {'learning_rate': 0.0012167028814593455, 'max_depth': 10, 'n_estimators': 114, 'min_child_weight': 0.4467752817973907, 'subsample': 0.6558555380447055, 'colsample_bytree': 0.7600340105889054, 'gamma': 0.1537592023548176}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.832345:  80%|████████  | 8/10 [44:21<11:05, 332.62s/it, 2661.65/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 09:09:22,698] Trial 7 finished with value: 0.8323453801529145 and parameters: {'learning_rate': 0.002870165242185818, 'max_depth': 10, 'n_estimators': 244, 'min_child_weight': 5.727904470799623, 'subsample': 0.9474136752138245, 'colsample_bytree': 0.7989499894055425, 'gamma': 4.869640941520899}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.832345:  90%|█████████ | 9/10 [49:41<05:28, 328.75s/it, 2981.90/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 09:14:42,949] Trial 8 finished with value: 0.8323453801529145 and parameters: {'learning_rate': 0.0016565580440884786, 'max_depth': 4, 'n_estimators': 61, 'min_child_weight': 0.02001342062287998, 'subsample': 0.6943386448447411, 'colsample_bytree': 0.6356745158869479, 'gamma': 2.0651425578959257}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.832345: 100%|██████████| 10/10 [55:25<00:00, 332.59s/it, 3325.86/7200 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 09:20:26,906] Trial 9 finished with value: 0.8323453801529145 and parameters: {'learning_rate': 0.0076510536667541975, 'max_depth': 5, 'n_estimators': 186, 'min_child_weight': 0.0036618192203924276, 'subsample': 0.9010984903770198, 'colsample_bytree': 0.5372753218398854, 'gamma': 8.862326508576253}. Best is trial 0 with value: 0.8323453801529145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 09:28:12 - INFO - Лучшие параметры: {'learning_rate': 0.008468008575248327, 'max_depth': 10, 'n_estimators': 233, 'min_child_weight': 0.24810409748678125, 'subsample': 0.5780093202212182, 'colsample_bytree': 0.5779972601681014, 'gamma': 0.0017073967431528124}\n",
      "2025-04-26 09:28:12 - INFO - Результаты:\n",
      "2025-04-26 09:28:12 - INFO - Метрики для K=10:\n",
      "2025-04-26 09:28:12 - INFO - NDCG@10: 0.8398\n",
      "2025-04-26 09:28:12 - INFO - Precision@10: 0.0997\n",
      "2025-04-26 09:28:12 - INFO - Recall@10: 0.2215\n",
      "2025-04-26 09:28:12 - INFO - Diversity@10: 0.0033\n",
      "2025-04-26 09:28:12 - INFO - Novelty@10: 0.7153\n",
      "2025-04-26 09:28:12 - INFO - Serendipity@10: 0.0028\n",
      "2025-04-26 09:28:12 - INFO - --------------------------------\n",
      "2025-04-26 09:28:12 - INFO - Метрики для K=100:\n",
      "2025-04-26 09:28:12 - INFO - NDCG@100: 0.8277\n",
      "2025-04-26 09:28:12 - INFO - Precision@100: 0.0563\n",
      "2025-04-26 09:28:12 - INFO - Recall@100: 0.2472\n",
      "2025-04-26 09:28:12 - INFO - Diversity@100: 0.0123\n",
      "2025-04-26 09:28:12 - INFO - Novelty@100: 0.8362\n",
      "2025-04-26 09:28:12 - INFO - Serendipity@100: 0.0016\n",
      "2025-04-26 09:28:12 - INFO - --------------------------------\n",
      "2025-04-26 09:28:12 - INFO - Метрики для K=1000:\n",
      "2025-04-26 09:28:12 - INFO - NDCG@1000: 0.8323\n",
      "2025-04-26 09:28:12 - INFO - Precision@1000: 0.0388\n",
      "2025-04-26 09:28:12 - INFO - Recall@1000: 0.4242\n",
      "2025-04-26 09:28:12 - INFO - Diversity@1000: 0.0708\n",
      "2025-04-26 09:28:12 - INFO - Novelty@1000: 0.8876\n",
      "2025-04-26 09:28:12 - INFO - Serendipity@1000: 0.0011\n",
      "2025-04-26 09:28:12 - INFO - --------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    n_samples = 10000\n",
    "    # buyer_ids = test_data[\"buyer_id\"].unique()[:n_samples]\n",
    "    buyer_ids = test_data[\"buyer_id\"].unique()\n",
    "\n",
    "    temp_candidates = candidates[candidates[\"buyer_id\"].isin(buyer_ids)].copy()\n",
    "\n",
    "    product_ids = temp_candidates[\"product_id\"].unique()\n",
    "    temp_train_data = train_data[\n",
    "        (train_data[\"buyer_id\"].isin(buyer_ids))\n",
    "        & (train_data[\"product_id\"].isin(product_ids))\n",
    "    ].copy()\n",
    "\n",
    "    temp_test_data = test_data[\n",
    "        (test_data[\"buyer_id\"].isin(buyer_ids))\n",
    "        & (test_data[\"product_id\"].isin(product_ids))\n",
    "    ].copy()\n",
    "\n",
    "    metrics_calculator = MetricsCalculator([10, 100, 1000])\n",
    "\n",
    "    best_params, best_metrics = optimize_xgboost_parameters(\n",
    "        train_data=temp_train_data,\n",
    "        test_data=temp_test_data,\n",
    "        candidates=temp_candidates,\n",
    "        metrics_calculator=metrics_calculator,\n",
    "        item_categories=item_categories,\n",
    "        n_trials=10,\n",
    "        timeout=7200,\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Лучшие параметры: {best_params}\")\n",
    "    logging.info(\"Результаты:\")\n",
    "    for k in metrics_calculator.k_values:\n",
    "        logging.info(f\"Метрики для K={k}:\")\n",
    "        logging.info(f\"NDCG@{k}: {best_metrics[f'ndcg_{k}']:.4f}\")\n",
    "        logging.info(f\"Precision@{k}: {best_metrics[f'precision_{k}']:.4f}\")\n",
    "        logging.info(f\"Recall@{k}: {best_metrics[f'recall_{k}']:.4f}\")\n",
    "        logging.info(f\"Diversity@{k}: {best_metrics[f'diversity_{k}']:.4f}\")\n",
    "        logging.info(f\"Novelty@{k}: {best_metrics[f'novelty_{k}']:.4f}\")\n",
    "        logging.info(f\"Serendipity@{k}: {best_metrics[f'serendipity_{k}']:.4f}\")\n",
    "        logging.info(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение лучших моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 10:52:55 - INFO - Обучение модели MatrixFactorizationRanker...\n",
      "Epoch: 100%|██████████| 23/23 [08:19<00:00, 21.74s/it]\n",
      "2025-04-27 11:01:18 - INFO - Сохранение модели MatrixFactorizationRanker...\n",
      "2025-04-27 11:01:19 - INFO - Загрузка модели MatrixFactorizationRanker...\n",
      "2025-04-27 11:01:19 - INFO - Генерация финальных рекомендаций...\n",
      "2025-04-27 11:02:50 - INFO - Сохранение рекомендаций...\n",
      "2025-04-27 11:09:53 - INFO - Расчет финальных метрик...\n",
      "2025-04-27 11:14:02 - INFO - Результаты:\n",
      "2025-04-27 11:14:02 - INFO - Метрики для K=10:\n",
      "2025-04-27 11:14:02 - INFO - NDCG@10: 0.8367\n",
      "2025-04-27 11:14:02 - INFO - Precision@10: 0.0021\n",
      "2025-04-27 11:14:02 - INFO - Recall@10: 0.0037\n",
      "2025-04-27 11:14:02 - INFO - Diversity@10: 0.0055\n",
      "2025-04-27 11:14:02 - INFO - Novelty@10: 0.9963\n",
      "2025-04-27 11:14:02 - INFO - Serendipity@10: 0.0001\n",
      "2025-04-27 11:14:02 - INFO - --------------------------------\n",
      "2025-04-27 11:14:02 - INFO - Метрики для K=100:\n",
      "2025-04-27 11:14:02 - INFO - NDCG@100: 0.8246\n",
      "2025-04-27 11:14:02 - INFO - Precision@100: 0.0018\n",
      "2025-04-27 11:14:02 - INFO - Recall@100: 0.0142\n",
      "2025-04-27 11:14:02 - INFO - Diversity@100: 0.0224\n",
      "2025-04-27 11:14:02 - INFO - Novelty@100: 0.9858\n",
      "2025-04-27 11:14:02 - INFO - Serendipity@100: 0.0001\n",
      "2025-04-27 11:14:02 - INFO - --------------------------------\n",
      "2025-04-27 11:14:02 - INFO - Метрики для K=1000:\n",
      "2025-04-27 11:14:02 - INFO - NDCG@1000: 0.8291\n",
      "2025-04-27 11:14:02 - INFO - Precision@1000: 0.0024\n",
      "2025-04-27 11:14:02 - INFO - Recall@1000: 0.2632\n",
      "2025-04-27 11:14:02 - INFO - Diversity@1000: 0.0777\n",
      "2025-04-27 11:14:02 - INFO - Novelty@1000: 0.7368\n",
      "2025-04-27 11:14:02 - INFO - Serendipity@1000: 0.0001\n",
      "2025-04-27 11:14:02 - INFO - --------------------------------\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Обучение модели MatrixFactorizationRanker...\")\n",
    "model_params = {\n",
    "    \"n_components\": 184,\n",
    "    \"learning_rate\": 0.36596795884334166,\n",
    "    \"loss\": 'warp-kos',\n",
    "    \"n_epochs\": 23,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "mf_ranker = MatrixFactorizationRanker(**model_params)\n",
    "mf_ranker.fit(train_data)\n",
    "\n",
    "logging.info(\"Сохранение модели MatrixFactorizationRanker...\")\n",
    "model_path = (\n",
    "    f\"{DATA_PATH}/models/matrix_factorization_ranker_{ORGANIZATION_ID}_{PROCESSING_DATE}.joblib\"\n",
    ")\n",
    "mf_ranker.save_model(model_path)\n",
    "\n",
    "logging.info(\"Загрузка модели MatrixFactorizationRanker...\")\n",
    "mf_ranker = MatrixFactorizationRanker.load_model(model_path)\n",
    "\n",
    "logging.info(\"Генерация финальных рекомендаций...\")\n",
    "ranked_recommendations = mf_ranker.rank(\n",
    "    candidates=candidates, top_n=1000,\n",
    ")\n",
    "\n",
    "logging.info(\"Сохранение рекомендаций...\")\n",
    "ranked_recommendations.to_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_mf_recommendations.csv\", index=False\n",
    ")\n",
    "\n",
    "recommendations_dict = (\n",
    "    ranked_recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    ")\n",
    "\n",
    "logging.info(\"Расчет финальных метрик...\")\n",
    "metrics_calculator = MetricsCalculator([10, 100, 1000])\n",
    "final_metrics = metrics_calculator.calculate(\n",
    "    recommendations=recommendations_dict,\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    item_categories=item_categories,\n",
    ")\n",
    "\n",
    "logging.info(\"Результаты:\")\n",
    "for k in metrics_calculator.k_values:\n",
    "    logging.info(f\"Метрики для K={k}:\")\n",
    "    logging.info(f\"NDCG@{k}: {final_metrics[f'ndcg_{k}']:.4f}\")\n",
    "    logging.info(f\"Precision@{k}: {final_metrics[f'precision_{k}']:.4f}\")\n",
    "    logging.info(f\"Recall@{k}: {final_metrics[f'recall_{k}']:.4f}\")\n",
    "    logging.info(f\"Diversity@{k}: {final_metrics[f'diversity_{k}']:.4f}\")\n",
    "    logging.info(f\"Novelty@{k}: {final_metrics[f'novelty_{k}']:.4f}\")\n",
    "    logging.info(f\"Serendipity@{k}: {final_metrics[f'serendipity_{k}']:.4f}\")\n",
    "    logging.info(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:41:17 - INFO - Обучение модели CatBoostRanker...\n",
      "Подготовка отрицательных примеров: 100%|██████████| 601/601 [13:46<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.8790034\tbest: 0.8790034 (0)\ttotal: 6.77s\tremaining: 1h 52m 43s\n",
      "1:\ttest: 0.8790034\tbest: 0.8790034 (0)\ttotal: 9.55s\tremaining: 1h 19m 23s\n",
      "2:\ttest: 0.8790034\tbest: 0.8790034 (0)\ttotal: 16.2s\tremaining: 1h 29m 42s\n",
      "3:\ttest: 0.8790034\tbest: 0.8790034 (0)\ttotal: 18.6s\tremaining: 1h 17m 13s\n",
      "4:\ttest: 0.8790034\tbest: 0.8790034 (0)\ttotal: 21s\tremaining: 1h 9m 29s\n",
      "5:\ttest: 0.8790034\tbest: 0.8790034 (0)\ttotal: 23.5s\tremaining: 1h 4m 56s\n",
      "6:\ttest: 0.8790034\tbest: 0.8790034 (0)\ttotal: 26s\tremaining: 1h 1m 27s\n",
      "7:\ttest: 0.8790034\tbest: 0.8790034 (0)\ttotal: 28.6s\tremaining: 59m 12s\n",
      "8:\ttest: 0.8860134\tbest: 0.8860134 (8)\ttotal: 32.4s\tremaining: 59m 24s\n",
      "9:\ttest: 0.8860530\tbest: 0.8860530 (9)\ttotal: 38.6s\tremaining: 1h 3m 37s\n",
      "10:\ttest: 0.8860526\tbest: 0.8860530 (9)\ttotal: 42.4s\tremaining: 1h 3m 31s\n",
      "11:\ttest: 0.8860620\tbest: 0.8860620 (11)\ttotal: 45s\tremaining: 1h 1m 42s\n",
      "12:\ttest: 0.8860583\tbest: 0.8860620 (11)\ttotal: 47.5s\tremaining: 1h 4s\n",
      "13:\ttest: 0.8860441\tbest: 0.8860620 (11)\ttotal: 51.3s\tremaining: 1h 16s\n",
      "14:\ttest: 0.8860554\tbest: 0.8860620 (11)\ttotal: 53.8s\tremaining: 58m 53s\n",
      "15:\ttest: 0.8860365\tbest: 0.8860620 (11)\ttotal: 56.2s\tremaining: 57m 38s\n",
      "16:\ttest: 0.8860594\tbest: 0.8860620 (11)\ttotal: 58.6s\tremaining: 56m 31s\n",
      "17:\ttest: 0.8861234\tbest: 0.8861234 (17)\ttotal: 1m 1s\tremaining: 55m 35s\n",
      "18:\ttest: 0.8861346\tbest: 0.8861346 (18)\ttotal: 1m 3s\tremaining: 54m 37s\n",
      "19:\ttest: 0.8861395\tbest: 0.8861395 (19)\ttotal: 1m 5s\tremaining: 53m 46s\n",
      "20:\ttest: 0.8861317\tbest: 0.8861395 (19)\ttotal: 1m 8s\tremaining: 53m 3s\n",
      "21:\ttest: 0.8861438\tbest: 0.8861438 (21)\ttotal: 1m 10s\tremaining: 52m 21s\n",
      "22:\ttest: 0.8861392\tbest: 0.8861438 (21)\ttotal: 1m 13s\tremaining: 51m 42s\n",
      "23:\ttest: 0.8861392\tbest: 0.8861438 (21)\ttotal: 1m 15s\tremaining: 51m 13s\n",
      "24:\ttest: 0.8861437\tbest: 0.8861438 (21)\ttotal: 1m 18s\tremaining: 50m 44s\n",
      "25:\ttest: 0.8861420\tbest: 0.8861438 (21)\ttotal: 1m 20s\tremaining: 50m 17s\n",
      "26:\ttest: 0.8861199\tbest: 0.8861438 (21)\ttotal: 1m 22s\tremaining: 49m 49s\n",
      "27:\ttest: 0.8861199\tbest: 0.8861438 (21)\ttotal: 1m 25s\tremaining: 49m 23s\n",
      "28:\ttest: 0.8861314\tbest: 0.8861438 (21)\ttotal: 1m 27s\tremaining: 48m 58s\n",
      "29:\ttest: 0.8861501\tbest: 0.8861501 (29)\ttotal: 1m 30s\tremaining: 48m 34s\n",
      "30:\ttest: 0.8861428\tbest: 0.8861501 (29)\ttotal: 1m 32s\tremaining: 48m 12s\n",
      "31:\ttest: 0.8861251\tbest: 0.8861501 (29)\ttotal: 1m 34s\tremaining: 47m 48s\n",
      "32:\ttest: 0.8861586\tbest: 0.8861586 (32)\ttotal: 1m 37s\tremaining: 47m 27s\n",
      "33:\ttest: 0.8861328\tbest: 0.8861586 (32)\ttotal: 1m 39s\tremaining: 47m 6s\n",
      "34:\ttest: 0.8861259\tbest: 0.8861586 (32)\ttotal: 1m 41s\tremaining: 46m 47s\n",
      "35:\ttest: 0.8861594\tbest: 0.8861594 (35)\ttotal: 1m 44s\tremaining: 46m 29s\n",
      "36:\ttest: 0.8861594\tbest: 0.8861594 (35)\ttotal: 1m 46s\tremaining: 46m 12s\n",
      "37:\ttest: 0.8861853\tbest: 0.8861853 (37)\ttotal: 1m 48s\tremaining: 45m 57s\n",
      "38:\ttest: 0.8861444\tbest: 0.8861853 (37)\ttotal: 1m 51s\tremaining: 45m 42s\n",
      "39:\ttest: 0.8861839\tbest: 0.8861853 (37)\ttotal: 1m 53s\tremaining: 45m 27s\n",
      "40:\ttest: 0.8863209\tbest: 0.8863209 (40)\ttotal: 1m 56s\tremaining: 45m 13s\n",
      "41:\ttest: 0.8863273\tbest: 0.8863273 (41)\ttotal: 1m 58s\tremaining: 45m 2s\n",
      "42:\ttest: 0.8863122\tbest: 0.8863273 (41)\ttotal: 2m\tremaining: 44m 49s\n",
      "43:\ttest: 0.8864175\tbest: 0.8864175 (43)\ttotal: 2m 3s\tremaining: 44m 37s\n",
      "44:\ttest: 0.8864198\tbest: 0.8864198 (44)\ttotal: 2m 5s\tremaining: 44m 25s\n",
      "45:\ttest: 0.8864309\tbest: 0.8864309 (45)\ttotal: 2m 8s\tremaining: 44m 15s\n",
      "46:\ttest: 0.8869285\tbest: 0.8869285 (46)\ttotal: 2m 10s\tremaining: 44m 4s\n",
      "47:\ttest: 0.8870412\tbest: 0.8870412 (47)\ttotal: 2m 12s\tremaining: 43m 57s\n",
      "48:\ttest: 0.8871953\tbest: 0.8871953 (48)\ttotal: 2m 15s\tremaining: 43m 47s\n",
      "49:\ttest: 0.8872003\tbest: 0.8872003 (49)\ttotal: 2m 17s\tremaining: 43m 36s\n",
      "50:\ttest: 0.8874394\tbest: 0.8874394 (50)\ttotal: 2m 21s\tremaining: 43m 46s\n",
      "51:\ttest: 0.8874415\tbest: 0.8874415 (51)\ttotal: 2m 23s\tremaining: 43m 36s\n",
      "52:\ttest: 0.8874594\tbest: 0.8874594 (52)\ttotal: 2m 26s\tremaining: 43m 45s\n",
      "53:\ttest: 0.8874291\tbest: 0.8874594 (52)\ttotal: 2m 29s\tremaining: 43m 34s\n",
      "54:\ttest: 0.8874610\tbest: 0.8874610 (54)\ttotal: 2m 31s\tremaining: 43m 27s\n",
      "55:\ttest: 0.8873461\tbest: 0.8874610 (54)\ttotal: 2m 34s\tremaining: 43m 16s\n",
      "56:\ttest: 0.8874909\tbest: 0.8874909 (56)\ttotal: 2m 36s\tremaining: 43m 7s\n",
      "57:\ttest: 0.8874921\tbest: 0.8874921 (57)\ttotal: 2m 38s\tremaining: 42m 57s\n",
      "58:\ttest: 0.8876141\tbest: 0.8876141 (58)\ttotal: 2m 42s\tremaining: 43m 9s\n",
      "59:\ttest: 0.8875776\tbest: 0.8876141 (58)\ttotal: 2m 44s\tremaining: 43m\n",
      "60:\ttest: 0.8876772\tbest: 0.8876772 (60)\ttotal: 2m 47s\tremaining: 42m 53s\n",
      "61:\ttest: 0.8876788\tbest: 0.8876788 (61)\ttotal: 2m 49s\tremaining: 42m 42s\n",
      "62:\ttest: 0.8876834\tbest: 0.8876834 (62)\ttotal: 2m 51s\tremaining: 42m 33s\n",
      "63:\ttest: 0.8876834\tbest: 0.8876834 (62)\ttotal: 2m 53s\tremaining: 42m 23s\n",
      "64:\ttest: 0.8876536\tbest: 0.8876834 (62)\ttotal: 2m 56s\tremaining: 42m 13s\n",
      "65:\ttest: 0.8876798\tbest: 0.8876834 (62)\ttotal: 2m 58s\tremaining: 42m 5s\n",
      "66:\ttest: 0.8876855\tbest: 0.8876855 (66)\ttotal: 3m 1s\tremaining: 42m 6s\n",
      "67:\ttest: 0.8876926\tbest: 0.8876926 (67)\ttotal: 3m 3s\tremaining: 41m 57s\n",
      "68:\ttest: 0.8877266\tbest: 0.8877266 (68)\ttotal: 3m 6s\tremaining: 41m 51s\n",
      "69:\ttest: 0.8877217\tbest: 0.8877266 (68)\ttotal: 3m 8s\tremaining: 41m 44s\n",
      "70:\ttest: 0.8877262\tbest: 0.8877266 (68)\ttotal: 3m 11s\tremaining: 41m 44s\n",
      "71:\ttest: 0.8877167\tbest: 0.8877266 (68)\ttotal: 3m 13s\tremaining: 41m 36s\n",
      "72:\ttest: 0.8877167\tbest: 0.8877266 (68)\ttotal: 3m 16s\tremaining: 41m 29s\n",
      "73:\ttest: 0.8877157\tbest: 0.8877266 (68)\ttotal: 3m 18s\tremaining: 41m 22s\n",
      "74:\ttest: 0.8876760\tbest: 0.8877266 (68)\ttotal: 3m 20s\tremaining: 41m 14s\n",
      "75:\ttest: 0.8877049\tbest: 0.8877266 (68)\ttotal: 3m 22s\tremaining: 41m 7s\n",
      "76:\ttest: 0.8877152\tbest: 0.8877266 (68)\ttotal: 3m 25s\tremaining: 41m\n",
      "77:\ttest: 0.8877028\tbest: 0.8877266 (68)\ttotal: 3m 27s\tremaining: 40m 51s\n",
      "78:\ttest: 0.8876687\tbest: 0.8877266 (68)\ttotal: 3m 29s\tremaining: 40m 43s\n",
      "79:\ttest: 0.8876705\tbest: 0.8877266 (68)\ttotal: 3m 31s\tremaining: 40m 37s\n",
      "80:\ttest: 0.8876687\tbest: 0.8877266 (68)\ttotal: 3m 34s\tremaining: 40m 31s\n",
      "81:\ttest: 0.8876716\tbest: 0.8877266 (68)\ttotal: 3m 36s\tremaining: 40m 24s\n",
      "82:\ttest: 0.8876825\tbest: 0.8877266 (68)\ttotal: 3m 38s\tremaining: 40m 18s\n",
      "83:\ttest: 0.8876830\tbest: 0.8877266 (68)\ttotal: 3m 41s\tremaining: 40m 11s\n",
      "84:\ttest: 0.8876899\tbest: 0.8877266 (68)\ttotal: 3m 43s\tremaining: 40m 6s\n",
      "85:\ttest: 0.8876963\tbest: 0.8877266 (68)\ttotal: 3m 45s\tremaining: 39m 59s\n",
      "86:\ttest: 0.8876963\tbest: 0.8877266 (68)\ttotal: 3m 48s\tremaining: 39m 53s\n",
      "87:\ttest: 0.8876962\tbest: 0.8877266 (68)\ttotal: 3m 50s\tremaining: 39m 48s\n",
      "88:\ttest: 0.8877011\tbest: 0.8877266 (68)\ttotal: 3m 52s\tremaining: 39m 42s\n",
      "89:\ttest: 0.8876922\tbest: 0.8877266 (68)\ttotal: 3m 55s\tremaining: 39m 37s\n",
      "90:\ttest: 0.8877137\tbest: 0.8877266 (68)\ttotal: 3m 57s\tremaining: 39m 30s\n",
      "91:\ttest: 0.8877137\tbest: 0.8877266 (68)\ttotal: 3m 59s\tremaining: 39m 24s\n",
      "92:\ttest: 0.8877648\tbest: 0.8877648 (92)\ttotal: 4m 2s\tremaining: 39m 20s\n",
      "93:\ttest: 0.8877471\tbest: 0.8877648 (92)\ttotal: 4m 4s\tremaining: 39m 15s\n",
      "94:\ttest: 0.8877439\tbest: 0.8877648 (92)\ttotal: 4m 6s\tremaining: 39m 8s\n",
      "95:\ttest: 0.8877604\tbest: 0.8877648 (92)\ttotal: 4m 9s\tremaining: 39m 8s\n",
      "96:\ttest: 0.8877707\tbest: 0.8877707 (96)\ttotal: 4m 11s\tremaining: 39m 2s\n",
      "97:\ttest: 0.8877707\tbest: 0.8877707 (96)\ttotal: 4m 13s\tremaining: 38m 56s\n",
      "98:\ttest: 0.8877707\tbest: 0.8877707 (96)\ttotal: 4m 16s\tremaining: 38m 51s\n",
      "99:\ttest: 0.8877707\tbest: 0.8877707 (99)\ttotal: 4m 18s\tremaining: 38m 46s\n",
      "100:\ttest: 0.8877717\tbest: 0.8877717 (100)\ttotal: 4m 20s\tremaining: 38m 40s\n",
      "101:\ttest: 0.8877717\tbest: 0.8877717 (100)\ttotal: 4m 23s\tremaining: 38m 36s\n",
      "102:\ttest: 0.8877708\tbest: 0.8877717 (100)\ttotal: 4m 25s\tremaining: 38m 29s\n",
      "103:\ttest: 0.8878027\tbest: 0.8878027 (103)\ttotal: 4m 28s\tremaining: 38m 28s\n",
      "104:\ttest: 0.8878187\tbest: 0.8878187 (104)\ttotal: 4m 31s\tremaining: 38m 32s\n",
      "105:\ttest: 0.8878441\tbest: 0.8878441 (105)\ttotal: 4m 34s\tremaining: 38m 33s\n",
      "106:\ttest: 0.8877816\tbest: 0.8878441 (105)\ttotal: 4m 37s\tremaining: 38m 37s\n",
      "107:\ttest: 0.8877672\tbest: 0.8878441 (105)\ttotal: 4m 39s\tremaining: 38m 32s\n",
      "108:\ttest: 0.8877675\tbest: 0.8878441 (105)\ttotal: 4m 42s\tremaining: 38m 26s\n",
      "109:\ttest: 0.8877924\tbest: 0.8878441 (105)\ttotal: 4m 44s\tremaining: 38m 22s\n",
      "110:\ttest: 0.8877828\tbest: 0.8878441 (105)\ttotal: 4m 46s\tremaining: 38m 16s\n",
      "111:\ttest: 0.8877798\tbest: 0.8878441 (105)\ttotal: 4m 49s\tremaining: 38m 11s\n",
      "112:\ttest: 0.8877810\tbest: 0.8878441 (105)\ttotal: 4m 51s\tremaining: 38m 6s\n",
      "113:\ttest: 0.8877767\tbest: 0.8878441 (105)\ttotal: 4m 54s\tremaining: 38m 12s\n",
      "114:\ttest: 0.8877738\tbest: 0.8878441 (105)\ttotal: 4m 57s\tremaining: 38m 6s\n",
      "115:\ttest: 0.8877589\tbest: 0.8878441 (105)\ttotal: 4m 59s\tremaining: 38m 1s\n",
      "116:\ttest: 0.8877480\tbest: 0.8878441 (105)\ttotal: 5m 1s\tremaining: 37m 56s\n",
      "117:\ttest: 0.8877504\tbest: 0.8878441 (105)\ttotal: 5m 4s\tremaining: 37m 52s\n",
      "118:\ttest: 0.8877594\tbest: 0.8878441 (105)\ttotal: 5m 7s\tremaining: 37m 55s\n",
      "119:\ttest: 0.8877607\tbest: 0.8878441 (105)\ttotal: 5m 9s\tremaining: 37m 50s\n",
      "120:\ttest: 0.8877537\tbest: 0.8878441 (105)\ttotal: 5m 12s\tremaining: 37m 50s\n",
      "121:\ttest: 0.8877726\tbest: 0.8878441 (105)\ttotal: 5m 16s\tremaining: 37m 55s\n",
      "122:\ttest: 0.8880627\tbest: 0.8880627 (122)\ttotal: 5m 21s\tremaining: 38m 10s\n",
      "123:\ttest: 0.8880496\tbest: 0.8880627 (122)\ttotal: 5m 24s\tremaining: 38m 13s\n",
      "124:\ttest: 0.8880456\tbest: 0.8880627 (122)\ttotal: 5m 27s\tremaining: 38m 11s\n",
      "125:\ttest: 0.8880747\tbest: 0.8880747 (125)\ttotal: 5m 30s\tremaining: 38m 15s\n",
      "126:\ttest: 0.8880697\tbest: 0.8880747 (125)\ttotal: 5m 34s\tremaining: 38m 19s\n",
      "127:\ttest: 0.8879453\tbest: 0.8880747 (125)\ttotal: 5m 39s\tremaining: 38m 32s\n",
      "128:\ttest: 0.8879284\tbest: 0.8880747 (125)\ttotal: 5m 42s\tremaining: 38m 34s\n",
      "129:\ttest: 0.8879302\tbest: 0.8880747 (125)\ttotal: 5m 48s\tremaining: 38m 49s\n",
      "130:\ttest: 0.8879330\tbest: 0.8880747 (125)\ttotal: 5m 51s\tremaining: 38m 50s\n",
      "131:\ttest: 0.8879251\tbest: 0.8880747 (125)\ttotal: 5m 53s\tremaining: 38m 44s\n",
      "132:\ttest: 0.8880113\tbest: 0.8880747 (125)\ttotal: 5m 56s\tremaining: 38m 45s\n",
      "133:\ttest: 0.8880314\tbest: 0.8880747 (125)\ttotal: 5m 58s\tremaining: 38m 39s\n",
      "134:\ttest: 0.8880183\tbest: 0.8880747 (125)\ttotal: 6m 1s\tremaining: 38m 33s\n",
      "135:\ttest: 0.8880239\tbest: 0.8880747 (125)\ttotal: 6m 3s\tremaining: 38m 28s\n",
      "136:\ttest: 0.8880069\tbest: 0.8880747 (125)\ttotal: 6m 5s\tremaining: 38m 22s\n",
      "137:\ttest: 0.8880223\tbest: 0.8880747 (125)\ttotal: 6m 7s\tremaining: 38m 16s\n",
      "138:\ttest: 0.8900370\tbest: 0.8900370 (138)\ttotal: 6m 11s\tremaining: 38m 19s\n",
      "139:\ttest: 0.8956805\tbest: 0.8956805 (139)\ttotal: 6m 14s\tremaining: 38m 23s\n",
      "140:\ttest: 0.8962174\tbest: 0.8962174 (140)\ttotal: 6m 17s\tremaining: 38m 17s\n",
      "141:\ttest: 0.8969163\tbest: 0.8969163 (141)\ttotal: 6m 19s\tremaining: 38m 11s\n",
      "142:\ttest: 0.8995194\tbest: 0.8995194 (142)\ttotal: 6m 22s\tremaining: 38m 12s\n",
      "143:\ttest: 0.8996217\tbest: 0.8996217 (143)\ttotal: 6m 25s\tremaining: 38m 13s\n",
      "144:\ttest: 0.8996938\tbest: 0.8996938 (144)\ttotal: 6m 28s\tremaining: 38m 8s\n",
      "145:\ttest: 0.8997140\tbest: 0.8997140 (145)\ttotal: 6m 30s\tremaining: 38m 2s\n",
      "146:\ttest: 0.8997343\tbest: 0.8997343 (146)\ttotal: 6m 32s\tremaining: 37m 57s\n",
      "147:\ttest: 0.8997460\tbest: 0.8997460 (147)\ttotal: 6m 35s\tremaining: 37m 54s\n",
      "148:\ttest: 0.8997774\tbest: 0.8997774 (148)\ttotal: 6m 37s\tremaining: 37m 48s\n",
      "149:\ttest: 0.8998756\tbest: 0.8998756 (149)\ttotal: 6m 39s\tremaining: 37m 43s\n",
      "150:\ttest: 0.8999805\tbest: 0.8999805 (150)\ttotal: 6m 41s\tremaining: 37m 38s\n",
      "151:\ttest: 0.9000912\tbest: 0.9000912 (151)\ttotal: 6m 43s\tremaining: 37m 32s\n",
      "152:\ttest: 0.9001724\tbest: 0.9001724 (152)\ttotal: 6m 46s\tremaining: 37m 27s\n",
      "153:\ttest: 0.9003502\tbest: 0.9003502 (153)\ttotal: 6m 48s\tremaining: 37m 21s\n",
      "154:\ttest: 0.9005252\tbest: 0.9005252 (154)\ttotal: 6m 50s\tremaining: 37m 16s\n",
      "155:\ttest: 0.9007001\tbest: 0.9007001 (155)\ttotal: 6m 52s\tremaining: 37m 10s\n",
      "156:\ttest: 0.9009033\tbest: 0.9009033 (156)\ttotal: 6m 54s\tremaining: 37m 5s\n",
      "157:\ttest: 0.9011681\tbest: 0.9011681 (157)\ttotal: 6m 56s\tremaining: 37m\n",
      "158:\ttest: 0.9014489\tbest: 0.9014489 (158)\ttotal: 6m 58s\tremaining: 36m 55s\n",
      "159:\ttest: 0.9024681\tbest: 0.9024681 (159)\ttotal: 7m\tremaining: 36m 50s\n",
      "160:\ttest: 0.9028454\tbest: 0.9028454 (160)\ttotal: 7m 3s\tremaining: 36m 44s\n",
      "161:\ttest: 0.9029826\tbest: 0.9029826 (161)\ttotal: 7m 5s\tremaining: 36m 39s\n",
      "162:\ttest: 0.9030819\tbest: 0.9030819 (162)\ttotal: 7m 7s\tremaining: 36m 34s\n",
      "163:\ttest: 0.9032034\tbest: 0.9032034 (163)\ttotal: 7m 9s\tremaining: 36m 28s\n",
      "164:\ttest: 0.9033272\tbest: 0.9033272 (164)\ttotal: 7m 11s\tremaining: 36m 23s\n",
      "165:\ttest: 0.9036137\tbest: 0.9036137 (165)\ttotal: 7m 13s\tremaining: 36m 18s\n",
      "166:\ttest: 0.9037092\tbest: 0.9037092 (166)\ttotal: 7m 15s\tremaining: 36m 13s\n",
      "167:\ttest: 0.9038122\tbest: 0.9038122 (167)\ttotal: 7m 18s\tremaining: 36m 12s\n",
      "168:\ttest: 0.9041671\tbest: 0.9041671 (168)\ttotal: 7m 20s\tremaining: 36m 6s\n",
      "169:\ttest: 0.9042470\tbest: 0.9042470 (169)\ttotal: 7m 22s\tremaining: 36m 2s\n",
      "170:\ttest: 0.9043520\tbest: 0.9043520 (170)\ttotal: 7m 24s\tremaining: 35m 57s\n",
      "171:\ttest: 0.9044318\tbest: 0.9044318 (171)\ttotal: 7m 27s\tremaining: 35m 52s\n",
      "172:\ttest: 0.9045066\tbest: 0.9045066 (172)\ttotal: 7m 29s\tremaining: 35m 47s\n",
      "173:\ttest: 0.9046755\tbest: 0.9046755 (173)\ttotal: 7m 31s\tremaining: 35m 42s\n",
      "174:\ttest: 0.9047270\tbest: 0.9047270 (174)\ttotal: 7m 33s\tremaining: 35m 38s\n",
      "175:\ttest: 0.9048701\tbest: 0.9048701 (175)\ttotal: 7m 35s\tremaining: 35m 34s\n",
      "176:\ttest: 0.9049782\tbest: 0.9049782 (176)\ttotal: 7m 38s\tremaining: 35m 30s\n",
      "177:\ttest: 0.9051307\tbest: 0.9051307 (177)\ttotal: 7m 40s\tremaining: 35m 25s\n",
      "178:\ttest: 0.9051644\tbest: 0.9051644 (178)\ttotal: 7m 42s\tremaining: 35m 21s\n",
      "179:\ttest: 0.9052856\tbest: 0.9052856 (179)\ttotal: 7m 44s\tremaining: 35m 15s\n",
      "180:\ttest: 0.9053837\tbest: 0.9053837 (180)\ttotal: 7m 46s\tremaining: 35m 10s\n",
      "181:\ttest: 0.9054860\tbest: 0.9054860 (181)\ttotal: 7m 48s\tremaining: 35m 6s\n",
      "182:\ttest: 0.9055092\tbest: 0.9055092 (182)\ttotal: 7m 50s\tremaining: 35m 2s\n",
      "183:\ttest: 0.9055334\tbest: 0.9055334 (183)\ttotal: 7m 53s\tremaining: 34m 58s\n",
      "184:\ttest: 0.9057090\tbest: 0.9057090 (184)\ttotal: 7m 55s\tremaining: 34m 53s\n",
      "185:\ttest: 0.9062141\tbest: 0.9062141 (185)\ttotal: 7m 57s\tremaining: 34m 49s\n",
      "186:\ttest: 0.9062921\tbest: 0.9062921 (186)\ttotal: 7m 59s\tremaining: 34m 44s\n",
      "187:\ttest: 0.9063525\tbest: 0.9063525 (187)\ttotal: 8m 1s\tremaining: 34m 40s\n",
      "188:\ttest: 0.9063963\tbest: 0.9063963 (188)\ttotal: 8m 3s\tremaining: 34m 36s\n",
      "189:\ttest: 0.9064659\tbest: 0.9064659 (189)\ttotal: 8m 6s\tremaining: 34m 32s\n",
      "190:\ttest: 0.9064777\tbest: 0.9064777 (190)\ttotal: 8m 8s\tremaining: 34m 28s\n",
      "191:\ttest: 0.9064946\tbest: 0.9064946 (191)\ttotal: 8m 10s\tremaining: 34m 24s\n",
      "192:\ttest: 0.9065500\tbest: 0.9065500 (192)\ttotal: 8m 12s\tremaining: 34m 19s\n",
      "193:\ttest: 0.9065994\tbest: 0.9065994 (193)\ttotal: 8m 14s\tremaining: 34m 15s\n",
      "194:\ttest: 0.9066051\tbest: 0.9066051 (194)\ttotal: 8m 17s\tremaining: 34m 12s\n",
      "195:\ttest: 0.9066811\tbest: 0.9066811 (195)\ttotal: 8m 19s\tremaining: 34m 8s\n",
      "196:\ttest: 0.9066982\tbest: 0.9066982 (196)\ttotal: 8m 21s\tremaining: 34m 4s\n",
      "197:\ttest: 0.9066966\tbest: 0.9066982 (196)\ttotal: 8m 23s\tremaining: 34m\n",
      "198:\ttest: 0.9066859\tbest: 0.9066982 (196)\ttotal: 8m 26s\tremaining: 33m 57s\n",
      "199:\ttest: 0.9067037\tbest: 0.9067037 (199)\ttotal: 8m 28s\tremaining: 33m 53s\n",
      "200:\ttest: 0.9067338\tbest: 0.9067338 (200)\ttotal: 8m 30s\tremaining: 33m 48s\n",
      "201:\ttest: 0.9067665\tbest: 0.9067665 (201)\ttotal: 8m 32s\tremaining: 33m 44s\n",
      "202:\ttest: 0.9067701\tbest: 0.9067701 (202)\ttotal: 8m 34s\tremaining: 33m 40s\n",
      "203:\ttest: 0.9068019\tbest: 0.9068019 (203)\ttotal: 8m 36s\tremaining: 33m 36s\n",
      "204:\ttest: 0.9068638\tbest: 0.9068638 (204)\ttotal: 8m 39s\tremaining: 33m 32s\n",
      "205:\ttest: 0.9069136\tbest: 0.9069136 (205)\ttotal: 8m 41s\tremaining: 33m 28s\n",
      "206:\ttest: 0.9069251\tbest: 0.9069251 (206)\ttotal: 8m 43s\tremaining: 33m 25s\n",
      "207:\ttest: 0.9069582\tbest: 0.9069582 (207)\ttotal: 8m 45s\tremaining: 33m 21s\n",
      "208:\ttest: 0.9069630\tbest: 0.9069630 (208)\ttotal: 8m 47s\tremaining: 33m 18s\n",
      "209:\ttest: 0.9069652\tbest: 0.9069652 (209)\ttotal: 8m 50s\tremaining: 33m 14s\n",
      "210:\ttest: 0.9069880\tbest: 0.9069880 (210)\ttotal: 8m 52s\tremaining: 33m 10s\n",
      "211:\ttest: 0.9070661\tbest: 0.9070661 (211)\ttotal: 8m 54s\tremaining: 33m 6s\n",
      "212:\ttest: 0.9070932\tbest: 0.9070932 (212)\ttotal: 8m 56s\tremaining: 33m 2s\n",
      "213:\ttest: 0.9071127\tbest: 0.9071127 (213)\ttotal: 8m 58s\tremaining: 32m 58s\n",
      "214:\ttest: 0.9071485\tbest: 0.9071485 (214)\ttotal: 9m\tremaining: 32m 54s\n",
      "215:\ttest: 0.9071802\tbest: 0.9071802 (215)\ttotal: 9m 3s\tremaining: 32m 51s\n",
      "216:\ttest: 0.9072302\tbest: 0.9072302 (216)\ttotal: 9m 5s\tremaining: 32m 47s\n",
      "217:\ttest: 0.9072596\tbest: 0.9072596 (217)\ttotal: 9m 7s\tremaining: 32m 44s\n",
      "218:\ttest: 0.9073066\tbest: 0.9073066 (218)\ttotal: 9m 9s\tremaining: 32m 40s\n",
      "219:\ttest: 0.9073230\tbest: 0.9073230 (219)\ttotal: 9m 11s\tremaining: 32m 36s\n",
      "220:\ttest: 0.9073461\tbest: 0.9073461 (220)\ttotal: 9m 14s\tremaining: 32m 33s\n",
      "221:\ttest: 0.9073753\tbest: 0.9073753 (221)\ttotal: 9m 16s\tremaining: 32m 29s\n",
      "222:\ttest: 0.9073917\tbest: 0.9073917 (222)\ttotal: 9m 18s\tremaining: 32m 25s\n",
      "223:\ttest: 0.9073999\tbest: 0.9073999 (223)\ttotal: 9m 20s\tremaining: 32m 22s\n",
      "224:\ttest: 0.9074020\tbest: 0.9074020 (224)\ttotal: 9m 22s\tremaining: 32m 18s\n",
      "225:\ttest: 0.9074370\tbest: 0.9074370 (225)\ttotal: 9m 25s\tremaining: 32m 15s\n",
      "226:\ttest: 0.9074504\tbest: 0.9074504 (226)\ttotal: 9m 27s\tremaining: 32m 11s\n",
      "227:\ttest: 0.9074668\tbest: 0.9074668 (227)\ttotal: 9m 29s\tremaining: 32m 8s\n",
      "228:\ttest: 0.9074767\tbest: 0.9074767 (228)\ttotal: 9m 31s\tremaining: 32m 4s\n",
      "229:\ttest: 0.9074972\tbest: 0.9074972 (229)\ttotal: 9m 33s\tremaining: 32m 1s\n",
      "230:\ttest: 0.9075124\tbest: 0.9075124 (230)\ttotal: 9m 36s\tremaining: 31m 57s\n",
      "231:\ttest: 0.9075137\tbest: 0.9075137 (231)\ttotal: 9m 38s\tremaining: 31m 53s\n",
      "232:\ttest: 0.9075293\tbest: 0.9075293 (232)\ttotal: 9m 40s\tremaining: 31m 50s\n",
      "233:\ttest: 0.9075459\tbest: 0.9075459 (233)\ttotal: 9m 42s\tremaining: 31m 47s\n",
      "234:\ttest: 0.9075502\tbest: 0.9075502 (234)\ttotal: 9m 44s\tremaining: 31m 43s\n",
      "235:\ttest: 0.9075683\tbest: 0.9075683 (235)\ttotal: 9m 46s\tremaining: 31m 40s\n",
      "236:\ttest: 0.9075855\tbest: 0.9075855 (236)\ttotal: 9m 49s\tremaining: 31m 36s\n",
      "237:\ttest: 0.9075934\tbest: 0.9075934 (237)\ttotal: 9m 51s\tremaining: 31m 33s\n",
      "238:\ttest: 0.9075948\tbest: 0.9075948 (238)\ttotal: 9m 53s\tremaining: 31m 29s\n",
      "239:\ttest: 0.9076015\tbest: 0.9076015 (239)\ttotal: 9m 55s\tremaining: 31m 26s\n",
      "240:\ttest: 0.9076016\tbest: 0.9076016 (240)\ttotal: 9m 57s\tremaining: 31m 22s\n",
      "241:\ttest: 0.9076052\tbest: 0.9076052 (241)\ttotal: 9m 59s\tremaining: 31m 18s\n",
      "242:\ttest: 0.9076162\tbest: 0.9076162 (242)\ttotal: 10m 2s\tremaining: 31m 15s\n",
      "243:\ttest: 0.9076174\tbest: 0.9076174 (243)\ttotal: 10m 4s\tremaining: 31m 12s\n",
      "244:\ttest: 0.9076313\tbest: 0.9076313 (244)\ttotal: 10m 6s\tremaining: 31m 8s\n",
      "245:\ttest: 0.9076354\tbest: 0.9076354 (245)\ttotal: 10m 8s\tremaining: 31m 5s\n",
      "246:\ttest: 0.9076423\tbest: 0.9076423 (246)\ttotal: 10m 10s\tremaining: 31m 2s\n",
      "247:\ttest: 0.9076457\tbest: 0.9076457 (247)\ttotal: 10m 13s\tremaining: 30m 58s\n",
      "248:\ttest: 0.9076516\tbest: 0.9076516 (248)\ttotal: 10m 15s\tremaining: 30m 56s\n",
      "249:\ttest: 0.9076638\tbest: 0.9076638 (249)\ttotal: 10m 17s\tremaining: 30m 53s\n",
      "250:\ttest: 0.9076653\tbest: 0.9076653 (250)\ttotal: 10m 19s\tremaining: 30m 50s\n",
      "251:\ttest: 0.9076773\tbest: 0.9076773 (251)\ttotal: 10m 22s\tremaining: 30m 46s\n",
      "252:\ttest: 0.9076862\tbest: 0.9076862 (252)\ttotal: 10m 24s\tremaining: 30m 43s\n",
      "253:\ttest: 0.9076967\tbest: 0.9076967 (253)\ttotal: 10m 26s\tremaining: 30m 40s\n",
      "254:\ttest: 0.9077035\tbest: 0.9077035 (254)\ttotal: 10m 28s\tremaining: 30m 37s\n",
      "255:\ttest: 0.9077103\tbest: 0.9077103 (255)\ttotal: 10m 30s\tremaining: 30m 33s\n",
      "256:\ttest: 0.9077137\tbest: 0.9077137 (256)\ttotal: 10m 33s\tremaining: 30m 30s\n",
      "257:\ttest: 0.9077160\tbest: 0.9077160 (257)\ttotal: 10m 35s\tremaining: 30m 27s\n",
      "258:\ttest: 0.9077234\tbest: 0.9077234 (258)\ttotal: 10m 37s\tremaining: 30m 23s\n",
      "259:\ttest: 0.9077243\tbest: 0.9077243 (259)\ttotal: 10m 39s\tremaining: 30m 20s\n",
      "260:\ttest: 0.9077272\tbest: 0.9077272 (260)\ttotal: 10m 41s\tremaining: 30m 17s\n",
      "261:\ttest: 0.9077401\tbest: 0.9077401 (261)\ttotal: 10m 44s\tremaining: 30m 14s\n",
      "262:\ttest: 0.9077640\tbest: 0.9077640 (262)\ttotal: 10m 46s\tremaining: 30m 10s\n",
      "263:\ttest: 0.9077706\tbest: 0.9077706 (263)\ttotal: 10m 48s\tremaining: 30m 7s\n",
      "264:\ttest: 0.9077850\tbest: 0.9077850 (264)\ttotal: 10m 50s\tremaining: 30m 4s\n",
      "265:\ttest: 0.9077861\tbest: 0.9077861 (265)\ttotal: 10m 52s\tremaining: 30m 1s\n",
      "266:\ttest: 0.9077978\tbest: 0.9077978 (266)\ttotal: 10m 54s\tremaining: 29m 58s\n",
      "267:\ttest: 0.9077996\tbest: 0.9077996 (267)\ttotal: 10m 57s\tremaining: 29m 54s\n",
      "268:\ttest: 0.9078095\tbest: 0.9078095 (268)\ttotal: 10m 59s\tremaining: 29m 51s\n",
      "269:\ttest: 0.9078273\tbest: 0.9078273 (269)\ttotal: 11m 1s\tremaining: 29m 48s\n",
      "270:\ttest: 0.9078357\tbest: 0.9078357 (270)\ttotal: 11m 4s\tremaining: 29m 46s\n",
      "271:\ttest: 0.9078465\tbest: 0.9078465 (271)\ttotal: 11m 6s\tremaining: 29m 42s\n",
      "272:\ttest: 0.9078353\tbest: 0.9078465 (271)\ttotal: 11m 8s\tremaining: 29m 39s\n",
      "273:\ttest: 0.9078392\tbest: 0.9078465 (271)\ttotal: 11m 10s\tremaining: 29m 36s\n",
      "274:\ttest: 0.9078669\tbest: 0.9078669 (274)\ttotal: 11m 12s\tremaining: 29m 33s\n",
      "275:\ttest: 0.9078856\tbest: 0.9078856 (275)\ttotal: 11m 14s\tremaining: 29m 30s\n",
      "276:\ttest: 0.9078897\tbest: 0.9078897 (276)\ttotal: 11m 16s\tremaining: 29m 26s\n",
      "277:\ttest: 0.9079149\tbest: 0.9079149 (277)\ttotal: 11m 18s\tremaining: 29m 23s\n",
      "278:\ttest: 0.9079170\tbest: 0.9079170 (278)\ttotal: 11m 20s\tremaining: 29m 19s\n",
      "279:\ttest: 0.9079223\tbest: 0.9079223 (279)\ttotal: 11m 23s\tremaining: 29m 16s\n",
      "280:\ttest: 0.9079447\tbest: 0.9079447 (280)\ttotal: 11m 25s\tremaining: 29m 13s\n",
      "281:\ttest: 0.9079570\tbest: 0.9079570 (281)\ttotal: 11m 27s\tremaining: 29m 10s\n",
      "282:\ttest: 0.9079555\tbest: 0.9079570 (281)\ttotal: 11m 29s\tremaining: 29m 6s\n",
      "283:\ttest: 0.9079822\tbest: 0.9079822 (283)\ttotal: 11m 31s\tremaining: 29m 3s\n",
      "284:\ttest: 0.9080203\tbest: 0.9080203 (284)\ttotal: 11m 33s\tremaining: 29m\n",
      "285:\ttest: 0.9080338\tbest: 0.9080338 (285)\ttotal: 11m 35s\tremaining: 28m 57s\n",
      "286:\ttest: 0.9080421\tbest: 0.9080421 (286)\ttotal: 11m 38s\tremaining: 28m 54s\n",
      "287:\ttest: 0.9080428\tbest: 0.9080428 (287)\ttotal: 11m 40s\tremaining: 28m 51s\n",
      "288:\ttest: 0.9080414\tbest: 0.9080428 (287)\ttotal: 11m 42s\tremaining: 28m 48s\n",
      "289:\ttest: 0.9080342\tbest: 0.9080428 (287)\ttotal: 11m 44s\tremaining: 28m 45s\n",
      "290:\ttest: 0.9080358\tbest: 0.9080428 (287)\ttotal: 11m 46s\tremaining: 28m 42s\n",
      "291:\ttest: 0.9080499\tbest: 0.9080499 (291)\ttotal: 11m 48s\tremaining: 28m 38s\n",
      "292:\ttest: 0.9080484\tbest: 0.9080499 (291)\ttotal: 11m 51s\tremaining: 28m 35s\n",
      "293:\ttest: 0.9080504\tbest: 0.9080504 (293)\ttotal: 11m 53s\tremaining: 28m 32s\n",
      "294:\ttest: 0.9080372\tbest: 0.9080504 (293)\ttotal: 11m 55s\tremaining: 28m 29s\n",
      "295:\ttest: 0.9080398\tbest: 0.9080504 (293)\ttotal: 11m 57s\tremaining: 28m 26s\n",
      "296:\ttest: 0.9080307\tbest: 0.9080504 (293)\ttotal: 12m 1s\tremaining: 28m 27s\n",
      "297:\ttest: 0.9080353\tbest: 0.9080504 (293)\ttotal: 12m 3s\tremaining: 28m 24s\n",
      "298:\ttest: 0.9081111\tbest: 0.9081111 (298)\ttotal: 12m 6s\tremaining: 28m 23s\n",
      "299:\ttest: 0.9081111\tbest: 0.9081111 (298)\ttotal: 12m 8s\tremaining: 28m 20s\n",
      "300:\ttest: 0.9081139\tbest: 0.9081139 (300)\ttotal: 12m 11s\tremaining: 28m 19s\n",
      "301:\ttest: 0.9081126\tbest: 0.9081139 (300)\ttotal: 12m 14s\tremaining: 28m 16s\n",
      "302:\ttest: 0.9081233\tbest: 0.9081233 (302)\ttotal: 12m 16s\tremaining: 28m 13s\n",
      "303:\ttest: 0.9081162\tbest: 0.9081233 (302)\ttotal: 12m 18s\tremaining: 28m 10s\n",
      "304:\ttest: 0.9081229\tbest: 0.9081233 (302)\ttotal: 12m 20s\tremaining: 28m 6s\n",
      "305:\ttest: 0.9081340\tbest: 0.9081340 (305)\ttotal: 12m 22s\tremaining: 28m 3s\n",
      "306:\ttest: 0.9081396\tbest: 0.9081396 (306)\ttotal: 12m 24s\tremaining: 28m\n",
      "307:\ttest: 0.9081402\tbest: 0.9081402 (307)\ttotal: 12m 26s\tremaining: 27m 57s\n",
      "308:\ttest: 0.9081386\tbest: 0.9081402 (307)\ttotal: 12m 28s\tremaining: 27m 54s\n",
      "309:\ttest: 0.9081391\tbest: 0.9081402 (307)\ttotal: 12m 30s\tremaining: 27m 51s\n",
      "310:\ttest: 0.9081461\tbest: 0.9081461 (310)\ttotal: 12m 33s\tremaining: 27m 48s\n",
      "311:\ttest: 0.9081448\tbest: 0.9081461 (310)\ttotal: 12m 35s\tremaining: 27m 45s\n",
      "312:\ttest: 0.9081581\tbest: 0.9081581 (312)\ttotal: 12m 37s\tremaining: 27m 42s\n",
      "313:\ttest: 0.9081288\tbest: 0.9081581 (312)\ttotal: 12m 39s\tremaining: 27m 39s\n",
      "314:\ttest: 0.9081362\tbest: 0.9081581 (312)\ttotal: 12m 41s\tremaining: 27m 36s\n",
      "315:\ttest: 0.9081285\tbest: 0.9081581 (312)\ttotal: 12m 43s\tremaining: 27m 32s\n",
      "316:\ttest: 0.9081386\tbest: 0.9081581 (312)\ttotal: 12m 45s\tremaining: 27m 29s\n",
      "317:\ttest: 0.9081333\tbest: 0.9081581 (312)\ttotal: 12m 47s\tremaining: 27m 26s\n",
      "318:\ttest: 0.9081509\tbest: 0.9081581 (312)\ttotal: 12m 49s\tremaining: 27m 23s\n",
      "319:\ttest: 0.9081418\tbest: 0.9081581 (312)\ttotal: 12m 51s\tremaining: 27m 20s\n",
      "320:\ttest: 0.9081347\tbest: 0.9081581 (312)\ttotal: 12m 53s\tremaining: 27m 16s\n",
      "321:\ttest: 0.9081297\tbest: 0.9081581 (312)\ttotal: 12m 55s\tremaining: 27m 13s\n",
      "322:\ttest: 0.9081424\tbest: 0.9081581 (312)\ttotal: 12m 57s\tremaining: 27m 10s\n",
      "323:\ttest: 0.9081537\tbest: 0.9081581 (312)\ttotal: 13m\tremaining: 27m 7s\n",
      "324:\ttest: 0.9085088\tbest: 0.9085088 (324)\ttotal: 13m 2s\tremaining: 27m 5s\n",
      "325:\ttest: 0.9085091\tbest: 0.9085091 (325)\ttotal: 13m 4s\tremaining: 27m 2s\n",
      "326:\ttest: 0.9085109\tbest: 0.9085109 (326)\ttotal: 13m 6s\tremaining: 26m 59s\n",
      "327:\ttest: 0.9085395\tbest: 0.9085395 (327)\ttotal: 13m 8s\tremaining: 26m 56s\n",
      "328:\ttest: 0.9085368\tbest: 0.9085395 (327)\ttotal: 13m 11s\tremaining: 26m 53s\n",
      "329:\ttest: 0.9085363\tbest: 0.9085395 (327)\ttotal: 13m 13s\tremaining: 26m 50s\n",
      "330:\ttest: 0.9085392\tbest: 0.9085395 (327)\ttotal: 13m 15s\tremaining: 26m 47s\n",
      "331:\ttest: 0.9085407\tbest: 0.9085407 (331)\ttotal: 13m 17s\tremaining: 26m 44s\n",
      "332:\ttest: 0.9087113\tbest: 0.9087113 (332)\ttotal: 13m 20s\tremaining: 26m 43s\n",
      "333:\ttest: 0.9087151\tbest: 0.9087151 (333)\ttotal: 13m 22s\tremaining: 26m 41s\n",
      "334:\ttest: 0.9087126\tbest: 0.9087151 (333)\ttotal: 13m 25s\tremaining: 26m 38s\n",
      "335:\ttest: 0.9087083\tbest: 0.9087151 (333)\ttotal: 13m 28s\tremaining: 26m 38s\n",
      "336:\ttest: 0.9087144\tbest: 0.9087151 (333)\ttotal: 13m 31s\tremaining: 26m 35s\n",
      "337:\ttest: 0.9087133\tbest: 0.9087151 (333)\ttotal: 13m 33s\tremaining: 26m 32s\n",
      "338:\ttest: 0.9087126\tbest: 0.9087151 (333)\ttotal: 13m 35s\tremaining: 26m 29s\n",
      "339:\ttest: 0.9087135\tbest: 0.9087151 (333)\ttotal: 13m 37s\tremaining: 26m 26s\n",
      "340:\ttest: 0.9087111\tbest: 0.9087151 (333)\ttotal: 13m 39s\tremaining: 26m 23s\n",
      "341:\ttest: 0.9087053\tbest: 0.9087151 (333)\ttotal: 13m 41s\tremaining: 26m 20s\n",
      "342:\ttest: 0.9086989\tbest: 0.9087151 (333)\ttotal: 13m 43s\tremaining: 26m 17s\n",
      "343:\ttest: 0.9087182\tbest: 0.9087182 (343)\ttotal: 13m 45s\tremaining: 26m 14s\n",
      "344:\ttest: 0.9087233\tbest: 0.9087233 (344)\ttotal: 13m 48s\tremaining: 26m 12s\n",
      "345:\ttest: 0.9087259\tbest: 0.9087259 (345)\ttotal: 13m 50s\tremaining: 26m 9s\n",
      "346:\ttest: 0.9087256\tbest: 0.9087259 (345)\ttotal: 13m 52s\tremaining: 26m 6s\n",
      "347:\ttest: 0.9087296\tbest: 0.9087296 (347)\ttotal: 13m 55s\tremaining: 26m 5s\n",
      "348:\ttest: 0.9087334\tbest: 0.9087334 (348)\ttotal: 13m 57s\tremaining: 26m 2s\n",
      "349:\ttest: 0.9087286\tbest: 0.9087334 (348)\ttotal: 13m 59s\tremaining: 25m 59s\n",
      "350:\ttest: 0.9087352\tbest: 0.9087352 (350)\ttotal: 14m 1s\tremaining: 25m 56s\n",
      "351:\ttest: 0.9087313\tbest: 0.9087352 (350)\ttotal: 14m 3s\tremaining: 25m 53s\n",
      "352:\ttest: 0.9087265\tbest: 0.9087352 (350)\ttotal: 14m 5s\tremaining: 25m 50s\n",
      "353:\ttest: 0.9087252\tbest: 0.9087352 (350)\ttotal: 14m 9s\tremaining: 25m 50s\n",
      "354:\ttest: 0.9087431\tbest: 0.9087431 (354)\ttotal: 14m 11s\tremaining: 25m 47s\n",
      "355:\ttest: 0.9087443\tbest: 0.9087443 (355)\ttotal: 14m 13s\tremaining: 25m 44s\n",
      "356:\ttest: 0.9087399\tbest: 0.9087443 (355)\ttotal: 14m 15s\tremaining: 25m 41s\n",
      "357:\ttest: 0.9087527\tbest: 0.9087527 (357)\ttotal: 14m 17s\tremaining: 25m 38s\n",
      "358:\ttest: 0.9087564\tbest: 0.9087564 (358)\ttotal: 14m 19s\tremaining: 25m 35s\n",
      "359:\ttest: 0.9087758\tbest: 0.9087758 (359)\ttotal: 14m 21s\tremaining: 25m 32s\n",
      "360:\ttest: 0.9087902\tbest: 0.9087902 (360)\ttotal: 14m 23s\tremaining: 25m 29s\n",
      "361:\ttest: 0.9087976\tbest: 0.9087976 (361)\ttotal: 14m 25s\tremaining: 25m 26s\n",
      "362:\ttest: 0.9088005\tbest: 0.9088005 (362)\ttotal: 14m 27s\tremaining: 25m 23s\n",
      "363:\ttest: 0.9088190\tbest: 0.9088190 (363)\ttotal: 14m 30s\tremaining: 25m 21s\n",
      "364:\ttest: 0.9088241\tbest: 0.9088241 (364)\ttotal: 14m 32s\tremaining: 25m 18s\n",
      "365:\ttest: 0.9088530\tbest: 0.9088530 (365)\ttotal: 14m 34s\tremaining: 25m 15s\n",
      "366:\ttest: 0.9088516\tbest: 0.9088530 (365)\ttotal: 14m 37s\tremaining: 25m 13s\n",
      "367:\ttest: 0.9088468\tbest: 0.9088530 (365)\ttotal: 14m 39s\tremaining: 25m 10s\n",
      "368:\ttest: 0.9088514\tbest: 0.9088530 (365)\ttotal: 14m 41s\tremaining: 25m 7s\n",
      "369:\ttest: 0.9088511\tbest: 0.9088530 (365)\ttotal: 14m 43s\tremaining: 25m 4s\n",
      "370:\ttest: 0.9088518\tbest: 0.9088530 (365)\ttotal: 14m 45s\tremaining: 25m 1s\n",
      "371:\ttest: 0.9088514\tbest: 0.9088530 (365)\ttotal: 14m 47s\tremaining: 24m 58s\n",
      "372:\ttest: 0.9088470\tbest: 0.9088530 (365)\ttotal: 14m 49s\tremaining: 24m 55s\n",
      "373:\ttest: 0.9088531\tbest: 0.9088531 (373)\ttotal: 14m 53s\tremaining: 24m 55s\n",
      "374:\ttest: 0.9088629\tbest: 0.9088629 (374)\ttotal: 14m 55s\tremaining: 24m 52s\n",
      "375:\ttest: 0.9088661\tbest: 0.9088661 (375)\ttotal: 14m 57s\tremaining: 24m 49s\n",
      "376:\ttest: 0.9088620\tbest: 0.9088661 (375)\ttotal: 14m 59s\tremaining: 24m 46s\n",
      "377:\ttest: 0.9088644\tbest: 0.9088661 (375)\ttotal: 15m 1s\tremaining: 24m 43s\n",
      "378:\ttest: 0.9088403\tbest: 0.9088661 (375)\ttotal: 15m 3s\tremaining: 24m 40s\n",
      "379:\ttest: 0.9088505\tbest: 0.9088661 (375)\ttotal: 15m 5s\tremaining: 24m 37s\n",
      "380:\ttest: 0.9088634\tbest: 0.9088661 (375)\ttotal: 15m 7s\tremaining: 24m 34s\n",
      "381:\ttest: 0.9088425\tbest: 0.9088661 (375)\ttotal: 15m 9s\tremaining: 24m 31s\n",
      "382:\ttest: 0.9088462\tbest: 0.9088661 (375)\ttotal: 15m 11s\tremaining: 24m 28s\n",
      "383:\ttest: 0.9088458\tbest: 0.9088661 (375)\ttotal: 15m 13s\tremaining: 24m 26s\n",
      "384:\ttest: 0.9088359\tbest: 0.9088661 (375)\ttotal: 15m 16s\tremaining: 24m 23s\n",
      "385:\ttest: 0.9088259\tbest: 0.9088661 (375)\ttotal: 15m 18s\tremaining: 24m 20s\n",
      "386:\ttest: 0.9088346\tbest: 0.9088661 (375)\ttotal: 15m 19s\tremaining: 24m 17s\n",
      "387:\ttest: 0.9088426\tbest: 0.9088661 (375)\ttotal: 15m 23s\tremaining: 24m 16s\n",
      "388:\ttest: 0.9088391\tbest: 0.9088661 (375)\ttotal: 15m 25s\tremaining: 24m 13s\n",
      "389:\ttest: 0.9088345\tbest: 0.9088661 (375)\ttotal: 15m 27s\tremaining: 24m 10s\n",
      "390:\ttest: 0.9088341\tbest: 0.9088661 (375)\ttotal: 15m 29s\tremaining: 24m 7s\n",
      "391:\ttest: 0.9088309\tbest: 0.9088661 (375)\ttotal: 15m 31s\tremaining: 24m 4s\n",
      "392:\ttest: 0.9088358\tbest: 0.9088661 (375)\ttotal: 15m 33s\tremaining: 24m 2s\n",
      "393:\ttest: 0.9088497\tbest: 0.9088661 (375)\ttotal: 15m 35s\tremaining: 23m 59s\n",
      "394:\ttest: 0.9088457\tbest: 0.9088661 (375)\ttotal: 15m 37s\tremaining: 23m 56s\n",
      "395:\ttest: 0.9088209\tbest: 0.9088661 (375)\ttotal: 15m 39s\tremaining: 23m 53s\n",
      "396:\ttest: 0.9088248\tbest: 0.9088661 (375)\ttotal: 15m 42s\tremaining: 23m 50s\n",
      "397:\ttest: 0.9088411\tbest: 0.9088661 (375)\ttotal: 15m 43s\tremaining: 23m 47s\n",
      "398:\ttest: 0.9088531\tbest: 0.9088661 (375)\ttotal: 15m 47s\tremaining: 23m 46s\n",
      "399:\ttest: 0.9088547\tbest: 0.9088661 (375)\ttotal: 15m 49s\tremaining: 23m 43s\n",
      "400:\ttest: 0.9088530\tbest: 0.9088661 (375)\ttotal: 15m 52s\tremaining: 23m 42s\n",
      "401:\ttest: 0.9088620\tbest: 0.9088661 (375)\ttotal: 15m 54s\tremaining: 23m 39s\n",
      "402:\ttest: 0.9088633\tbest: 0.9088661 (375)\ttotal: 15m 57s\tremaining: 23m 39s\n",
      "403:\ttest: 0.9088584\tbest: 0.9088661 (375)\ttotal: 15m 59s\tremaining: 23m 36s\n",
      "404:\ttest: 0.9088780\tbest: 0.9088780 (404)\ttotal: 16m 2s\tremaining: 23m 34s\n",
      "405:\ttest: 0.9088643\tbest: 0.9088780 (404)\ttotal: 16m 4s\tremaining: 23m 31s\n",
      "406:\ttest: 0.9088710\tbest: 0.9088780 (404)\ttotal: 16m 6s\tremaining: 23m 28s\n",
      "407:\ttest: 0.9088743\tbest: 0.9088780 (404)\ttotal: 16m 9s\tremaining: 23m 26s\n",
      "408:\ttest: 0.9088597\tbest: 0.9088780 (404)\ttotal: 16m 11s\tremaining: 23m 23s\n",
      "409:\ttest: 0.9088506\tbest: 0.9088780 (404)\ttotal: 16m 13s\tremaining: 23m 20s\n",
      "410:\ttest: 0.9088441\tbest: 0.9088780 (404)\ttotal: 16m 15s\tremaining: 23m 17s\n",
      "411:\ttest: 0.9088471\tbest: 0.9088780 (404)\ttotal: 16m 18s\tremaining: 23m 17s\n",
      "412:\ttest: 0.9088726\tbest: 0.9088780 (404)\ttotal: 16m 22s\tremaining: 23m 17s\n",
      "413:\ttest: 0.9088621\tbest: 0.9088780 (404)\ttotal: 16m 26s\tremaining: 23m 16s\n",
      "414:\ttest: 0.9088520\tbest: 0.9088780 (404)\ttotal: 16m 28s\tremaining: 23m 13s\n",
      "415:\ttest: 0.9088776\tbest: 0.9088780 (404)\ttotal: 16m 31s\tremaining: 23m 11s\n",
      "416:\ttest: 0.9088845\tbest: 0.9088845 (416)\ttotal: 16m 33s\tremaining: 23m 8s\n",
      "417:\ttest: 0.9088803\tbest: 0.9088845 (416)\ttotal: 16m 35s\tremaining: 23m 6s\n",
      "418:\ttest: 0.9088739\tbest: 0.9088845 (416)\ttotal: 16m 38s\tremaining: 23m 4s\n",
      "419:\ttest: 0.9088653\tbest: 0.9088845 (416)\ttotal: 16m 41s\tremaining: 23m 3s\n",
      "420:\ttest: 0.9088843\tbest: 0.9088845 (416)\ttotal: 16m 43s\tremaining: 23m\n",
      "421:\ttest: 0.9088940\tbest: 0.9088940 (421)\ttotal: 16m 45s\tremaining: 22m 57s\n",
      "422:\ttest: 0.9088954\tbest: 0.9088954 (422)\ttotal: 16m 47s\tremaining: 22m 54s\n",
      "423:\ttest: 0.9088982\tbest: 0.9088982 (423)\ttotal: 16m 50s\tremaining: 22m 52s\n",
      "424:\ttest: 0.9088958\tbest: 0.9088982 (423)\ttotal: 16m 52s\tremaining: 22m 49s\n",
      "425:\ttest: 0.9089033\tbest: 0.9089033 (425)\ttotal: 16m 54s\tremaining: 22m 47s\n",
      "426:\ttest: 0.9089040\tbest: 0.9089040 (426)\ttotal: 16m 56s\tremaining: 22m 44s\n",
      "427:\ttest: 0.9089109\tbest: 0.9089109 (427)\ttotal: 16m 58s\tremaining: 22m 41s\n",
      "428:\ttest: 0.9089191\tbest: 0.9089191 (428)\ttotal: 17m\tremaining: 22m 38s\n",
      "429:\ttest: 0.9089575\tbest: 0.9089575 (429)\ttotal: 17m 3s\tremaining: 22m 36s\n",
      "430:\ttest: 0.9089550\tbest: 0.9089575 (429)\ttotal: 17m 5s\tremaining: 22m 33s\n",
      "431:\ttest: 0.9089632\tbest: 0.9089632 (431)\ttotal: 17m 7s\tremaining: 22m 31s\n",
      "432:\ttest: 0.9089593\tbest: 0.9089632 (431)\ttotal: 17m 9s\tremaining: 22m 28s\n",
      "433:\ttest: 0.9089724\tbest: 0.9089724 (433)\ttotal: 17m 11s\tremaining: 22m 25s\n",
      "434:\ttest: 0.9089618\tbest: 0.9089724 (433)\ttotal: 17m 14s\tremaining: 22m 23s\n",
      "435:\ttest: 0.9089699\tbest: 0.9089724 (433)\ttotal: 17m 16s\tremaining: 22m 21s\n",
      "436:\ttest: 0.9089854\tbest: 0.9089854 (436)\ttotal: 17m 18s\tremaining: 22m 18s\n",
      "437:\ttest: 0.9089906\tbest: 0.9089906 (437)\ttotal: 17m 20s\tremaining: 22m 15s\n",
      "438:\ttest: 0.9089902\tbest: 0.9089906 (437)\ttotal: 17m 22s\tremaining: 22m 12s\n",
      "439:\ttest: 0.9089906\tbest: 0.9089906 (437)\ttotal: 17m 24s\tremaining: 22m 9s\n",
      "440:\ttest: 0.9089877\tbest: 0.9089906 (437)\ttotal: 17m 26s\tremaining: 22m 6s\n",
      "441:\ttest: 0.9089846\tbest: 0.9089906 (437)\ttotal: 17m 28s\tremaining: 22m 4s\n",
      "442:\ttest: 0.9089823\tbest: 0.9089906 (437)\ttotal: 17m 30s\tremaining: 22m 1s\n",
      "443:\ttest: 0.9089947\tbest: 0.9089947 (443)\ttotal: 17m 32s\tremaining: 21m 58s\n",
      "444:\ttest: 0.9089869\tbest: 0.9089947 (443)\ttotal: 17m 34s\tremaining: 21m 55s\n",
      "445:\ttest: 0.9089921\tbest: 0.9089947 (443)\ttotal: 17m 37s\tremaining: 21m 53s\n",
      "446:\ttest: 0.9090016\tbest: 0.9090016 (446)\ttotal: 17m 39s\tremaining: 21m 50s\n",
      "447:\ttest: 0.9089943\tbest: 0.9090016 (446)\ttotal: 17m 41s\tremaining: 21m 47s\n",
      "448:\ttest: 0.9089905\tbest: 0.9090016 (446)\ttotal: 17m 43s\tremaining: 21m 44s\n",
      "449:\ttest: 0.9089904\tbest: 0.9090016 (446)\ttotal: 17m 45s\tremaining: 21m 41s\n",
      "450:\ttest: 0.9089794\tbest: 0.9090016 (446)\ttotal: 17m 47s\tremaining: 21m 39s\n",
      "451:\ttest: 0.9089780\tbest: 0.9090016 (446)\ttotal: 17m 49s\tremaining: 21m 36s\n",
      "452:\ttest: 0.9089687\tbest: 0.9090016 (446)\ttotal: 17m 51s\tremaining: 21m 33s\n",
      "453:\ttest: 0.9089551\tbest: 0.9090016 (446)\ttotal: 17m 54s\tremaining: 21m 32s\n",
      "454:\ttest: 0.9089571\tbest: 0.9090016 (446)\ttotal: 17m 56s\tremaining: 21m 29s\n",
      "455:\ttest: 0.9089589\tbest: 0.9090016 (446)\ttotal: 18m\tremaining: 21m 29s\n",
      "456:\ttest: 0.9089526\tbest: 0.9090016 (446)\ttotal: 18m 4s\tremaining: 21m 28s\n",
      "457:\ttest: 0.9089615\tbest: 0.9090016 (446)\ttotal: 18m 6s\tremaining: 21m 25s\n",
      "458:\ttest: 0.9089465\tbest: 0.9090016 (446)\ttotal: 18m 8s\tremaining: 21m 23s\n",
      "459:\ttest: 0.9089670\tbest: 0.9090016 (446)\ttotal: 18m 10s\tremaining: 21m 20s\n",
      "460:\ttest: 0.9089906\tbest: 0.9090016 (446)\ttotal: 18m 14s\tremaining: 21m 19s\n",
      "461:\ttest: 0.9089823\tbest: 0.9090016 (446)\ttotal: 18m 16s\tremaining: 21m 17s\n",
      "462:\ttest: 0.9089691\tbest: 0.9090016 (446)\ttotal: 18m 18s\tremaining: 21m 14s\n",
      "463:\ttest: 0.9091244\tbest: 0.9091244 (463)\ttotal: 18m 21s\tremaining: 21m 12s\n",
      "464:\ttest: 0.9091173\tbest: 0.9091244 (463)\ttotal: 18m 23s\tremaining: 21m 9s\n",
      "465:\ttest: 0.9091072\tbest: 0.9091244 (463)\ttotal: 18m 27s\tremaining: 21m 9s\n",
      "466:\ttest: 0.9091153\tbest: 0.9091244 (463)\ttotal: 18m 30s\tremaining: 21m 6s\n",
      "467:\ttest: 0.9091521\tbest: 0.9091521 (467)\ttotal: 18m 32s\tremaining: 21m 4s\n",
      "468:\ttest: 0.9091612\tbest: 0.9091612 (468)\ttotal: 18m 34s\tremaining: 21m 1s\n",
      "469:\ttest: 0.9091632\tbest: 0.9091632 (469)\ttotal: 18m 36s\tremaining: 20m 59s\n",
      "470:\ttest: 0.9091560\tbest: 0.9091632 (469)\ttotal: 18m 38s\tremaining: 20m 56s\n",
      "471:\ttest: 0.9091550\tbest: 0.9091632 (469)\ttotal: 18m 41s\tremaining: 20m 54s\n",
      "472:\ttest: 0.9091520\tbest: 0.9091632 (469)\ttotal: 18m 43s\tremaining: 20m 52s\n",
      "473:\ttest: 0.9091572\tbest: 0.9091632 (469)\ttotal: 18m 45s\tremaining: 20m 49s\n",
      "474:\ttest: 0.9091608\tbest: 0.9091632 (469)\ttotal: 18m 47s\tremaining: 20m 46s\n",
      "475:\ttest: 0.9091673\tbest: 0.9091673 (475)\ttotal: 18m 49s\tremaining: 20m 43s\n",
      "476:\ttest: 0.9091782\tbest: 0.9091782 (476)\ttotal: 18m 51s\tremaining: 20m 41s\n",
      "477:\ttest: 0.9091942\tbest: 0.9091942 (477)\ttotal: 18m 54s\tremaining: 20m 38s\n",
      "478:\ttest: 0.9092021\tbest: 0.9092021 (478)\ttotal: 18m 56s\tremaining: 20m 35s\n",
      "479:\ttest: 0.9091914\tbest: 0.9092021 (478)\ttotal: 18m 58s\tremaining: 20m 33s\n",
      "480:\ttest: 0.9092029\tbest: 0.9092029 (480)\ttotal: 19m\tremaining: 20m 30s\n",
      "481:\ttest: 0.9092038\tbest: 0.9092038 (481)\ttotal: 19m 2s\tremaining: 20m 28s\n",
      "482:\ttest: 0.9092111\tbest: 0.9092111 (482)\ttotal: 19m 4s\tremaining: 20m 25s\n",
      "483:\ttest: 0.9092453\tbest: 0.9092453 (483)\ttotal: 19m 6s\tremaining: 20m 22s\n",
      "484:\ttest: 0.9092402\tbest: 0.9092453 (483)\ttotal: 19m 8s\tremaining: 20m 19s\n",
      "485:\ttest: 0.9092443\tbest: 0.9092453 (483)\ttotal: 19m 10s\tremaining: 20m 17s\n",
      "486:\ttest: 0.9092422\tbest: 0.9092453 (483)\ttotal: 19m 12s\tremaining: 20m 14s\n",
      "487:\ttest: 0.9092427\tbest: 0.9092453 (483)\ttotal: 19m 15s\tremaining: 20m 12s\n",
      "488:\ttest: 0.9092422\tbest: 0.9092453 (483)\ttotal: 19m 18s\tremaining: 20m 10s\n",
      "489:\ttest: 0.9092196\tbest: 0.9092453 (483)\ttotal: 19m 22s\tremaining: 20m 9s\n",
      "490:\ttest: 0.9092201\tbest: 0.9092453 (483)\ttotal: 19m 24s\tremaining: 20m 6s\n",
      "491:\ttest: 0.9092307\tbest: 0.9092453 (483)\ttotal: 19m 26s\tremaining: 20m 3s\n",
      "492:\ttest: 0.9092238\tbest: 0.9092453 (483)\ttotal: 19m 28s\tremaining: 20m 1s\n",
      "493:\ttest: 0.9092345\tbest: 0.9092453 (483)\ttotal: 19m 30s\tremaining: 19m 58s\n",
      "494:\ttest: 0.9092320\tbest: 0.9092453 (483)\ttotal: 19m 32s\tremaining: 19m 55s\n",
      "495:\ttest: 0.9092048\tbest: 0.9092453 (483)\ttotal: 19m 34s\tremaining: 19m 53s\n",
      "496:\ttest: 0.9091969\tbest: 0.9092453 (483)\ttotal: 19m 36s\tremaining: 19m 50s\n",
      "497:\ttest: 0.9091982\tbest: 0.9092453 (483)\ttotal: 19m 38s\tremaining: 19m 47s\n",
      "498:\ttest: 0.9092140\tbest: 0.9092453 (483)\ttotal: 19m 40s\tremaining: 19m 45s\n",
      "499:\ttest: 0.9092039\tbest: 0.9092453 (483)\ttotal: 19m 42s\tremaining: 19m 42s\n",
      "500:\ttest: 0.9092074\tbest: 0.9092453 (483)\ttotal: 19m 44s\tremaining: 19m 39s\n",
      "501:\ttest: 0.9092253\tbest: 0.9092453 (483)\ttotal: 19m 46s\tremaining: 19m 36s\n",
      "502:\ttest: 0.9092257\tbest: 0.9092453 (483)\ttotal: 19m 48s\tremaining: 19m 34s\n",
      "503:\ttest: 0.9092283\tbest: 0.9092453 (483)\ttotal: 19m 50s\tremaining: 19m 31s\n",
      "504:\ttest: 0.9092278\tbest: 0.9092453 (483)\ttotal: 19m 52s\tremaining: 19m 28s\n",
      "505:\ttest: 0.9092915\tbest: 0.9092915 (505)\ttotal: 19m 55s\tremaining: 19m 27s\n",
      "506:\ttest: 0.9092708\tbest: 0.9092915 (505)\ttotal: 19m 57s\tremaining: 19m 24s\n",
      "507:\ttest: 0.9092796\tbest: 0.9092915 (505)\ttotal: 19m 59s\tremaining: 19m 22s\n",
      "508:\ttest: 0.9092808\tbest: 0.9092915 (505)\ttotal: 20m 1s\tremaining: 19m 19s\n",
      "509:\ttest: 0.9092999\tbest: 0.9092999 (509)\ttotal: 20m 3s\tremaining: 19m 16s\n",
      "510:\ttest: 0.9092988\tbest: 0.9092999 (509)\ttotal: 20m 5s\tremaining: 19m 14s\n",
      "511:\ttest: 0.9093159\tbest: 0.9093159 (511)\ttotal: 20m 9s\tremaining: 19m 12s\n",
      "512:\ttest: 0.9093165\tbest: 0.9093165 (512)\ttotal: 20m 11s\tremaining: 19m 9s\n",
      "513:\ttest: 0.9093126\tbest: 0.9093165 (512)\ttotal: 20m 13s\tremaining: 19m 6s\n",
      "514:\ttest: 0.9093197\tbest: 0.9093197 (514)\ttotal: 20m 14s\tremaining: 19m 4s\n",
      "515:\ttest: 0.9093203\tbest: 0.9093203 (515)\ttotal: 20m 16s\tremaining: 19m 1s\n",
      "516:\ttest: 0.9093177\tbest: 0.9093203 (515)\ttotal: 20m 18s\tremaining: 18m 58s\n",
      "517:\ttest: 0.9093310\tbest: 0.9093310 (517)\ttotal: 20m 20s\tremaining: 18m 56s\n",
      "518:\ttest: 0.9093307\tbest: 0.9093310 (517)\ttotal: 20m 23s\tremaining: 18m 53s\n",
      "519:\ttest: 0.9093318\tbest: 0.9093318 (519)\ttotal: 20m 25s\tremaining: 18m 50s\n",
      "520:\ttest: 0.9093291\tbest: 0.9093318 (519)\ttotal: 20m 27s\tremaining: 18m 48s\n",
      "521:\ttest: 0.9093264\tbest: 0.9093318 (519)\ttotal: 20m 29s\tremaining: 18m 45s\n",
      "522:\ttest: 0.9093303\tbest: 0.9093318 (519)\ttotal: 20m 31s\tremaining: 18m 42s\n",
      "523:\ttest: 0.9093304\tbest: 0.9093318 (519)\ttotal: 20m 33s\tremaining: 18m 40s\n",
      "524:\ttest: 0.9093296\tbest: 0.9093318 (519)\ttotal: 20m 35s\tremaining: 18m 38s\n",
      "525:\ttest: 0.9093252\tbest: 0.9093318 (519)\ttotal: 20m 37s\tremaining: 18m 35s\n",
      "526:\ttest: 0.9093210\tbest: 0.9093318 (519)\ttotal: 20m 39s\tremaining: 18m 32s\n",
      "527:\ttest: 0.9093358\tbest: 0.9093358 (527)\ttotal: 20m 41s\tremaining: 18m 30s\n",
      "528:\ttest: 0.9093342\tbest: 0.9093358 (527)\ttotal: 20m 43s\tremaining: 18m 27s\n",
      "529:\ttest: 0.9093322\tbest: 0.9093358 (527)\ttotal: 20m 46s\tremaining: 18m 25s\n",
      "530:\ttest: 0.9093307\tbest: 0.9093358 (527)\ttotal: 20m 48s\tremaining: 18m 22s\n",
      "531:\ttest: 0.9093287\tbest: 0.9093358 (527)\ttotal: 20m 50s\tremaining: 18m 19s\n",
      "532:\ttest: 0.9093295\tbest: 0.9093358 (527)\ttotal: 20m 52s\tremaining: 18m 17s\n",
      "533:\ttest: 0.9093295\tbest: 0.9093358 (527)\ttotal: 20m 54s\tremaining: 18m 14s\n",
      "534:\ttest: 0.9093308\tbest: 0.9093358 (527)\ttotal: 20m 56s\tremaining: 18m 11s\n",
      "535:\ttest: 0.9093466\tbest: 0.9093466 (535)\ttotal: 20m 58s\tremaining: 18m 9s\n",
      "536:\ttest: 0.9093468\tbest: 0.9093468 (536)\ttotal: 21m\tremaining: 18m 6s\n",
      "537:\ttest: 0.9093387\tbest: 0.9093468 (536)\ttotal: 21m 2s\tremaining: 18m 3s\n",
      "538:\ttest: 0.9093526\tbest: 0.9093526 (538)\ttotal: 21m 4s\tremaining: 18m 1s\n",
      "539:\ttest: 0.9093593\tbest: 0.9093593 (539)\ttotal: 21m 6s\tremaining: 17m 58s\n",
      "540:\ttest: 0.9093591\tbest: 0.9093593 (539)\ttotal: 21m 8s\tremaining: 17m 55s\n",
      "541:\ttest: 0.9093592\tbest: 0.9093593 (539)\ttotal: 21m 9s\tremaining: 17m 53s\n",
      "542:\ttest: 0.9093564\tbest: 0.9093593 (539)\ttotal: 21m 12s\tremaining: 17m 50s\n",
      "543:\ttest: 0.9093635\tbest: 0.9093635 (543)\ttotal: 21m 14s\tremaining: 17m 47s\n",
      "544:\ttest: 0.9093573\tbest: 0.9093635 (543)\ttotal: 21m 16s\tremaining: 17m 45s\n",
      "545:\ttest: 0.9093545\tbest: 0.9093635 (543)\ttotal: 21m 18s\tremaining: 17m 42s\n",
      "546:\ttest: 0.9093454\tbest: 0.9093635 (543)\ttotal: 21m 21s\tremaining: 17m 41s\n",
      "547:\ttest: 0.9093497\tbest: 0.9093635 (543)\ttotal: 21m 23s\tremaining: 17m 38s\n",
      "548:\ttest: 0.9093504\tbest: 0.9093635 (543)\ttotal: 21m 25s\tremaining: 17m 35s\n",
      "549:\ttest: 0.9093514\tbest: 0.9093635 (543)\ttotal: 21m 27s\tremaining: 17m 33s\n",
      "550:\ttest: 0.9093400\tbest: 0.9093635 (543)\ttotal: 21m 29s\tremaining: 17m 31s\n",
      "551:\ttest: 0.9093486\tbest: 0.9093635 (543)\ttotal: 21m 31s\tremaining: 17m 28s\n",
      "552:\ttest: 0.9093544\tbest: 0.9093635 (543)\ttotal: 21m 33s\tremaining: 17m 25s\n",
      "553:\ttest: 0.9093548\tbest: 0.9093635 (543)\ttotal: 21m 35s\tremaining: 17m 23s\n",
      "554:\ttest: 0.9093577\tbest: 0.9093635 (543)\ttotal: 21m 37s\tremaining: 17m 20s\n",
      "555:\ttest: 0.9093470\tbest: 0.9093635 (543)\ttotal: 21m 39s\tremaining: 17m 17s\n",
      "556:\ttest: 0.9093480\tbest: 0.9093635 (543)\ttotal: 21m 42s\tremaining: 17m 16s\n",
      "557:\ttest: 0.9093466\tbest: 0.9093635 (543)\ttotal: 21m 44s\tremaining: 17m 13s\n",
      "558:\ttest: 0.9093310\tbest: 0.9093635 (543)\ttotal: 21m 46s\tremaining: 17m 10s\n",
      "559:\ttest: 0.9093408\tbest: 0.9093635 (543)\ttotal: 21m 49s\tremaining: 17m 8s\n",
      "560:\ttest: 0.9093702\tbest: 0.9093702 (560)\ttotal: 21m 51s\tremaining: 17m 6s\n",
      "561:\ttest: 0.9093696\tbest: 0.9093702 (560)\ttotal: 21m 53s\tremaining: 17m 3s\n",
      "562:\ttest: 0.9093673\tbest: 0.9093702 (560)\ttotal: 21m 55s\tremaining: 17m 1s\n",
      "563:\ttest: 0.9093685\tbest: 0.9093702 (560)\ttotal: 21m 57s\tremaining: 16m 58s\n",
      "564:\ttest: 0.9093696\tbest: 0.9093702 (560)\ttotal: 21m 59s\tremaining: 16m 55s\n",
      "565:\ttest: 0.9093825\tbest: 0.9093825 (565)\ttotal: 22m 1s\tremaining: 16m 53s\n",
      "566:\ttest: 0.9093827\tbest: 0.9093827 (566)\ttotal: 22m 3s\tremaining: 16m 50s\n",
      "567:\ttest: 0.9093941\tbest: 0.9093941 (567)\ttotal: 22m 5s\tremaining: 16m 47s\n",
      "568:\ttest: 0.9093961\tbest: 0.9093961 (568)\ttotal: 22m 7s\tremaining: 16m 45s\n",
      "569:\ttest: 0.9093928\tbest: 0.9093961 (568)\ttotal: 22m 9s\tremaining: 16m 42s\n",
      "570:\ttest: 0.9093881\tbest: 0.9093961 (568)\ttotal: 22m 11s\tremaining: 16m 40s\n",
      "571:\ttest: 0.9093909\tbest: 0.9093961 (568)\ttotal: 22m 13s\tremaining: 16m 37s\n",
      "572:\ttest: 0.9093969\tbest: 0.9093969 (572)\ttotal: 22m 15s\tremaining: 16m 35s\n",
      "573:\ttest: 0.9093974\tbest: 0.9093974 (573)\ttotal: 22m 17s\tremaining: 16m 32s\n",
      "574:\ttest: 0.9093882\tbest: 0.9093974 (573)\ttotal: 22m 19s\tremaining: 16m 29s\n",
      "575:\ttest: 0.9093909\tbest: 0.9093974 (573)\ttotal: 22m 21s\tremaining: 16m 27s\n",
      "576:\ttest: 0.9093696\tbest: 0.9093974 (573)\ttotal: 22m 23s\tremaining: 16m 24s\n",
      "577:\ttest: 0.9093673\tbest: 0.9093974 (573)\ttotal: 22m 25s\tremaining: 16m 22s\n",
      "578:\ttest: 0.9093747\tbest: 0.9093974 (573)\ttotal: 22m 27s\tremaining: 16m 20s\n",
      "579:\ttest: 0.9094060\tbest: 0.9094060 (579)\ttotal: 22m 31s\tremaining: 16m 18s\n",
      "580:\ttest: 0.9094029\tbest: 0.9094060 (579)\ttotal: 22m 33s\tremaining: 16m 15s\n",
      "581:\ttest: 0.9094050\tbest: 0.9094060 (579)\ttotal: 22m 35s\tremaining: 16m 13s\n",
      "582:\ttest: 0.9094086\tbest: 0.9094086 (582)\ttotal: 22m 37s\tremaining: 16m 10s\n",
      "583:\ttest: 0.9094118\tbest: 0.9094118 (583)\ttotal: 22m 39s\tremaining: 16m 8s\n",
      "584:\ttest: 0.9094165\tbest: 0.9094165 (584)\ttotal: 22m 41s\tremaining: 16m 5s\n",
      "585:\ttest: 0.9094183\tbest: 0.9094183 (585)\ttotal: 22m 43s\tremaining: 16m 3s\n",
      "586:\ttest: 0.9094393\tbest: 0.9094393 (586)\ttotal: 22m 45s\tremaining: 16m\n",
      "587:\ttest: 0.9094145\tbest: 0.9094393 (586)\ttotal: 22m 48s\tremaining: 15m 58s\n",
      "588:\ttest: 0.9094154\tbest: 0.9094393 (586)\ttotal: 22m 50s\tremaining: 15m 56s\n",
      "589:\ttest: 0.9094006\tbest: 0.9094393 (586)\ttotal: 22m 52s\tremaining: 15m 53s\n",
      "590:\ttest: 0.9094007\tbest: 0.9094393 (586)\ttotal: 22m 53s\tremaining: 15m 50s\n",
      "591:\ttest: 0.9094062\tbest: 0.9094393 (586)\ttotal: 22m 55s\tremaining: 15m 48s\n",
      "592:\ttest: 0.9093968\tbest: 0.9094393 (586)\ttotal: 22m 57s\tremaining: 15m 45s\n",
      "593:\ttest: 0.9093963\tbest: 0.9094393 (586)\ttotal: 23m\tremaining: 15m 43s\n",
      "594:\ttest: 0.9093944\tbest: 0.9094393 (586)\ttotal: 23m 2s\tremaining: 15m 41s\n",
      "595:\ttest: 0.9093896\tbest: 0.9094393 (586)\ttotal: 23m 4s\tremaining: 15m 38s\n",
      "596:\ttest: 0.9093812\tbest: 0.9094393 (586)\ttotal: 23m 6s\tremaining: 15m 35s\n",
      "597:\ttest: 0.9093823\tbest: 0.9094393 (586)\ttotal: 23m 8s\tremaining: 15m 33s\n",
      "598:\ttest: 0.9093890\tbest: 0.9094393 (586)\ttotal: 23m 10s\tremaining: 15m 30s\n",
      "599:\ttest: 0.9098597\tbest: 0.9098597 (599)\ttotal: 23m 12s\tremaining: 15m 28s\n",
      "600:\ttest: 0.9098612\tbest: 0.9098612 (600)\ttotal: 23m 14s\tremaining: 15m 26s\n",
      "601:\ttest: 0.9098728\tbest: 0.9098728 (601)\ttotal: 23m 18s\tremaining: 15m 24s\n",
      "602:\ttest: 0.9098688\tbest: 0.9098728 (601)\ttotal: 23m 20s\tremaining: 15m 21s\n",
      "603:\ttest: 0.9098888\tbest: 0.9098888 (603)\ttotal: 23m 23s\tremaining: 15m 20s\n",
      "604:\ttest: 0.9105883\tbest: 0.9105883 (604)\ttotal: 23m 26s\tremaining: 15m 18s\n",
      "605:\ttest: 0.9106333\tbest: 0.9106333 (605)\ttotal: 23m 28s\tremaining: 15m 15s\n",
      "606:\ttest: 0.9106285\tbest: 0.9106333 (605)\ttotal: 23m 30s\tremaining: 15m 13s\n",
      "607:\ttest: 0.9106364\tbest: 0.9106364 (607)\ttotal: 23m 32s\tremaining: 15m 10s\n",
      "608:\ttest: 0.9106390\tbest: 0.9106390 (608)\ttotal: 23m 34s\tremaining: 15m 8s\n",
      "609:\ttest: 0.9106333\tbest: 0.9106390 (608)\ttotal: 23m 36s\tremaining: 15m 5s\n",
      "610:\ttest: 0.9106174\tbest: 0.9106390 (608)\ttotal: 23m 38s\tremaining: 15m 3s\n",
      "611:\ttest: 0.9106412\tbest: 0.9106412 (611)\ttotal: 23m 40s\tremaining: 15m\n",
      "612:\ttest: 0.9106419\tbest: 0.9106419 (612)\ttotal: 23m 42s\tremaining: 14m 58s\n",
      "613:\ttest: 0.9107094\tbest: 0.9107094 (613)\ttotal: 23m 44s\tremaining: 14m 55s\n",
      "614:\ttest: 0.9107044\tbest: 0.9107094 (613)\ttotal: 23m 46s\tremaining: 14m 53s\n",
      "615:\ttest: 0.9106939\tbest: 0.9107094 (613)\ttotal: 23m 48s\tremaining: 14m 50s\n",
      "616:\ttest: 0.9106968\tbest: 0.9107094 (613)\ttotal: 23m 50s\tremaining: 14m 48s\n",
      "617:\ttest: 0.9107098\tbest: 0.9107098 (617)\ttotal: 23m 53s\tremaining: 14m 45s\n",
      "618:\ttest: 0.9107108\tbest: 0.9107108 (618)\ttotal: 23m 55s\tremaining: 14m 43s\n",
      "619:\ttest: 0.9107101\tbest: 0.9107108 (618)\ttotal: 23m 57s\tremaining: 14m 41s\n",
      "620:\ttest: 0.9107122\tbest: 0.9107122 (620)\ttotal: 23m 59s\tremaining: 14m 38s\n",
      "621:\ttest: 0.9107105\tbest: 0.9107122 (620)\ttotal: 24m 1s\tremaining: 14m 36s\n",
      "622:\ttest: 0.9107199\tbest: 0.9107199 (622)\ttotal: 24m 3s\tremaining: 14m 33s\n",
      "623:\ttest: 0.9107047\tbest: 0.9107199 (622)\ttotal: 24m 5s\tremaining: 14m 31s\n",
      "624:\ttest: 0.9106949\tbest: 0.9107199 (622)\ttotal: 24m 7s\tremaining: 14m 28s\n",
      "625:\ttest: 0.9106950\tbest: 0.9107199 (622)\ttotal: 24m 9s\tremaining: 14m 26s\n",
      "626:\ttest: 0.9106949\tbest: 0.9107199 (622)\ttotal: 24m 11s\tremaining: 14m 23s\n",
      "627:\ttest: 0.9106913\tbest: 0.9107199 (622)\ttotal: 24m 13s\tremaining: 14m 20s\n",
      "628:\ttest: 0.9106905\tbest: 0.9107199 (622)\ttotal: 24m 16s\tremaining: 14m 18s\n",
      "629:\ttest: 0.9106980\tbest: 0.9107199 (622)\ttotal: 24m 19s\tremaining: 14m 16s\n",
      "630:\ttest: 0.9106991\tbest: 0.9107199 (622)\ttotal: 24m 21s\tremaining: 14m 14s\n",
      "631:\ttest: 0.9106809\tbest: 0.9107199 (622)\ttotal: 24m 23s\tremaining: 14m 11s\n",
      "632:\ttest: 0.9106931\tbest: 0.9107199 (622)\ttotal: 24m 24s\tremaining: 14m 9s\n",
      "633:\ttest: 0.9106977\tbest: 0.9107199 (622)\ttotal: 24m 27s\tremaining: 14m 7s\n",
      "634:\ttest: 0.9106731\tbest: 0.9107199 (622)\ttotal: 24m 30s\tremaining: 14m 5s\n",
      "635:\ttest: 0.9106786\tbest: 0.9107199 (622)\ttotal: 24m 32s\tremaining: 14m 2s\n",
      "636:\ttest: 0.9106786\tbest: 0.9107199 (622)\ttotal: 24m 34s\tremaining: 14m\n",
      "637:\ttest: 0.9106785\tbest: 0.9107199 (622)\ttotal: 24m 36s\tremaining: 13m 57s\n",
      "638:\ttest: 0.9106681\tbest: 0.9107199 (622)\ttotal: 24m 38s\tremaining: 13m 55s\n",
      "639:\ttest: 0.9106685\tbest: 0.9107199 (622)\ttotal: 24m 40s\tremaining: 13m 52s\n",
      "640:\ttest: 0.9106619\tbest: 0.9107199 (622)\ttotal: 24m 42s\tremaining: 13m 50s\n",
      "641:\ttest: 0.9106562\tbest: 0.9107199 (622)\ttotal: 24m 46s\tremaining: 13m 48s\n",
      "642:\ttest: 0.9106602\tbest: 0.9107199 (622)\ttotal: 24m 48s\tremaining: 13m 46s\n",
      "643:\ttest: 0.9106588\tbest: 0.9107199 (622)\ttotal: 24m 50s\tremaining: 13m 43s\n",
      "644:\ttest: 0.9106582\tbest: 0.9107199 (622)\ttotal: 24m 52s\tremaining: 13m 41s\n",
      "645:\ttest: 0.9106647\tbest: 0.9107199 (622)\ttotal: 24m 54s\tremaining: 13m 38s\n",
      "646:\ttest: 0.9106871\tbest: 0.9107199 (622)\ttotal: 24m 56s\tremaining: 13m 36s\n",
      "647:\ttest: 0.9106862\tbest: 0.9107199 (622)\ttotal: 24m 59s\tremaining: 13m 34s\n",
      "648:\ttest: 0.9106956\tbest: 0.9107199 (622)\ttotal: 25m 1s\tremaining: 13m 32s\n",
      "649:\ttest: 0.9106865\tbest: 0.9107199 (622)\ttotal: 25m 3s\tremaining: 13m 29s\n",
      "650:\ttest: 0.9106882\tbest: 0.9107199 (622)\ttotal: 25m 5s\tremaining: 13m 27s\n",
      "651:\ttest: 0.9106978\tbest: 0.9107199 (622)\ttotal: 25m 7s\tremaining: 13m 24s\n",
      "652:\ttest: 0.9106988\tbest: 0.9107199 (622)\ttotal: 25m 9s\tremaining: 13m 22s\n",
      "653:\ttest: 0.9106961\tbest: 0.9107199 (622)\ttotal: 25m 11s\tremaining: 13m 19s\n",
      "654:\ttest: 0.9106978\tbest: 0.9107199 (622)\ttotal: 25m 13s\tremaining: 13m 17s\n",
      "655:\ttest: 0.9107117\tbest: 0.9107199 (622)\ttotal: 25m 15s\tremaining: 13m 14s\n",
      "656:\ttest: 0.9107128\tbest: 0.9107199 (622)\ttotal: 25m 17s\tremaining: 13m 12s\n",
      "657:\ttest: 0.9107186\tbest: 0.9107199 (622)\ttotal: 25m 19s\tremaining: 13m 9s\n",
      "658:\ttest: 0.9107153\tbest: 0.9107199 (622)\ttotal: 25m 21s\tremaining: 13m 7s\n",
      "659:\ttest: 0.9107054\tbest: 0.9107199 (622)\ttotal: 25m 23s\tremaining: 13m 4s\n",
      "660:\ttest: 0.9107059\tbest: 0.9107199 (622)\ttotal: 25m 26s\tremaining: 13m 2s\n",
      "661:\ttest: 0.9107300\tbest: 0.9107300 (661)\ttotal: 25m 28s\tremaining: 13m\n",
      "662:\ttest: 0.9107271\tbest: 0.9107300 (661)\ttotal: 25m 30s\tremaining: 12m 58s\n",
      "663:\ttest: 0.9107471\tbest: 0.9107471 (663)\ttotal: 25m 32s\tremaining: 12m 55s\n",
      "664:\ttest: 0.9110438\tbest: 0.9110438 (664)\ttotal: 25m 36s\tremaining: 12m 54s\n",
      "665:\ttest: 0.9110548\tbest: 0.9110548 (665)\ttotal: 25m 39s\tremaining: 12m 51s\n",
      "666:\ttest: 0.9110492\tbest: 0.9110548 (665)\ttotal: 25m 41s\tremaining: 12m 49s\n",
      "667:\ttest: 0.9110325\tbest: 0.9110548 (665)\ttotal: 25m 44s\tremaining: 12m 47s\n",
      "668:\ttest: 0.9110295\tbest: 0.9110548 (665)\ttotal: 25m 48s\tremaining: 12m 46s\n",
      "669:\ttest: 0.9110410\tbest: 0.9110548 (665)\ttotal: 25m 52s\tremaining: 12m 44s\n",
      "670:\ttest: 0.9110575\tbest: 0.9110575 (670)\ttotal: 25m 54s\tremaining: 12m 42s\n",
      "671:\ttest: 0.9110649\tbest: 0.9110649 (671)\ttotal: 25m 56s\tremaining: 12m 39s\n",
      "672:\ttest: 0.9110733\tbest: 0.9110733 (672)\ttotal: 25m 58s\tremaining: 12m 37s\n",
      "673:\ttest: 0.9112251\tbest: 0.9112251 (673)\ttotal: 26m 1s\tremaining: 12m 35s\n",
      "674:\ttest: 0.9112228\tbest: 0.9112251 (673)\ttotal: 26m 3s\tremaining: 12m 32s\n",
      "675:\ttest: 0.9112182\tbest: 0.9112251 (673)\ttotal: 26m 5s\tremaining: 12m 30s\n",
      "676:\ttest: 0.9112199\tbest: 0.9112251 (673)\ttotal: 26m 7s\tremaining: 12m 27s\n",
      "677:\ttest: 0.9112094\tbest: 0.9112251 (673)\ttotal: 26m 10s\tremaining: 12m 25s\n",
      "678:\ttest: 0.9111966\tbest: 0.9112251 (673)\ttotal: 26m 11s\tremaining: 12m 23s\n",
      "679:\ttest: 0.9112192\tbest: 0.9112251 (673)\ttotal: 26m 16s\tremaining: 12m 21s\n",
      "680:\ttest: 0.9112416\tbest: 0.9112416 (680)\ttotal: 26m 18s\tremaining: 12m 19s\n",
      "681:\ttest: 0.9112520\tbest: 0.9112520 (681)\ttotal: 26m 20s\tremaining: 12m 16s\n",
      "682:\ttest: 0.9112614\tbest: 0.9112614 (682)\ttotal: 26m 24s\tremaining: 12m 15s\n",
      "683:\ttest: 0.9112793\tbest: 0.9112793 (683)\ttotal: 26m 27s\tremaining: 12m 13s\n",
      "684:\ttest: 0.9112806\tbest: 0.9112806 (684)\ttotal: 26m 29s\tremaining: 12m 10s\n",
      "685:\ttest: 0.9112696\tbest: 0.9112806 (684)\ttotal: 26m 31s\tremaining: 12m 8s\n",
      "686:\ttest: 0.9112587\tbest: 0.9112806 (684)\ttotal: 26m 33s\tremaining: 12m 5s\n",
      "687:\ttest: 0.9112715\tbest: 0.9112806 (684)\ttotal: 26m 35s\tremaining: 12m 3s\n",
      "688:\ttest: 0.9112717\tbest: 0.9112806 (684)\ttotal: 26m 37s\tremaining: 12m\n",
      "689:\ttest: 0.9112453\tbest: 0.9112806 (684)\ttotal: 26m 39s\tremaining: 11m 58s\n",
      "690:\ttest: 0.9112699\tbest: 0.9112806 (684)\ttotal: 26m 43s\tremaining: 11m 56s\n",
      "691:\ttest: 0.9112464\tbest: 0.9112806 (684)\ttotal: 26m 45s\tremaining: 11m 54s\n",
      "692:\ttest: 0.9112811\tbest: 0.9112811 (692)\ttotal: 26m 47s\tremaining: 11m 51s\n",
      "693:\ttest: 0.9112552\tbest: 0.9112811 (692)\ttotal: 26m 48s\tremaining: 11m 49s\n",
      "694:\ttest: 0.9112449\tbest: 0.9112811 (692)\ttotal: 26m 50s\tremaining: 11m 46s\n",
      "695:\ttest: 0.9112368\tbest: 0.9112811 (692)\ttotal: 26m 52s\tremaining: 11m 44s\n",
      "696:\ttest: 0.9113745\tbest: 0.9113745 (696)\ttotal: 26m 58s\tremaining: 11m 43s\n",
      "697:\ttest: 0.9113846\tbest: 0.9113846 (697)\ttotal: 26m 59s\tremaining: 11m 40s\n",
      "698:\ttest: 0.9113883\tbest: 0.9113883 (698)\ttotal: 27m 1s\tremaining: 11m 38s\n",
      "699:\ttest: 0.9113714\tbest: 0.9113883 (698)\ttotal: 27m 3s\tremaining: 11m 35s\n",
      "700:\ttest: 0.9113842\tbest: 0.9113883 (698)\ttotal: 27m 6s\tremaining: 11m 33s\n",
      "701:\ttest: 0.9114217\tbest: 0.9114217 (701)\ttotal: 27m 8s\tremaining: 11m 31s\n",
      "702:\ttest: 0.9114092\tbest: 0.9114217 (701)\ttotal: 27m 10s\tremaining: 11m 28s\n",
      "703:\ttest: 0.9114902\tbest: 0.9114902 (703)\ttotal: 27m 13s\tremaining: 11m 26s\n",
      "704:\ttest: 0.9114896\tbest: 0.9114902 (703)\ttotal: 27m 16s\tremaining: 11m 24s\n",
      "705:\ttest: 0.9115531\tbest: 0.9115531 (705)\ttotal: 27m 20s\tremaining: 11m 22s\n",
      "706:\ttest: 0.9115559\tbest: 0.9115559 (706)\ttotal: 27m 22s\tremaining: 11m 20s\n",
      "707:\ttest: 0.9115656\tbest: 0.9115656 (707)\ttotal: 27m 24s\tremaining: 11m 18s\n",
      "708:\ttest: 0.9115681\tbest: 0.9115681 (708)\ttotal: 27m 26s\tremaining: 11m 15s\n",
      "709:\ttest: 0.9115834\tbest: 0.9115834 (709)\ttotal: 27m 29s\tremaining: 11m 13s\n",
      "710:\ttest: 0.9116108\tbest: 0.9116108 (710)\ttotal: 27m 31s\tremaining: 11m 11s\n",
      "711:\ttest: 0.9116106\tbest: 0.9116108 (710)\ttotal: 27m 33s\tremaining: 11m 8s\n",
      "712:\ttest: 0.9116466\tbest: 0.9116466 (712)\ttotal: 27m 35s\tremaining: 11m 6s\n",
      "713:\ttest: 0.9116468\tbest: 0.9116468 (713)\ttotal: 27m 37s\tremaining: 11m 4s\n",
      "714:\ttest: 0.9116434\tbest: 0.9116468 (713)\ttotal: 27m 42s\tremaining: 11m 2s\n",
      "715:\ttest: 0.9116433\tbest: 0.9116468 (713)\ttotal: 27m 45s\tremaining: 11m\n",
      "716:\ttest: 0.9116481\tbest: 0.9116481 (716)\ttotal: 27m 47s\tremaining: 10m 58s\n",
      "717:\ttest: 0.9116597\tbest: 0.9116597 (717)\ttotal: 27m 51s\tremaining: 10m 56s\n",
      "718:\ttest: 0.9116956\tbest: 0.9116956 (718)\ttotal: 27m 55s\tremaining: 10m 55s\n",
      "719:\ttest: 0.9116959\tbest: 0.9116959 (719)\ttotal: 27m 57s\tremaining: 10m 52s\n",
      "720:\ttest: 0.9117036\tbest: 0.9117036 (720)\ttotal: 28m\tremaining: 10m 50s\n",
      "721:\ttest: 0.9117362\tbest: 0.9117362 (721)\ttotal: 28m 5s\tremaining: 10m 48s\n",
      "722:\ttest: 0.9117262\tbest: 0.9117362 (721)\ttotal: 28m 9s\tremaining: 10m 47s\n",
      "723:\ttest: 0.9117363\tbest: 0.9117363 (723)\ttotal: 28m 14s\tremaining: 10m 45s\n",
      "724:\ttest: 0.9117377\tbest: 0.9117377 (724)\ttotal: 28m 18s\tremaining: 10m 44s\n",
      "725:\ttest: 0.9117548\tbest: 0.9117548 (725)\ttotal: 28m 22s\tremaining: 10m 42s\n",
      "726:\ttest: 0.9117561\tbest: 0.9117561 (726)\ttotal: 28m 25s\tremaining: 10m 40s\n",
      "727:\ttest: 0.9117540\tbest: 0.9117561 (726)\ttotal: 28m 29s\tremaining: 10m 38s\n",
      "728:\ttest: 0.9117644\tbest: 0.9117644 (728)\ttotal: 28m 32s\tremaining: 10m 36s\n",
      "729:\ttest: 0.9117654\tbest: 0.9117654 (729)\ttotal: 28m 35s\tremaining: 10m 34s\n",
      "730:\ttest: 0.9117540\tbest: 0.9117654 (729)\ttotal: 28m 38s\tremaining: 10m 32s\n",
      "731:\ttest: 0.9117603\tbest: 0.9117654 (729)\ttotal: 28m 40s\tremaining: 10m 29s\n",
      "732:\ttest: 0.9117639\tbest: 0.9117654 (729)\ttotal: 28m 43s\tremaining: 10m 27s\n",
      "733:\ttest: 0.9117641\tbest: 0.9117654 (729)\ttotal: 28m 45s\tremaining: 10m 25s\n",
      "734:\ttest: 0.9117655\tbest: 0.9117655 (734)\ttotal: 28m 48s\tremaining: 10m 23s\n",
      "735:\ttest: 0.9121672\tbest: 0.9121672 (735)\ttotal: 28m 51s\tremaining: 10m 21s\n",
      "736:\ttest: 0.9121636\tbest: 0.9121672 (735)\ttotal: 28m 53s\tremaining: 10m 18s\n",
      "737:\ttest: 0.9121636\tbest: 0.9121672 (735)\ttotal: 28m 55s\tremaining: 10m 16s\n",
      "738:\ttest: 0.9121639\tbest: 0.9121672 (735)\ttotal: 28m 57s\tremaining: 10m 13s\n",
      "739:\ttest: 0.9121606\tbest: 0.9121672 (735)\ttotal: 29m\tremaining: 10m 11s\n",
      "740:\ttest: 0.9121620\tbest: 0.9121672 (735)\ttotal: 29m 2s\tremaining: 10m 9s\n",
      "741:\ttest: 0.9121622\tbest: 0.9121672 (735)\ttotal: 29m 4s\tremaining: 10m 6s\n",
      "742:\ttest: 0.9121642\tbest: 0.9121672 (735)\ttotal: 29m 6s\tremaining: 10m 4s\n",
      "743:\ttest: 0.9121680\tbest: 0.9121680 (743)\ttotal: 29m 10s\tremaining: 10m 2s\n",
      "744:\ttest: 0.9121840\tbest: 0.9121840 (744)\ttotal: 29m 12s\tremaining: 9m 59s\n",
      "745:\ttest: 0.9121817\tbest: 0.9121840 (744)\ttotal: 29m 14s\tremaining: 9m 57s\n",
      "746:\ttest: 0.9121625\tbest: 0.9121840 (744)\ttotal: 29m 16s\tremaining: 9m 54s\n",
      "747:\ttest: 0.9121631\tbest: 0.9121840 (744)\ttotal: 29m 18s\tremaining: 9m 52s\n",
      "748:\ttest: 0.9121621\tbest: 0.9121840 (744)\ttotal: 29m 20s\tremaining: 9m 50s\n",
      "749:\ttest: 0.9121684\tbest: 0.9121840 (744)\ttotal: 29m 23s\tremaining: 9m 47s\n",
      "750:\ttest: 0.9121669\tbest: 0.9121840 (744)\ttotal: 29m 25s\tremaining: 9m 45s\n",
      "751:\ttest: 0.9121697\tbest: 0.9121840 (744)\ttotal: 29m 27s\tremaining: 9m 42s\n",
      "752:\ttest: 0.9121700\tbest: 0.9121840 (744)\ttotal: 29m 30s\tremaining: 9m 40s\n",
      "753:\ttest: 0.9121703\tbest: 0.9121840 (744)\ttotal: 29m 32s\tremaining: 9m 38s\n",
      "754:\ttest: 0.9121835\tbest: 0.9121840 (744)\ttotal: 29m 34s\tremaining: 9m 35s\n",
      "755:\ttest: 0.9121754\tbest: 0.9121840 (744)\ttotal: 29m 35s\tremaining: 9m 33s\n",
      "756:\ttest: 0.9121759\tbest: 0.9121840 (744)\ttotal: 29m 37s\tremaining: 9m 30s\n",
      "757:\ttest: 0.9121737\tbest: 0.9121840 (744)\ttotal: 29m 40s\tremaining: 9m 28s\n",
      "758:\ttest: 0.9121786\tbest: 0.9121840 (744)\ttotal: 29m 42s\tremaining: 9m 25s\n",
      "759:\ttest: 0.9121788\tbest: 0.9121840 (744)\ttotal: 29m 44s\tremaining: 9m 23s\n",
      "760:\ttest: 0.9122006\tbest: 0.9122006 (760)\ttotal: 29m 46s\tremaining: 9m 21s\n",
      "761:\ttest: 0.9122017\tbest: 0.9122017 (761)\ttotal: 29m 49s\tremaining: 9m 18s\n",
      "762:\ttest: 0.9125609\tbest: 0.9125609 (762)\ttotal: 29m 52s\tremaining: 9m 16s\n",
      "763:\ttest: 0.9125571\tbest: 0.9125609 (762)\ttotal: 29m 55s\tremaining: 9m 14s\n",
      "764:\ttest: 0.9125631\tbest: 0.9125631 (764)\ttotal: 29m 57s\tremaining: 9m 12s\n",
      "765:\ttest: 0.9125593\tbest: 0.9125631 (764)\ttotal: 30m 1s\tremaining: 9m 10s\n",
      "766:\ttest: 0.9125607\tbest: 0.9125631 (764)\ttotal: 30m 4s\tremaining: 9m 8s\n",
      "767:\ttest: 0.9125640\tbest: 0.9125640 (767)\ttotal: 30m 6s\tremaining: 9m 5s\n",
      "768:\ttest: 0.9125581\tbest: 0.9125640 (767)\ttotal: 30m 8s\tremaining: 9m 3s\n",
      "769:\ttest: 0.9125619\tbest: 0.9125640 (767)\ttotal: 30m 10s\tremaining: 9m\n",
      "770:\ttest: 0.9126269\tbest: 0.9126269 (770)\ttotal: 30m 14s\tremaining: 8m 58s\n",
      "771:\ttest: 0.9126228\tbest: 0.9126269 (770)\ttotal: 30m 16s\tremaining: 8m 56s\n",
      "772:\ttest: 0.9126221\tbest: 0.9126269 (770)\ttotal: 30m 19s\tremaining: 8m 54s\n",
      "773:\ttest: 0.9126248\tbest: 0.9126269 (770)\ttotal: 30m 21s\tremaining: 8m 51s\n",
      "774:\ttest: 0.9126142\tbest: 0.9126269 (770)\ttotal: 30m 24s\tremaining: 8m 49s\n",
      "775:\ttest: 0.9126139\tbest: 0.9126269 (770)\ttotal: 30m 27s\tremaining: 8m 47s\n",
      "776:\ttest: 0.9126243\tbest: 0.9126269 (770)\ttotal: 30m 29s\tremaining: 8m 45s\n",
      "777:\ttest: 0.9126329\tbest: 0.9126329 (777)\ttotal: 30m 31s\tremaining: 8m 42s\n",
      "778:\ttest: 0.9126924\tbest: 0.9126924 (778)\ttotal: 30m 34s\tremaining: 8m 40s\n",
      "779:\ttest: 0.9126998\tbest: 0.9126998 (779)\ttotal: 30m 36s\tremaining: 8m 37s\n",
      "780:\ttest: 0.9127061\tbest: 0.9127061 (780)\ttotal: 30m 38s\tremaining: 8m 35s\n",
      "781:\ttest: 0.9127114\tbest: 0.9127114 (781)\ttotal: 30m 40s\tremaining: 8m 33s\n",
      "782:\ttest: 0.9129609\tbest: 0.9129609 (782)\ttotal: 30m 43s\tremaining: 8m 30s\n",
      "783:\ttest: 0.9129771\tbest: 0.9129771 (783)\ttotal: 30m 45s\tremaining: 8m 28s\n",
      "784:\ttest: 0.9129858\tbest: 0.9129858 (784)\ttotal: 30m 46s\tremaining: 8m 25s\n",
      "785:\ttest: 0.9129836\tbest: 0.9129858 (784)\ttotal: 30m 48s\tremaining: 8m 23s\n",
      "786:\ttest: 0.9129790\tbest: 0.9129858 (784)\ttotal: 30m 51s\tremaining: 8m 21s\n",
      "787:\ttest: 0.9130087\tbest: 0.9130087 (787)\ttotal: 30m 53s\tremaining: 8m 18s\n",
      "788:\ttest: 0.9130071\tbest: 0.9130087 (787)\ttotal: 30m 55s\tremaining: 8m 16s\n",
      "789:\ttest: 0.9130073\tbest: 0.9130087 (787)\ttotal: 30m 57s\tremaining: 8m 13s\n",
      "790:\ttest: 0.9130084\tbest: 0.9130087 (787)\ttotal: 31m\tremaining: 8m 11s\n",
      "791:\ttest: 0.9130149\tbest: 0.9130149 (791)\ttotal: 31m 1s\tremaining: 8m 9s\n",
      "792:\ttest: 0.9130198\tbest: 0.9130198 (792)\ttotal: 31m 3s\tremaining: 8m 6s\n",
      "793:\ttest: 0.9130114\tbest: 0.9130198 (792)\ttotal: 31m 5s\tremaining: 8m 4s\n",
      "794:\ttest: 0.9130236\tbest: 0.9130236 (794)\ttotal: 31m 7s\tremaining: 8m 1s\n",
      "795:\ttest: 0.9130437\tbest: 0.9130437 (795)\ttotal: 31m 11s\tremaining: 7m 59s\n",
      "796:\ttest: 0.9130535\tbest: 0.9130535 (796)\ttotal: 31m 13s\tremaining: 7m 57s\n",
      "797:\ttest: 0.9130553\tbest: 0.9130553 (797)\ttotal: 31m 15s\tremaining: 7m 54s\n",
      "798:\ttest: 0.9130581\tbest: 0.9130581 (798)\ttotal: 31m 17s\tremaining: 7m 52s\n",
      "799:\ttest: 0.9130673\tbest: 0.9130673 (799)\ttotal: 31m 19s\tremaining: 7m 49s\n",
      "800:\ttest: 0.9130561\tbest: 0.9130673 (799)\ttotal: 31m 21s\tremaining: 7m 47s\n",
      "801:\ttest: 0.9130620\tbest: 0.9130673 (799)\ttotal: 31m 23s\tremaining: 7m 44s\n",
      "802:\ttest: 0.9130677\tbest: 0.9130677 (802)\ttotal: 31m 26s\tremaining: 7m 42s\n",
      "803:\ttest: 0.9130718\tbest: 0.9130718 (803)\ttotal: 31m 29s\tremaining: 7m 40s\n",
      "804:\ttest: 0.9130686\tbest: 0.9130718 (803)\ttotal: 31m 31s\tremaining: 7m 38s\n",
      "805:\ttest: 0.9130671\tbest: 0.9130718 (803)\ttotal: 31m 33s\tremaining: 7m 35s\n",
      "806:\ttest: 0.9130680\tbest: 0.9130718 (803)\ttotal: 31m 35s\tremaining: 7m 33s\n",
      "807:\ttest: 0.9130714\tbest: 0.9130718 (803)\ttotal: 31m 37s\tremaining: 7m 30s\n",
      "808:\ttest: 0.9130861\tbest: 0.9130861 (808)\ttotal: 31m 39s\tremaining: 7m 28s\n",
      "809:\ttest: 0.9130857\tbest: 0.9130861 (808)\ttotal: 31m 40s\tremaining: 7m 25s\n",
      "810:\ttest: 0.9130870\tbest: 0.9130870 (810)\ttotal: 31m 42s\tremaining: 7m 23s\n",
      "811:\ttest: 0.9130950\tbest: 0.9130950 (811)\ttotal: 31m 44s\tremaining: 7m 21s\n",
      "812:\ttest: 0.9130962\tbest: 0.9130962 (812)\ttotal: 31m 46s\tremaining: 7m 18s\n",
      "813:\ttest: 0.9130963\tbest: 0.9130963 (813)\ttotal: 31m 48s\tremaining: 7m 16s\n",
      "814:\ttest: 0.9131008\tbest: 0.9131008 (814)\ttotal: 31m 50s\tremaining: 7m 13s\n",
      "815:\ttest: 0.9131063\tbest: 0.9131063 (815)\ttotal: 31m 52s\tremaining: 7m 11s\n",
      "816:\ttest: 0.9131013\tbest: 0.9131063 (815)\ttotal: 31m 54s\tremaining: 7m 8s\n",
      "817:\ttest: 0.9131140\tbest: 0.9131140 (817)\ttotal: 31m 56s\tremaining: 7m 6s\n",
      "818:\ttest: 0.9131155\tbest: 0.9131155 (818)\ttotal: 31m 58s\tremaining: 7m 3s\n",
      "819:\ttest: 0.9131134\tbest: 0.9131155 (818)\ttotal: 32m\tremaining: 7m 1s\n",
      "820:\ttest: 0.9131186\tbest: 0.9131186 (820)\ttotal: 32m 4s\tremaining: 6m 59s\n",
      "821:\ttest: 0.9131215\tbest: 0.9131215 (821)\ttotal: 32m 6s\tremaining: 6m 57s\n",
      "822:\ttest: 0.9131392\tbest: 0.9131392 (822)\ttotal: 32m 8s\tremaining: 6m 54s\n",
      "823:\ttest: 0.9131413\tbest: 0.9131413 (823)\ttotal: 32m 10s\tremaining: 6m 52s\n",
      "824:\ttest: 0.9131271\tbest: 0.9131413 (823)\ttotal: 32m 11s\tremaining: 6m 49s\n",
      "825:\ttest: 0.9131333\tbest: 0.9131413 (823)\ttotal: 32m 13s\tremaining: 6m 47s\n",
      "826:\ttest: 0.9131318\tbest: 0.9131413 (823)\ttotal: 32m 15s\tremaining: 6m 44s\n",
      "827:\ttest: 0.9131293\tbest: 0.9131413 (823)\ttotal: 32m 17s\tremaining: 6m 42s\n",
      "828:\ttest: 0.9131242\tbest: 0.9131413 (823)\ttotal: 32m 19s\tremaining: 6m 40s\n",
      "829:\ttest: 0.9131251\tbest: 0.9131413 (823)\ttotal: 32m 21s\tremaining: 6m 37s\n",
      "830:\ttest: 0.9131284\tbest: 0.9131413 (823)\ttotal: 32m 23s\tremaining: 6m 35s\n",
      "831:\ttest: 0.9131226\tbest: 0.9131413 (823)\ttotal: 32m 25s\tremaining: 6m 32s\n",
      "832:\ttest: 0.9131227\tbest: 0.9131413 (823)\ttotal: 32m 27s\tremaining: 6m 30s\n",
      "833:\ttest: 0.9131247\tbest: 0.9131413 (823)\ttotal: 32m 29s\tremaining: 6m 27s\n",
      "834:\ttest: 0.9131260\tbest: 0.9131413 (823)\ttotal: 32m 31s\tremaining: 6m 25s\n",
      "835:\ttest: 0.9131245\tbest: 0.9131413 (823)\ttotal: 32m 33s\tremaining: 6m 23s\n",
      "836:\ttest: 0.9131198\tbest: 0.9131413 (823)\ttotal: 32m 35s\tremaining: 6m 20s\n",
      "837:\ttest: 0.9131305\tbest: 0.9131413 (823)\ttotal: 32m 36s\tremaining: 6m 18s\n",
      "838:\ttest: 0.9131250\tbest: 0.9131413 (823)\ttotal: 32m 38s\tremaining: 6m 15s\n",
      "839:\ttest: 0.9131223\tbest: 0.9131413 (823)\ttotal: 32m 40s\tremaining: 6m 13s\n",
      "840:\ttest: 0.9131254\tbest: 0.9131413 (823)\ttotal: 32m 43s\tremaining: 6m 11s\n",
      "841:\ttest: 0.9131299\tbest: 0.9131413 (823)\ttotal: 32m 45s\tremaining: 6m 8s\n",
      "842:\ttest: 0.9131321\tbest: 0.9131413 (823)\ttotal: 32m 47s\tremaining: 6m 6s\n",
      "843:\ttest: 0.9131315\tbest: 0.9131413 (823)\ttotal: 32m 49s\tremaining: 6m 3s\n",
      "844:\ttest: 0.9131491\tbest: 0.9131491 (844)\ttotal: 32m 51s\tremaining: 6m 1s\n",
      "845:\ttest: 0.9131524\tbest: 0.9131524 (845)\ttotal: 32m 52s\tremaining: 5m 59s\n",
      "846:\ttest: 0.9131513\tbest: 0.9131524 (845)\ttotal: 32m 54s\tremaining: 5m 56s\n",
      "847:\ttest: 0.9131556\tbest: 0.9131556 (847)\ttotal: 32m 56s\tremaining: 5m 54s\n",
      "848:\ttest: 0.9131542\tbest: 0.9131556 (847)\ttotal: 32m 58s\tremaining: 5m 51s\n",
      "849:\ttest: 0.9131500\tbest: 0.9131556 (847)\ttotal: 33m\tremaining: 5m 49s\n",
      "850:\ttest: 0.9131544\tbest: 0.9131556 (847)\ttotal: 33m 2s\tremaining: 5m 47s\n",
      "851:\ttest: 0.9131580\tbest: 0.9131580 (851)\ttotal: 33m 4s\tremaining: 5m 44s\n",
      "852:\ttest: 0.9131576\tbest: 0.9131580 (851)\ttotal: 33m 6s\tremaining: 5m 42s\n",
      "853:\ttest: 0.9131573\tbest: 0.9131580 (851)\ttotal: 33m 8s\tremaining: 5m 39s\n",
      "854:\ttest: 0.9131616\tbest: 0.9131616 (854)\ttotal: 33m 10s\tremaining: 5m 37s\n",
      "855:\ttest: 0.9131553\tbest: 0.9131616 (854)\ttotal: 33m 12s\tremaining: 5m 35s\n",
      "856:\ttest: 0.9131541\tbest: 0.9131616 (854)\ttotal: 33m 14s\tremaining: 5m 32s\n",
      "857:\ttest: 0.9131497\tbest: 0.9131616 (854)\ttotal: 33m 16s\tremaining: 5m 30s\n",
      "858:\ttest: 0.9131513\tbest: 0.9131616 (854)\ttotal: 33m 18s\tremaining: 5m 28s\n",
      "859:\ttest: 0.9132463\tbest: 0.9132463 (859)\ttotal: 33m 21s\tremaining: 5m 25s\n",
      "860:\ttest: 0.9132414\tbest: 0.9132463 (859)\ttotal: 33m 24s\tremaining: 5m 23s\n",
      "861:\ttest: 0.9132467\tbest: 0.9132467 (861)\ttotal: 33m 26s\tremaining: 5m 21s\n",
      "862:\ttest: 0.9132461\tbest: 0.9132467 (861)\ttotal: 33m 28s\tremaining: 5m 18s\n",
      "863:\ttest: 0.9132501\tbest: 0.9132501 (863)\ttotal: 33m 30s\tremaining: 5m 16s\n",
      "864:\ttest: 0.9132879\tbest: 0.9132879 (864)\ttotal: 33m 32s\tremaining: 5m 14s\n",
      "865:\ttest: 0.9132873\tbest: 0.9132879 (864)\ttotal: 33m 34s\tremaining: 5m 11s\n",
      "866:\ttest: 0.9132872\tbest: 0.9132879 (864)\ttotal: 33m 36s\tremaining: 5m 9s\n",
      "867:\ttest: 0.9132914\tbest: 0.9132914 (867)\ttotal: 33m 38s\tremaining: 5m 6s\n",
      "868:\ttest: 0.9132872\tbest: 0.9132914 (867)\ttotal: 33m 40s\tremaining: 5m 4s\n",
      "869:\ttest: 0.9132876\tbest: 0.9132914 (867)\ttotal: 33m 42s\tremaining: 5m 2s\n",
      "870:\ttest: 0.9132858\tbest: 0.9132914 (867)\ttotal: 33m 44s\tremaining: 4m 59s\n",
      "871:\ttest: 0.9132951\tbest: 0.9132951 (871)\ttotal: 33m 46s\tremaining: 4m 57s\n",
      "872:\ttest: 0.9132933\tbest: 0.9132951 (871)\ttotal: 33m 48s\tremaining: 4m 55s\n",
      "873:\ttest: 0.9133005\tbest: 0.9133005 (873)\ttotal: 33m 50s\tremaining: 4m 52s\n",
      "874:\ttest: 0.9132989\tbest: 0.9133005 (873)\ttotal: 33m 53s\tremaining: 4m 50s\n",
      "875:\ttest: 0.9133054\tbest: 0.9133054 (875)\ttotal: 33m 55s\tremaining: 4m 48s\n",
      "876:\ttest: 0.9133118\tbest: 0.9133118 (876)\ttotal: 33m 57s\tremaining: 4m 45s\n",
      "877:\ttest: 0.9133120\tbest: 0.9133120 (877)\ttotal: 34m\tremaining: 4m 43s\n",
      "878:\ttest: 0.9133086\tbest: 0.9133120 (877)\ttotal: 34m 2s\tremaining: 4m 41s\n",
      "879:\ttest: 0.9132985\tbest: 0.9133120 (877)\ttotal: 34m 4s\tremaining: 4m 38s\n",
      "880:\ttest: 0.9133018\tbest: 0.9133120 (877)\ttotal: 34m 6s\tremaining: 4m 36s\n",
      "881:\ttest: 0.9133103\tbest: 0.9133120 (877)\ttotal: 34m 8s\tremaining: 4m 34s\n",
      "882:\ttest: 0.9133144\tbest: 0.9133144 (882)\ttotal: 34m 11s\tremaining: 4m 31s\n",
      "883:\ttest: 0.9133291\tbest: 0.9133291 (883)\ttotal: 34m 13s\tremaining: 4m 29s\n",
      "884:\ttest: 0.9133325\tbest: 0.9133325 (884)\ttotal: 34m 15s\tremaining: 4m 27s\n",
      "885:\ttest: 0.9133337\tbest: 0.9133337 (885)\ttotal: 34m 17s\tremaining: 4m 24s\n",
      "886:\ttest: 0.9134421\tbest: 0.9134421 (886)\ttotal: 34m 20s\tremaining: 4m 22s\n",
      "887:\ttest: 0.9134359\tbest: 0.9134421 (886)\ttotal: 34m 22s\tremaining: 4m 20s\n",
      "888:\ttest: 0.9134323\tbest: 0.9134421 (886)\ttotal: 34m 24s\tremaining: 4m 17s\n",
      "889:\ttest: 0.9134312\tbest: 0.9134421 (886)\ttotal: 34m 26s\tremaining: 4m 15s\n",
      "890:\ttest: 0.9134333\tbest: 0.9134421 (886)\ttotal: 34m 28s\tremaining: 4m 13s\n",
      "891:\ttest: 0.9134391\tbest: 0.9134421 (886)\ttotal: 34m 30s\tremaining: 4m 10s\n",
      "892:\ttest: 0.9134359\tbest: 0.9134421 (886)\ttotal: 34m 32s\tremaining: 4m 8s\n",
      "893:\ttest: 0.9134355\tbest: 0.9134421 (886)\ttotal: 34m 35s\tremaining: 4m 6s\n",
      "894:\ttest: 0.9134571\tbest: 0.9134571 (894)\ttotal: 34m 37s\tremaining: 4m 3s\n",
      "895:\ttest: 0.9134456\tbest: 0.9134571 (894)\ttotal: 34m 39s\tremaining: 4m 1s\n",
      "896:\ttest: 0.9134448\tbest: 0.9134571 (894)\ttotal: 34m 41s\tremaining: 3m 58s\n",
      "897:\ttest: 0.9134460\tbest: 0.9134571 (894)\ttotal: 34m 43s\tremaining: 3m 56s\n",
      "898:\ttest: 0.9134453\tbest: 0.9134571 (894)\ttotal: 34m 44s\tremaining: 3m 54s\n",
      "899:\ttest: 0.9134478\tbest: 0.9134571 (894)\ttotal: 34m 46s\tremaining: 3m 51s\n",
      "900:\ttest: 0.9134489\tbest: 0.9134571 (894)\ttotal: 34m 48s\tremaining: 3m 49s\n",
      "901:\ttest: 0.9134444\tbest: 0.9134571 (894)\ttotal: 34m 50s\tremaining: 3m 47s\n",
      "902:\ttest: 0.9134453\tbest: 0.9134571 (894)\ttotal: 34m 55s\tremaining: 3m 45s\n",
      "903:\ttest: 0.9134597\tbest: 0.9134597 (903)\ttotal: 34m 58s\tremaining: 3m 42s\n",
      "904:\ttest: 0.9134564\tbest: 0.9134597 (903)\ttotal: 35m\tremaining: 3m 40s\n",
      "905:\ttest: 0.9134647\tbest: 0.9134647 (905)\ttotal: 35m 2s\tremaining: 3m 38s\n",
      "906:\ttest: 0.9134679\tbest: 0.9134679 (906)\ttotal: 35m 4s\tremaining: 3m 35s\n",
      "907:\ttest: 0.9134659\tbest: 0.9134679 (906)\ttotal: 35m 6s\tremaining: 3m 33s\n",
      "908:\ttest: 0.9134735\tbest: 0.9134735 (908)\ttotal: 35m 8s\tremaining: 3m 31s\n",
      "909:\ttest: 0.9134833\tbest: 0.9134833 (909)\ttotal: 35m 10s\tremaining: 3m 28s\n",
      "910:\ttest: 0.9134838\tbest: 0.9134838 (910)\ttotal: 35m 12s\tremaining: 3m 26s\n",
      "911:\ttest: 0.9134836\tbest: 0.9134838 (910)\ttotal: 35m 14s\tremaining: 3m 24s\n",
      "912:\ttest: 0.9134937\tbest: 0.9134937 (912)\ttotal: 35m 16s\tremaining: 3m 21s\n",
      "913:\ttest: 0.9134933\tbest: 0.9134937 (912)\ttotal: 35m 18s\tremaining: 3m 19s\n",
      "914:\ttest: 0.9134922\tbest: 0.9134937 (912)\ttotal: 35m 21s\tremaining: 3m 17s\n",
      "915:\ttest: 0.9134928\tbest: 0.9134937 (912)\ttotal: 35m 23s\tremaining: 3m 14s\n",
      "916:\ttest: 0.9134931\tbest: 0.9134937 (912)\ttotal: 35m 25s\tremaining: 3m 12s\n",
      "917:\ttest: 0.9134922\tbest: 0.9134937 (912)\ttotal: 35m 27s\tremaining: 3m 10s\n",
      "918:\ttest: 0.9134881\tbest: 0.9134937 (912)\ttotal: 35m 29s\tremaining: 3m 7s\n",
      "919:\ttest: 0.9134884\tbest: 0.9134937 (912)\ttotal: 35m 31s\tremaining: 3m 5s\n",
      "920:\ttest: 0.9134864\tbest: 0.9134937 (912)\ttotal: 35m 33s\tremaining: 3m 2s\n",
      "921:\ttest: 0.9134876\tbest: 0.9134937 (912)\ttotal: 35m 35s\tremaining: 3m\n",
      "922:\ttest: 0.9134763\tbest: 0.9134937 (912)\ttotal: 35m 37s\tremaining: 2m 58s\n",
      "923:\ttest: 0.9134812\tbest: 0.9134937 (912)\ttotal: 35m 40s\tremaining: 2m 56s\n",
      "924:\ttest: 0.9134819\tbest: 0.9134937 (912)\ttotal: 35m 42s\tremaining: 2m 53s\n",
      "925:\ttest: 0.9134762\tbest: 0.9134937 (912)\ttotal: 35m 44s\tremaining: 2m 51s\n",
      "926:\ttest: 0.9134772\tbest: 0.9134937 (912)\ttotal: 35m 47s\tremaining: 2m 49s\n",
      "927:\ttest: 0.9134768\tbest: 0.9134937 (912)\ttotal: 35m 49s\tremaining: 2m 46s\n",
      "928:\ttest: 0.9134783\tbest: 0.9134937 (912)\ttotal: 35m 51s\tremaining: 2m 44s\n",
      "929:\ttest: 0.9134795\tbest: 0.9134937 (912)\ttotal: 35m 53s\tremaining: 2m 42s\n",
      "930:\ttest: 0.9134692\tbest: 0.9134937 (912)\ttotal: 35m 55s\tremaining: 2m 39s\n",
      "931:\ttest: 0.9134711\tbest: 0.9134937 (912)\ttotal: 35m 57s\tremaining: 2m 37s\n",
      "932:\ttest: 0.9134736\tbest: 0.9134937 (912)\ttotal: 36m 1s\tremaining: 2m 35s\n",
      "933:\ttest: 0.9134838\tbest: 0.9134937 (912)\ttotal: 36m 3s\tremaining: 2m 32s\n",
      "934:\ttest: 0.9134840\tbest: 0.9134937 (912)\ttotal: 36m 5s\tremaining: 2m 30s\n",
      "935:\ttest: 0.9134803\tbest: 0.9134937 (912)\ttotal: 36m 9s\tremaining: 2m 28s\n",
      "936:\ttest: 0.9134982\tbest: 0.9134982 (936)\ttotal: 36m 12s\tremaining: 2m 26s\n",
      "937:\ttest: 0.9134970\tbest: 0.9134982 (936)\ttotal: 36m 14s\tremaining: 2m 23s\n",
      "938:\ttest: 0.9135022\tbest: 0.9135022 (938)\ttotal: 36m 17s\tremaining: 2m 21s\n",
      "939:\ttest: 0.9135354\tbest: 0.9135354 (939)\ttotal: 36m 23s\tremaining: 2m 19s\n",
      "940:\ttest: 0.9135323\tbest: 0.9135354 (939)\ttotal: 36m 26s\tremaining: 2m 17s\n",
      "941:\ttest: 0.9135393\tbest: 0.9135393 (941)\ttotal: 36m 28s\tremaining: 2m 14s\n",
      "942:\ttest: 0.9135338\tbest: 0.9135393 (941)\ttotal: 36m 30s\tremaining: 2m 12s\n",
      "943:\ttest: 0.9135355\tbest: 0.9135393 (941)\ttotal: 36m 34s\tremaining: 2m 10s\n",
      "944:\ttest: 0.9135386\tbest: 0.9135393 (941)\ttotal: 36m 36s\tremaining: 2m 7s\n",
      "945:\ttest: 0.9135506\tbest: 0.9135506 (945)\ttotal: 36m 40s\tremaining: 2m 5s\n",
      "946:\ttest: 0.9135550\tbest: 0.9135550 (946)\ttotal: 36m 43s\tremaining: 2m 3s\n",
      "947:\ttest: 0.9135473\tbest: 0.9135550 (946)\ttotal: 36m 45s\tremaining: 2m\n",
      "948:\ttest: 0.9135382\tbest: 0.9135550 (946)\ttotal: 36m 48s\tremaining: 1m 58s\n",
      "949:\ttest: 0.9135381\tbest: 0.9135550 (946)\ttotal: 36m 52s\tremaining: 1m 56s\n",
      "950:\ttest: 0.9135390\tbest: 0.9135550 (946)\ttotal: 36m 54s\tremaining: 1m 54s\n",
      "951:\ttest: 0.9135474\tbest: 0.9135550 (946)\ttotal: 36m 56s\tremaining: 1m 51s\n",
      "952:\ttest: 0.9135480\tbest: 0.9135550 (946)\ttotal: 36m 58s\tremaining: 1m 49s\n",
      "953:\ttest: 0.9135548\tbest: 0.9135550 (946)\ttotal: 37m\tremaining: 1m 47s\n",
      "954:\ttest: 0.9135460\tbest: 0.9135550 (946)\ttotal: 37m 2s\tremaining: 1m 44s\n",
      "955:\ttest: 0.9135247\tbest: 0.9135550 (946)\ttotal: 37m 4s\tremaining: 1m 42s\n",
      "956:\ttest: 0.9135326\tbest: 0.9135550 (946)\ttotal: 37m 6s\tremaining: 1m 40s\n",
      "957:\ttest: 0.9135425\tbest: 0.9135550 (946)\ttotal: 37m 8s\tremaining: 1m 37s\n",
      "958:\ttest: 0.9135205\tbest: 0.9135550 (946)\ttotal: 37m 11s\tremaining: 1m 35s\n",
      "959:\ttest: 0.9134931\tbest: 0.9135550 (946)\ttotal: 37m 15s\tremaining: 1m 33s\n",
      "960:\ttest: 0.9134990\tbest: 0.9135550 (946)\ttotal: 37m 19s\tremaining: 1m 30s\n",
      "961:\ttest: 0.9135130\tbest: 0.9135550 (946)\ttotal: 37m 22s\tremaining: 1m 28s\n",
      "962:\ttest: 0.9135176\tbest: 0.9135550 (946)\ttotal: 37m 25s\tremaining: 1m 26s\n",
      "963:\ttest: 0.9135343\tbest: 0.9135550 (946)\ttotal: 37m 28s\tremaining: 1m 23s\n",
      "964:\ttest: 0.9135356\tbest: 0.9135550 (946)\ttotal: 37m 30s\tremaining: 1m 21s\n",
      "965:\ttest: 0.9135239\tbest: 0.9135550 (946)\ttotal: 37m 31s\tremaining: 1m 19s\n",
      "966:\ttest: 0.9135252\tbest: 0.9135550 (946)\ttotal: 37m 35s\tremaining: 1m 16s\n",
      "967:\ttest: 0.9135136\tbest: 0.9135550 (946)\ttotal: 37m 36s\tremaining: 1m 14s\n",
      "968:\ttest: 0.9135191\tbest: 0.9135550 (946)\ttotal: 37m 40s\tremaining: 1m 12s\n",
      "969:\ttest: 0.9135203\tbest: 0.9135550 (946)\ttotal: 37m 42s\tremaining: 1m 9s\n",
      "970:\ttest: 0.9135167\tbest: 0.9135550 (946)\ttotal: 37m 44s\tremaining: 1m 7s\n",
      "971:\ttest: 0.9135196\tbest: 0.9135550 (946)\ttotal: 37m 46s\tremaining: 1m 5s\n",
      "972:\ttest: 0.9135284\tbest: 0.9135550 (946)\ttotal: 37m 49s\tremaining: 1m 2s\n",
      "973:\ttest: 0.9135332\tbest: 0.9135550 (946)\ttotal: 37m 51s\tremaining: 1m\n",
      "974:\ttest: 0.9135397\tbest: 0.9135550 (946)\ttotal: 37m 53s\tremaining: 58.3s\n",
      "975:\ttest: 0.9135333\tbest: 0.9135550 (946)\ttotal: 37m 57s\tremaining: 56s\n",
      "976:\ttest: 0.9135497\tbest: 0.9135550 (946)\ttotal: 37m 59s\tremaining: 53.7s\n",
      "977:\ttest: 0.9135538\tbest: 0.9135550 (946)\ttotal: 38m 1s\tremaining: 51.3s\n",
      "978:\ttest: 0.9135686\tbest: 0.9135686 (978)\ttotal: 38m 3s\tremaining: 49s\n",
      "979:\ttest: 0.9135765\tbest: 0.9135765 (979)\ttotal: 38m 5s\tremaining: 46.6s\n",
      "980:\ttest: 0.9135755\tbest: 0.9135765 (979)\ttotal: 38m 7s\tremaining: 44.3s\n",
      "981:\ttest: 0.9135749\tbest: 0.9135765 (979)\ttotal: 38m 9s\tremaining: 42s\n",
      "982:\ttest: 0.9135614\tbest: 0.9135765 (979)\ttotal: 38m 11s\tremaining: 39.6s\n",
      "983:\ttest: 0.9135637\tbest: 0.9135765 (979)\ttotal: 38m 14s\tremaining: 37.3s\n",
      "984:\ttest: 0.9135641\tbest: 0.9135765 (979)\ttotal: 38m 16s\tremaining: 35s\n",
      "985:\ttest: 0.9135610\tbest: 0.9135765 (979)\ttotal: 38m 18s\tremaining: 32.6s\n",
      "986:\ttest: 0.9135718\tbest: 0.9135765 (979)\ttotal: 38m 20s\tremaining: 30.3s\n",
      "987:\ttest: 0.9135653\tbest: 0.9135765 (979)\ttotal: 38m 23s\tremaining: 28s\n",
      "988:\ttest: 0.9135843\tbest: 0.9135843 (988)\ttotal: 38m 24s\tremaining: 25.6s\n",
      "989:\ttest: 0.9135822\tbest: 0.9135843 (988)\ttotal: 38m 26s\tremaining: 23.3s\n",
      "990:\ttest: 0.9135810\tbest: 0.9135843 (988)\ttotal: 38m 28s\tremaining: 21s\n",
      "991:\ttest: 0.9135845\tbest: 0.9135845 (991)\ttotal: 38m 32s\tremaining: 18.6s\n",
      "992:\ttest: 0.9135876\tbest: 0.9135876 (992)\ttotal: 38m 34s\tremaining: 16.3s\n",
      "993:\ttest: 0.9135821\tbest: 0.9135876 (992)\ttotal: 38m 37s\tremaining: 14s\n",
      "994:\ttest: 0.9135809\tbest: 0.9135876 (992)\ttotal: 38m 39s\tremaining: 11.7s\n",
      "995:\ttest: 0.9135827\tbest: 0.9135876 (992)\ttotal: 38m 41s\tremaining: 9.32s\n",
      "996:\ttest: 0.9135909\tbest: 0.9135909 (996)\ttotal: 38m 45s\tremaining: 7s\n",
      "997:\ttest: 0.9135908\tbest: 0.9135909 (996)\ttotal: 38m 48s\tremaining: 4.67s\n",
      "998:\ttest: 0.9135865\tbest: 0.9135909 (996)\ttotal: 38m 50s\tremaining: 2.33s\n",
      "999:\ttest: 0.9135847\tbest: 0.9135909 (996)\ttotal: 38m 54s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9135909445\n",
      "bestIteration = 996\n",
      "\n",
      "Shrink model to first 997 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 17:01:31 - INFO - Сохранение модели CatBoostRanker...\n",
      "2025-04-27 17:01:32 - INFO - Загрузка модели CatBoostRanker...\n",
      "2025-04-27 17:01:34 - INFO - Генерация финальных рекомендаций...\n",
      "IOStream.flush timed out\n",
      "2025-04-27 17:05:02 - INFO - Расчет финальных метрик...\n",
      "2025-04-27 17:08:34 - INFO - Результаты:\n",
      "2025-04-27 17:08:34 - INFO - Метрики для K=10:\n",
      "2025-04-27 17:08:34 - INFO - NDCG@10: 0.8367\n",
      "2025-04-27 17:08:34 - INFO - Precision@10: 0.1200\n",
      "2025-04-27 17:08:34 - INFO - Recall@10: 0.3078\n",
      "2025-04-27 17:08:34 - INFO - Diversity@10: 0.0052\n",
      "2025-04-27 17:08:34 - INFO - Novelty@10: 0.6922\n",
      "2025-04-27 17:08:34 - INFO - Serendipity@10: 0.0013\n",
      "2025-04-27 17:08:34 - INFO - --------------------------------\n",
      "2025-04-27 17:08:34 - INFO - Метрики для K=100:\n",
      "2025-04-27 17:08:34 - INFO - NDCG@100: 0.8246\n",
      "2025-04-27 17:08:34 - INFO - Precision@100: 0.0691\n",
      "2025-04-27 17:08:34 - INFO - Recall@100: 0.3458\n",
      "2025-04-27 17:08:34 - INFO - Diversity@100: 0.0225\n",
      "2025-04-27 17:08:34 - INFO - Novelty@100: 0.6542\n",
      "2025-04-27 17:08:34 - INFO - Serendipity@100: 0.0007\n",
      "2025-04-27 17:08:34 - INFO - --------------------------------\n",
      "2025-04-27 17:08:34 - INFO - Метрики для K=1000:\n",
      "2025-04-27 17:08:34 - INFO - NDCG@1000: 0.8291\n",
      "2025-04-27 17:08:34 - INFO - Precision@1000: 0.0473\n",
      "2025-04-27 17:08:34 - INFO - Recall@1000: 0.4843\n",
      "2025-04-27 17:08:34 - INFO - Diversity@1000: 0.0777\n",
      "2025-04-27 17:08:34 - INFO - Novelty@1000: 0.5157\n",
      "2025-04-27 17:08:34 - INFO - Serendipity@1000: 0.0005\n",
      "2025-04-27 17:08:34 - INFO - --------------------------------\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Обучение модели CatBoostRanker...\")\n",
    "model_params = {\n",
    "    \"learning_rate\": 0.004684283500572371,\n",
    "    \"iterations\": 1000,\n",
    "    \"depth\": 6,\n",
    "    \"l2_leaf_reg\": 5.36104374072008e-06,\n",
    "    \"random_seed\": 42,\n",
    "    \"thread_count\": -1,\n",
    "    \"verbose\": True,\n",
    "    \"task_type\": \"CPU\",\n",
    "    \"loss_function\": \"Logloss\",\n",
    "    \"eval_metric\": \"NDCG\",\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"random_strength\": 0.007652648646042903,\n",
    "    \"bagging_temperature\": 0.9487648394691982,\n",
    "    \"leaf_estimation_iterations\": 9,\n",
    "}\n",
    "\n",
    "cat_boost_ranker = CatBoostRanker(\n",
    "    learning_rate=model_params[\"learning_rate\"],\n",
    "    iterations=model_params[\"iterations\"],\n",
    "    depth=model_params[\"depth\"],\n",
    "    l2_leaf_reg=model_params[\"l2_leaf_reg\"],\n",
    "    random_seed=42,\n",
    "    thread_count=model_params[\"thread_count\"],\n",
    "    verbose=model_params[\"verbose\"],\n",
    ")\n",
    "cat_boost_ranker.model_params.update(model_params)\n",
    "cat_boost_ranker.fit(train_data, candidates)\n",
    "\n",
    "logging.info(\"Сохранение модели CatBoostRanker...\")\n",
    "model_path = (\n",
    "    f\"{DATA_PATH}/models/cat_boost_ranker_{ORGANIZATION_ID}_{PROCESSING_DATE}.cbm\"\n",
    ")\n",
    "cat_boost_ranker.save_model(model_path)\n",
    "\n",
    "logging.info(\"Загрузка модели CatBoostRanker...\")\n",
    "cat_boost_ranker = CatBoostRanker.load_model(model_path)\n",
    "\n",
    "logging.info(\"Генерация финальных рекомендаций...\")\n",
    "ranked_recommendations = cat_boost_ranker.rank(\n",
    "    candidates=candidates, train_data=train_data, top_n=1000\n",
    ")\n",
    "\n",
    "recommendations_dict = (\n",
    "    ranked_recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    ")\n",
    "\n",
    "logging.info(\"Расчет финальных метрик...\")\n",
    "metrics_calculator = MetricsCalculator([10, 100, 1000])\n",
    "final_metrics = metrics_calculator.calculate(\n",
    "    recommendations=recommendations_dict,\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    item_categories=item_categories,\n",
    ")\n",
    "\n",
    "logging.info(\"Результаты:\")\n",
    "for k in metrics_calculator.k_values:\n",
    "    logging.info(f\"Метрики для K={k}:\")\n",
    "    logging.info(f\"NDCG@{k}: {final_metrics[f'ndcg_{k}']:.4f}\")\n",
    "    logging.info(f\"Precision@{k}: {final_metrics[f'precision_{k}']:.4f}\")\n",
    "    logging.info(f\"Recall@{k}: {final_metrics[f'recall_{k}']:.4f}\")\n",
    "    logging.info(f\"Diversity@{k}: {final_metrics[f'diversity_{k}']:.4f}\")\n",
    "    logging.info(f\"Novelty@{k}: {final_metrics[f'novelty_{k}']:.4f}\")\n",
    "    logging.info(f\"Serendipity@{k}: {final_metrics[f'serendipity_{k}']:.4f}\")\n",
    "    logging.info(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 17:08:34 - INFO - Обучение модели XGBoostRanker...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:09:30] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"ndcg_exp_gain\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 17:09:36 - INFO - Сохранение модели XGBoostRanker...\n",
      "2025-04-27 17:09:36 - INFO - Загрузка модели XGBoostRanker...\n",
      "2025-04-27 17:09:36 - INFO - Генерация финальных рекомендаций...\n",
      "2025-04-27 17:13:13 - INFO - Сохранение рекомендаций...\n",
      "2025-04-27 17:19:11 - INFO - Расчет финальных метрик...\n",
      "2025-04-27 17:22:42 - INFO - Результаты:\n",
      "2025-04-27 17:22:42 - INFO - Метрики для K=10:\n",
      "2025-04-27 17:22:42 - INFO - NDCG@10: 0.8367\n",
      "2025-04-27 17:22:42 - INFO - Precision@10: 0.0962\n",
      "2025-04-27 17:22:42 - INFO - Recall@10: 0.2067\n",
      "2025-04-27 17:22:42 - INFO - Diversity@10: 0.0046\n",
      "2025-04-27 17:22:42 - INFO - Novelty@10: 0.7933\n",
      "2025-04-27 17:22:42 - INFO - Serendipity@10: 0.0009\n",
      "2025-04-27 17:22:42 - INFO - --------------------------------\n",
      "2025-04-27 17:22:42 - INFO - Метрики для K=100:\n",
      "2025-04-27 17:22:42 - INFO - NDCG@100: 0.8246\n",
      "2025-04-27 17:22:42 - INFO - Precision@100: 0.0538\n",
      "2025-04-27 17:22:42 - INFO - Recall@100: 0.2220\n",
      "2025-04-27 17:22:42 - INFO - Diversity@100: 0.0142\n",
      "2025-04-27 17:22:42 - INFO - Novelty@100: 0.7780\n",
      "2025-04-27 17:22:42 - INFO - Serendipity@100: 0.0005\n",
      "2025-04-27 17:22:42 - INFO - --------------------------------\n",
      "2025-04-27 17:22:42 - INFO - Метрики для K=1000:\n",
      "2025-04-27 17:22:42 - INFO - NDCG@1000: 0.8291\n",
      "2025-04-27 17:22:42 - INFO - Precision@1000: 0.0371\n",
      "2025-04-27 17:22:42 - INFO - Recall@1000: 0.4017\n",
      "2025-04-27 17:22:42 - INFO - Diversity@1000: 0.0722\n",
      "2025-04-27 17:22:42 - INFO - Novelty@1000: 0.5983\n",
      "2025-04-27 17:22:42 - INFO - Serendipity@1000: 0.0003\n",
      "2025-04-27 17:22:42 - INFO - --------------------------------\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Обучение модели XGBoostRanker...\")\n",
    "model_params = {\n",
    "    \"learning_rate\": 0.008468008575248327,\n",
    "    \"max_depth\": 10,\n",
    "    \"n_estimators\": 233,\n",
    "    \"min_child_weight\": 0.24810409748678125,\n",
    "    \"subsample\": 0.5780093202212182,\n",
    "    \"colsample_bytree\": 0.5779972601681014,\n",
    "    \"gamma\": 0.0017073967431528124,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "mf_ranker = XGBoostRanker(**model_params)\n",
    "mf_ranker.fit(train_data)\n",
    "\n",
    "logging.info(\"Сохранение модели XGBoostRanker...\")\n",
    "model_path = f\"{DATA_PATH}/models/xgb_ranker_{ORGANIZATION_ID}_{PROCESSING_DATE}.xgb\"\n",
    "mf_ranker.save_model(model_path)\n",
    "\n",
    "logging.info(\"Загрузка модели XGBoostRanker...\")\n",
    "mf_ranker = XGBoostRanker.load_model(model_path)\n",
    "\n",
    "logging.info(\"Генерация финальных рекомендаций...\")\n",
    "ranked_recommendations = mf_ranker.rank(\n",
    "    candidates=candidates, top_n=1000, train_data=train_data\n",
    ")\n",
    "\n",
    "logging.info(\"Сохранение рекомендаций...\")\n",
    "ranked_recommendations.to_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_xgb_recommendations.csv\", index=False\n",
    ")\n",
    "\n",
    "recommendations_dict = (\n",
    "    ranked_recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    ")\n",
    "\n",
    "logging.info(\"Расчет финальных метрик...\")\n",
    "metrics_calculator = MetricsCalculator([10, 100, 1000])\n",
    "final_metrics = metrics_calculator.calculate(\n",
    "    recommendations=recommendations_dict,\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    item_categories=item_categories,\n",
    ")\n",
    "\n",
    "logging.info(\"Результаты:\")\n",
    "for k in metrics_calculator.k_values:\n",
    "    logging.info(f\"Метрики для K={k}:\")\n",
    "    logging.info(f\"NDCG@{k}: {final_metrics[f'ndcg_{k}']:.4f}\")\n",
    "    logging.info(f\"Precision@{k}: {final_metrics[f'precision_{k}']:.4f}\")\n",
    "    logging.info(f\"Recall@{k}: {final_metrics[f'recall_{k}']:.4f}\")\n",
    "    logging.info(f\"Diversity@{k}: {final_metrics[f'diversity_{k}']:.4f}\")\n",
    "    logging.info(f\"Novelty@{k}: {final_metrics[f'novelty_{k}']:.4f}\")\n",
    "    logging.info(f\"Serendipity@{k}: {final_metrics[f'serendipity_{k}']:.4f}\")\n",
    "    logging.info(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лучшие гиперпараметры моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightFM (Матричная факторизация)**:\n",
    "```json\n",
    "{\n",
    "    \"n_components\": 184,\n",
    "    \"learning_rate\": 0.36596795884334166,\n",
    "    \"loss\": \"warp-kos\",\n",
    "    \"n_epochs\": 23,\n",
    "}\n",
    "```\n",
    "\n",
    "**CatBoost (Градиентный бустинг над решающими деревьями)**:\n",
    "```json\n",
    "{\n",
    "    \"learning_rate\": 0.004684283500572371, \n",
    "    \"depth\": 6, \n",
    "    \"l2_leaf_reg\": 5.36104374072008e-06,\n",
    "    \"random_strength\": 0.007652648646042903,\n",
    "    \"bagging_temperature\": 0.9487648394691982,\n",
    "    \"leaf_estimation_iterations\": 9\n",
    "}\n",
    "```\n",
    "\n",
    "**XGBoost (Градиентный бустинг над решающими деревьями)**\n",
    "```json\n",
    "{\n",
    "    \"learning_rate\": 0.008468008575248327,\n",
    "    \"max_depth\": 10,\n",
    "    \"n_estimators\": 233,\n",
    "    \"min_child_weight\": 0.24810409748678125,\n",
    "    \"subsample\": 0.5780093202212182,\n",
    "    \"colsample_bytree\": 0.5779972601681014,\n",
    "    \"gamma\": 0.0017073967431528124,\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики работы моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Метрика  | LightFM  | CatBoost  | XGBRanker  |\n",
    "|---|---|---|---|\n",
    "|  Время обучения |  9 мин. |  60 мин. |  1 мин. |\n",
    "|  Время получения предсказаний |  2 мин. |  4 мин. |  4 мин. |\n",
    "|  NDCG@10 |  0.8367 |  0.8367 |  0.8367 |\n",
    "|  NDCG@100 |  0.8246 |  0.8246 |  0.8246 |\n",
    "|  NDCG@1000 |  0.8291 |  0.8291 |  0.8291 |\n",
    "|  Precision@10 |  0.0021 |  0.1180 |  0.0962 |\n",
    "|  Precision@100 |  0.0018 |  0.0681 |  0.0538 |\n",
    "|  Precision@1000 |  0.0024 |  0.0466 |  0.0371 |\n",
    "|  Recall@10 |  0.0037 |  0.3042 |  0.2067 |\n",
    "|  Recall@100 |  0.0142 |  0.3436 |  0.2220 |\n",
    "|  Recall@1000 |  0.2632 |  0.4828 |  0.4017 |\n",
    "|  Diversity@10 |  0.0055 |  0.0053 |  0.0046 |\n",
    "|  Diversity@100 |  0.0224 |  0.0225 |  0.0142 |\n",
    "|  Diversity@1000 |  0.0777 |  0.0777 |  0.0722 |\n",
    "|  Novelty@10 |  0.9963 |  0.6958 |  0.7933 |\n",
    "|  Novelty@100 |  0.9858 |  0.6564 |  0.7780 |\n",
    "|  Novelty@1000 |  0.7368 |  0.5172 |  0.5983 |\n",
    "|  Serendipity@10 |  0.0001 |  0.0011 |  0.0009 |\n",
    "|  Serendipity@100 |  0.0001 |  0.0006 |  0.0005 |\n",
    "|  Serendipity@1000 |  0.0001 |  0.0004 |  0.0003 |\n",
    "\n",
    "**Сравнение временных затрат производилось на одинаковых датасетах и одинаковом оборудовании на платформе Intel Ice Lake с 32 vCPU и 256 Гб RAM**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе работы было произведено:\n",
    "- анализ подходящих моделей и алгоритмов машинного обучения для ранжирования сгенерированных кандидатов\n",
    "- реализована модель матричной факторизации - LightFM\n",
    "- реализована модель градиентного бустинга над решающими деревьями - CatBoost\n",
    "- реализована модель градиентного бустинга над решающими деревьями - XGBoost\n",
    "- проведено обучение моделей\n",
    "- выполнен анализ результатов работы моделей\n",
    "\n",
    "Согласно результатам работы моделей, значения ключевой метрики - NDCG - одинаковое у всех.\n",
    "\n",
    "CatBoost был отброшен в следствии долгого времени обучения.\n",
    "\n",
    "Выбор между LightFM и XGBRanker пал в пользу XGBRanker - быстрее обучается, предоставляет достаточно новые для пользователя рекомендации, но не столь экстремально новые, как LightFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
