{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор моделей для этапа ранжирования кандидатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание\n",
    "Для пакетной рекомендательной системы характерны 3 этапа работы:\n",
    "1. Генерация кандидатов. На этом этапе происходит отбор сотен или тысяч объектов-кандидатов из множества объектов, которых могут быть миллиарды. Цель заключается в том, чтобы включить в этот набор макси-мально релевантные пользователю объекты и исключить нерелевантные.\n",
    "2. Скоринг и ранжирование. Здесь система оценивает кандидатов, полученных на предыдущем этапе, и присваивает каждому из них баллы, основанные на их релевантности для пользователя. Т.к. количество объек-тов на этом этапе ограничено, то можно использовать больший набор при-знаков объектов, контекстные факторы, например время и геопозицию.\n",
    "3. Повторное ранжирование. В рамках данного этапа проводится корректировка списка кандидатов, применяются бизнес-правила, использу-ются метрики разнообразия и новизны для уточнения позиций объектов-кандидатов в итоговом списке рекомендаций.\n",
    "\n",
    "В рамках этой задачи выполняется подбор моделей для этапа скоринга и ранжирования.\n",
    "Ограничение: использование только CPU.\n",
    "\n",
    "### Имеющиеся данные\n",
    "Социально-демографическая информация о пользователях и список их покупок - дата, название, цена, количество.\n",
    "Список сгенерированных кандидатов в разрезе покупателей.\n",
    "\n",
    "Разделение на трейн и тест: даты покупок. Самые последние покупки в тесте.\n",
    "Названия датафреймов с данными: train_data, test_data, candidates.\n",
    "\n",
    "**Состав train_data и test_data**:\n",
    "```\n",
    " 0   buyer_id                           object  идентификатор покупателя\n",
    " 1   buyer_birth_date                   object  дата рождения покупателя\n",
    " 2   buyer_sms_allowed                  int64   покупатель разрешил присылать СМС\n",
    " 3   buyer_emails_allowed               int64   покупатель разрешил присылать email\n",
    " 4   buyer_account_status_loyal         int64   покупатель в статусе \"Лояльный\" (0 или 1)\n",
    " 5   buyer_account_status_unregistered  int64   покупатель в статусе \"Не зарегистрирован\" (0 или 1)\n",
    " 6   buyer_account_status_new           int64   покупатель в статусе \"Новый\" (0 или 1)\n",
    " 7   buyer_account_status_potential     int64   покупатель в статусе \"Потенциальный\" (0 или 1)\n",
    " 8   buyer_account_status_lost          int64   покупатель в статусе \"Потерянный\" (0 или 1)\n",
    " 9   buyer_account_status_sleeping      int64   покупатель в статусе \"Спящий\" (0 или 1)\n",
    " 10  buyer_is_female                    int64   покупатель - женщина (0 или 1)\n",
    " 11  buyer_age                          int64   возраст покупателя\n",
    " 12  order_id                           object  идентификатор покупки\n",
    " 13  order_date                         object  дата покупки\n",
    " 14  order_day_of_week                  int64   день недели, в который была совершена покупка\n",
    " 15  order_hour_of_day                  int64   час дня, в который была совершена покупка\n",
    " 16  product_id                         object  идентификатор товара\n",
    " 17  product_name                       object  название товара\n",
    " 18  product_group                      object  название группы товара\n",
    " 19  product_count                      float64 количество товара\n",
    " 20  product_sum                        float64 сумма за товар\n",
    "```\n",
    "\n",
    "**Состав списка сгенерированных кандидатов**:\n",
    "```\n",
    " 0   buyer_id                           object  идентификатор покупателя\n",
    " 1   product_id                         object  идентификатор товара\n",
    "```\n",
    "\n",
    "\n",
    "### Метрики\n",
    "- NDCG@K\n",
    "- Precision@K\n",
    "- Recall@K\n",
    "- Diversity@K\n",
    "- Novelty@K\n",
    "- Serendipity@K\n",
    "\n",
    "Оптимизируем метрику NDCG@K, т.к. на этапе скоринга и ранжирования основной задачей является формирование упорядоченного по релевантности списка рекомендаций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорты и конфигурация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from sklearn.metrics import ndcg_score\n",
    "import logging\n",
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "import optuna\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gc\n",
    "from catboost import CatBoost, Pool as CBPool\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import optuna\n",
    "from optuna.trial import Trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORGANIZATION_ID = \"\"\n",
    "PROCESSING_DATE = \"\"\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классы-утилиты и общие методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCalculator:\n",
    "    \"\"\"\n",
    "    Калькулятор метрик для оценки качества рекомендаций.\n",
    "\n",
    "    Args:\n",
    "        k_values: список значений K для расчета метрик @K\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k_values: List[int]):\n",
    "        self.k_values = sorted(k_values)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_ndcg_for_user(data_tuple: Tuple[str, Dict[str, np.ndarray], int]) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Вспомогательная функция для расчета NDCG одного пользователя.\n",
    "        \n",
    "        Args:\n",
    "            data_tuple: кортеж (user_id, user_data, k) с данными пользователя и параметром k\n",
    "            \n",
    "        Returns:\n",
    "            Optional[float]: значение NDCG для пользователя или None в случае ошибки\n",
    "        \"\"\"\n",
    "        user_id, user_data, k = data_tuple\n",
    "        try:\n",
    "            return ndcg_score(\n",
    "                y_true=[user_data[\"ideal\"]], \n",
    "                y_score=[user_data[\"relevance\"]], \n",
    "                k=k\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Ошибка при расчете NDCG для пользователя {user_id}: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def _calculate_ndcg_at_k(\n",
    "        self,\n",
    "        k: int,\n",
    "        prepared_data: Dict[str, Dict[str, np.ndarray]],\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Расчет метрики NDCG@K (Normalized Discounted Cumulative Gain) для заданного значения K.\n",
    "\n",
    "        NDCG - это метрика, которая оценивает качество ранжирования рекомендаций с учетом\n",
    "        их позиции в списке. В отличие от Precision и Recall, NDCG учитывает порядок\n",
    "        рекомендаций, придавая больший вес релевантным товарам на верхних позициях.\n",
    "\n",
    "        Формула расчета:\n",
    "        DCG@K = rel₁ + Σᵢ₌₂ᵏ (relᵢ / log₂(i + 1))\n",
    "        NDCG@K = DCG@K / IDCG@K\n",
    "        где:\n",
    "        - relᵢ - релевантность i-го товара (в нашем случае 0 или 1)\n",
    "        - IDCG@K - максимально возможное значение DCG@K (идеальное ранжирование)\n",
    "\n",
    "        Args:\n",
    "            k: количество рекомендаций для оценки\n",
    "            prepared_data: словарь подготовленных данных для расчета NDCG\n",
    "                {\n",
    "                    user_id: {\n",
    "                        \"relevance\": np.ndarray,  # вектор релевантности рекомендаций\n",
    "                        \"ideal\": np.ndarray       # вектор идеального ранжирования\n",
    "                    }\n",
    "                }\n",
    "\n",
    "        Returns:\n",
    "            float: среднее значение NDCG@K по всем пользователям.\n",
    "            Возвращает 0.0 в следующих случаях:\n",
    "            - если нет данных для расчета\n",
    "            - если все попытки расчета NDCG завершились ошибкой\n",
    "            - если prepared_data пуст\n",
    "\n",
    "        Example:\n",
    "            >>> prepared_data = {\n",
    "            ...     'user1': {\n",
    "            ...         'relevance': np.array([1, 0, 1, 0]),\n",
    "            ...         'ideal': np.array([1, 1, 0, 0])\n",
    "            ...     }\n",
    "            ... }\n",
    "            >>> calculator._calculate_ndcg_for_k(2, prepared_data)\n",
    "            0.6309  # NDCG@2 для данного примера\n",
    "\n",
    "        Notes:\n",
    "            - Особенности метрики:\n",
    "            * Учитывает порядок рекомендаций\n",
    "            * Нормализована на [0, 1]\n",
    "            * Чувствительна к позициям релевантных товаров\n",
    "            * Использует логарифмическое дисконтирование\n",
    "\n",
    "            - Интерпретация значений:\n",
    "            * 1.0 - идеальное ранжирование\n",
    "            * 0.0 - наихудшее возможное ранжирование\n",
    "            * Типичные значения в реальных системах: 0.3-0.7\n",
    "\n",
    "            - Важные замечания:\n",
    "            * Метод использует готовые векторы релевантности\n",
    "            * Требует предварительной подготовки данных\n",
    "            * Обрабатывает ошибки для каждого пользователя отдельно\n",
    "            * Использует реализацию из sklearn.metrics\n",
    "        \"\"\"\n",
    "        if not prepared_data:\n",
    "            return 0.0\n",
    "\n",
    "        # Подготавливаем данные для параллельной обработки\n",
    "        data_for_parallel = [(user_id, user_data, k) for user_id, user_data in prepared_data.items()]\n",
    "\n",
    "        # Используем контекстный менеджер для автоматического закрытия пула\n",
    "        with Pool() as pool:\n",
    "            # Запускаем параллельные вычисления\n",
    "            results = pool.map(MetricsCalculator._calculate_ndcg_for_user, data_for_parallel)\n",
    "            \n",
    "            # Фильтруем None значения и считаем среднее\n",
    "            valid_results = [r for r in results if r is not None]\n",
    "            \n",
    "            return float(np.mean(valid_results)) if valid_results else 0.0\n",
    "        \n",
    "    def _calculate_precision_for_user(\n",
    "        self,\n",
    "        pred_items_at_k: List[str],\n",
    "        true_items: set[str],\n",
    "        k: int,\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Расчет Precision@K для одного пользователя.\n",
    "\n",
    "        Precision (точность) показывает, какая доля рекомендованных системой товаров\n",
    "        в топ-K оказалась релевантной для пользователя. Эта метрика оценивает\n",
    "        способность системы давать точные рекомендации без \"шума\".\n",
    "\n",
    "        Формула расчета:\n",
    "        Precision@K = |релевантные товары ∩ рекомендованные товары в топ-K| / K\n",
    "\n",
    "        Args:\n",
    "            pred_items_at_k: список первых K рекомендованных товаров\n",
    "            true_items: множество фактически купленных товаров\n",
    "            k: количество рекомендаций для оценки\n",
    "\n",
    "        Returns:\n",
    "            float: значение Precision@K для пользователя\n",
    "            None: если нет данных для расчета\n",
    "\n",
    "        Example:\n",
    "            >>> pred_items_at_k = ['item1', 'item2']\n",
    "            >>> true_items = {'item1', 'item4'}\n",
    "            >>> calculator._calculate_precision_for_user(pred_items_at_k, true_items, k=2)\n",
    "            0.5  # из двух рекомендованных товаров только один оказался релевантным\n",
    "\n",
    "        Notes:\n",
    "            - Высокое значение Precision означает, что большинство рекомендаций релевантны\n",
    "            - Особенности метрики:\n",
    "            * Всегда нормализована на K\n",
    "            * При увеличении K обычно уменьшается\n",
    "            * Не учитывает общее количество релевантных товаров\n",
    "            * Чувствительна к порядку рекомендаций (в топ-K попадают первые K товаров)\n",
    "        \"\"\"\n",
    "        if not pred_items_at_k:\n",
    "            return None\n",
    "\n",
    "        relevant_count = len(set(pred_items_at_k) & true_items)\n",
    "        return relevant_count / k\n",
    "\n",
    "    def _calculate_recall_for_user(\n",
    "        self,\n",
    "        pred_items_at_k: List[str],\n",
    "        true_items: set[str],\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Расчет метрики Recall@K (полноты) для одного пользователя.\n",
    "\n",
    "        Recall (полнота) показывает, какую долю от всех релевантных товаров пользователя\n",
    "        система смогла рекомендовать в топ-K рекомендациях. Эта метрика оценивает\n",
    "        способность системы находить все интересные пользователю товары.\n",
    "\n",
    "        Формула расчета:\n",
    "        Recall@K = |релевантные товары ∩ рекомендованные товары в топ-K| / |релевантные товары|\n",
    "\n",
    "        Args:\n",
    "            pred_items_at_k: список первых K рекомендованных товаров\n",
    "            true_items: множество фактически купленных товаров\n",
    "\n",
    "        Returns:\n",
    "            float: значение Recall@K для пользователя\n",
    "            None: если нет данных для расчета (пустой список рекомендаций или нет релевантных товаров)\n",
    "\n",
    "        Example:\n",
    "            >>> pred_items_at_k = ['item1', 'item2']\n",
    "            >>> true_items = {'item1', 'item2', 'item4'}\n",
    "            >>> calculator._calculate_recall_for_user(pred_items_at_k, true_items)\n",
    "            0.667  # из трех релевантных товаров (item1, item2, item4)\n",
    "                # в топ-2 рекомендациях найдено два (item1, item2)\n",
    "\n",
    "        Notes:\n",
    "            - Особенности метрики для одного пользователя:\n",
    "            * Чувствительна к количеству релевантных товаров\n",
    "            * При увеличении K обычно растет\n",
    "            * Может быть низкой, если у пользователя много релевантных товаров\n",
    "            * Не учитывает порядок рекомендаций в топ-K\n",
    "            * Не учитывает нерелевантные рекомендации\n",
    "\n",
    "            - Важные замечания:\n",
    "            * Возвращает None для пустого списка рекомендаций\n",
    "            * Возвращает None если нет релевантных товаров\n",
    "            * Значение всегда в диапазоне [0, 1]\n",
    "            * Равен 1.0, если все релевантные товары найдены\n",
    "            * Равен 0.0, если не найдено ни одного релевантного товара\n",
    "        \"\"\"\n",
    "        if not pred_items_at_k or not true_items:\n",
    "            return None\n",
    "\n",
    "        relevant_count = len(set(pred_items_at_k) & true_items)\n",
    "        return relevant_count / len(true_items)\n",
    "\n",
    "    def _calculate_diversity_for_user(\n",
    "        self,\n",
    "        pred_items_at_k: List[str],\n",
    "        item_categories: Dict[str, str],\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Расчет метрики Diversity (разнообразия) для одного пользователя.\n",
    "\n",
    "        Diversity (разнообразие) измеряет насколько разнообразны рекомендации с точки зрения\n",
    "        категорий товаров. Высокое разнообразие означает, что система способна рекомендовать\n",
    "        товары из разных категорий, а не концентрируется только на нескольких популярных\n",
    "        категориях.\n",
    "\n",
    "        Формула расчета:\n",
    "        Diversity = количество уникальных категорий в рекомендациях / общее количество категорий\n",
    "\n",
    "        Args:\n",
    "            pred_items_at_k: список рекомендованных товаров\n",
    "            item_categories: словарь соответствия товаров и их категорий {item_id: category_id}\n",
    "\n",
    "        Returns:\n",
    "            float: значение Diversity для пользователя\n",
    "            None: если нет данных для расчета (пустой список рекомендаций или нет категорий)\n",
    "\n",
    "        Example:\n",
    "            >>> pred_items_at_k = ['item1', 'item2', 'item3']\n",
    "            >>> item_categories = {\n",
    "            ...     'item1': 'category1',\n",
    "            ...     'item2': 'category1',\n",
    "            ...     'item3': 'category2'\n",
    "            ... }\n",
    "            >>> calculator._calculate_diversity_for_user(pred_items_at_k, item_categories)\n",
    "            0.5  # рекомендации содержат товары из 2 категорий из 4 возможных\n",
    "\n",
    "        Notes:\n",
    "            - Высокое значение разнообразия может указывать на то, что система\n",
    "            предлагает пользователям широкий спектр товаров\n",
    "            - Низкое значение может говорить о том, что система \"зациклилась\"\n",
    "            на определенных категориях\n",
    "            - Особенности метрики для одного пользователя:\n",
    "            * Нормализована на общее количество доступных категорий\n",
    "            * Не зависит от порядка рекомендаций\n",
    "            * Учитывает только уникальные категории\n",
    "            * Игнорирует товары без категорий\n",
    "\n",
    "            - Важные замечания:\n",
    "            * Возвращает None для пустого списка рекомендаций\n",
    "            * Возвращает None если нет информации о категориях\n",
    "            * Значение всегда в диапазоне [0, 1]\n",
    "            * Равен 1.0, если рекомендации охватывают все категории\n",
    "            * Равен 0.0, если все рекомендации из одной категории\n",
    "\n",
    "        \"\"\"\n",
    "        if not pred_items_at_k or not item_categories:\n",
    "            return None\n",
    "\n",
    "        total_categories = len(set(item_categories.values()))\n",
    "        if total_categories == 0:\n",
    "            return None\n",
    "\n",
    "        recommended_categories = {\n",
    "            item_categories[item] for item in pred_items_at_k if item in item_categories\n",
    "        }\n",
    "\n",
    "        if not recommended_categories:\n",
    "            return None\n",
    "\n",
    "        return len(recommended_categories) / total_categories\n",
    "\n",
    "    def _calculate_novelty_for_user(\n",
    "        self,\n",
    "        pred_items_at_k: List[str],\n",
    "        user_items: set[str],\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Расчет метрики Novelty (новизны) для одного пользователя.\n",
    "\n",
    "        Novelty (новизна) измеряет способность системы рекомендовать товары,\n",
    "        которые пользователь ранее не встречал в своей истории покупок.\n",
    "        Эта метрика помогает оценить, насколько система способна предлагать\n",
    "        новый контент вместо того, чтобы рекомендовать только то,\n",
    "        что пользователь уже покупал.\n",
    "\n",
    "        Формула расчета:\n",
    "        Novelty = количество новых товаров в рекомендациях / количество рекомендаций\n",
    "        где новый товар - это товар, которого нет в истории покупок пользователя.\n",
    "\n",
    "        Args:\n",
    "            pred_items_at_k: список рекомендованных товаров\n",
    "            user_items: множество товаров из истории покупок пользователя\n",
    "\n",
    "        Returns:\n",
    "            float: значение Novelty для пользователя\n",
    "            None: если нет данных для расчета (пустой список рекомендаций)\n",
    "\n",
    "        Example:\n",
    "            >>> pred_items_at_k = ['item1', 'item2', 'item3']\n",
    "            >>> user_items = {'item1', 'item4'}\n",
    "            >>> calculator._calculate_novelty_for_user(pred_items_at_k, user_items)\n",
    "            0.667  # из трех рекомендованных товаров два (item2, item3) являются новыми\n",
    "\n",
    "        Notes:\n",
    "            - Особенности метрики для одного пользователя:\n",
    "            * Нормализована на количество рекомендаций\n",
    "            * Не зависит от порядка рекомендаций\n",
    "            * Учитывает только факт наличия/отсутствия товара в истории\n",
    "            * Не учитывает популярность товаров среди других пользователей\n",
    "\n",
    "            - Важные замечания:\n",
    "            * Возвращает None для пустого списка рекомендаций\n",
    "            * Значение всегда в диапазоне [0, 1]\n",
    "            * Равен 1.0, если все рекомендованные товары новые\n",
    "            * Равен 0.0, если все рекомендованные товары уже были у пользователя\n",
    "            * Пустая история покупок считается валидной (все товары будут новыми)\n",
    "\n",
    "            - Слишком высокое значение новизны может также означать, что рекомендации\n",
    "            недостаточно персонализированы или слишком случайны\n",
    "        \"\"\"\n",
    "        if not pred_items_at_k:\n",
    "            return None\n",
    "\n",
    "        new_items_count = sum(1 for item in pred_items_at_k if item not in user_items)\n",
    "        return new_items_count / len(pred_items_at_k)\n",
    "\n",
    "    def _calculate_serendipity_for_user(\n",
    "        self,\n",
    "        pred_items_at_k: List[str],\n",
    "        true_items: set[str],\n",
    "        popular_items: set[str],\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Расчет метрики Serendipity (неожиданности) для одного пользователя.\n",
    "\n",
    "        Serendipity (неожиданность) измеряет способность системы рекомендовать релевантные,\n",
    "        но неочевидные товары. Товар считается неожиданным, если он релевантен для пользователя\n",
    "        (присутствует в тестовых данных) и при этом не входит в множество популярных товаров.\n",
    "\n",
    "        Эта метрика помогает оценить способность системы находить \"скрытые жемчужины\" -\n",
    "        товары, которые нравятся пользователю, но не являются очевидным выбором.\n",
    "\n",
    "        Формула расчета:\n",
    "        Serendipity = количество релевантных неожиданных товаров / количество рекомендаций\n",
    "        где релевантный неожиданный товар - это товар, который:\n",
    "        1) есть в тестовых покупках пользователя (релевантный)\n",
    "        2) не входит в множество популярных товаров (неожиданный)\n",
    "\n",
    "        Args:\n",
    "            pred_items_at_k: список рекомендованных товаров\n",
    "            true_items: множество фактически купленных товаров из тестового периода\n",
    "            popular_items: множество идентификаторов популярных товаров\n",
    "\n",
    "        Returns:\n",
    "            float: значение Serendipity для пользователя\n",
    "            None: если нет данных для расчета (пустой список рекомендаций)\n",
    "\n",
    "        Example:\n",
    "            >>> pred_items_at_k = ['item1', 'item2', 'item3']\n",
    "            >>> true_items = {'item1', 'item3'}\n",
    "            >>> popular_items = {'item1', 'item2'}\n",
    "            >>> calculator._calculate_serendipity_for_user(\n",
    "            ...     pred_items_at_k, true_items, popular_items\n",
    "            ... )\n",
    "            0.333  # только item3 является релевантным и неожиданным\n",
    "                # (item1 релевантный, но популярный; item2 популярный и нерелевантный)\n",
    "\n",
    "        Notes:\n",
    "            - Особенности метрики для одного пользователя:\n",
    "            * Нормализована на количество рекомендаций\n",
    "            * Не зависит от порядка рекомендаций\n",
    "            * Учитывает как релевантность, так и популярность\n",
    "            * Более строгая метрика, чем Novelty или Precision\n",
    "            * Помогает оценить качество \"неочевидных\" рекомендаций\n",
    "\n",
    "            - Важные замечания:\n",
    "            * Возвращает None для пустого списка рекомендаций\n",
    "            * Значение всегда в диапазоне [0, 1]\n",
    "            * Равен 1.0, если все рекомендации релевантные и непопулярные\n",
    "            * Равен 0.0, если нет релевантных непопулярных рекомендаций\n",
    "            * Обычно имеет более низкие значения, чем другие метрики\n",
    "            * Зависит от выбранного порога популярности товаров\n",
    "        \"\"\"\n",
    "        if not pred_items_at_k:\n",
    "            return None\n",
    "\n",
    "        serendipitous_items = sum(\n",
    "            1\n",
    "            for item in pred_items_at_k\n",
    "            if item in true_items and item not in popular_items\n",
    "        )\n",
    "\n",
    "        return serendipitous_items / len(pred_items_at_k)\n",
    "\n",
    "    def _prepare_data(\n",
    "        self,\n",
    "        recommendations: Dict[str, List[str]],\n",
    "        test_data: pd.DataFrame,\n",
    "    ) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Подготовка данных для расчета NDCG.\n",
    "\n",
    "        Args:\n",
    "            recommendations: словарь {user_id: list of recommended items}\n",
    "            test_data: тестовый датафрейм с колонками 'buyer_id' и 'product_id'\n",
    "\n",
    "        Returns:\n",
    "            Dict с подготовленными данными для каждого пользователя\n",
    "        \"\"\"\n",
    "        true_items = test_data.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    "\n",
    "        all_items = sorted(\n",
    "            list(set(item for items in recommendations.values() for item in items))\n",
    "        )\n",
    "        item_to_idx = {item: idx for idx, item in enumerate(all_items)}\n",
    "\n",
    "        prepared_data = {}\n",
    "\n",
    "        for user_id, pred_items in recommendations.items():\n",
    "            if user_id not in true_items:\n",
    "                continue\n",
    "\n",
    "            true_set = set(true_items[user_id])\n",
    "            n_items = len(all_items)\n",
    "\n",
    "            relevance = np.zeros(n_items)\n",
    "            for item in pred_items:\n",
    "                if item in item_to_idx:\n",
    "                    relevance[item_to_idx[item]] = 1 if item in true_set else 0\n",
    "\n",
    "            ideal = np.zeros(n_items)\n",
    "            for item in true_set:\n",
    "                if item in item_to_idx:\n",
    "                    ideal[item_to_idx[item]] = 1\n",
    "\n",
    "            prepared_data[user_id] = {\"relevance\": relevance, \"ideal\": ideal}\n",
    "\n",
    "        return prepared_data\n",
    "\n",
    "    def calculate(\n",
    "        self,\n",
    "        recommendations: Dict[str, List[str]],\n",
    "        train_data: pd.DataFrame,\n",
    "        test_data: pd.DataFrame,\n",
    "        item_categories: Optional[Dict[str, str]] = None,\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Расчет всех метрик\n",
    "\n",
    "        Args:\n",
    "            recommendations: словарь {user_id: list of recommended items}\n",
    "            train_data: тренировочный датафрейм для расчета новизны\n",
    "            test_data: тестовый датафрейм с колонками 'buyer_id' и 'product_id'\n",
    "            item_categories: словарь {item_id: category_id} для расчета разнообразия\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: словарь с метриками NDCG@K, Precision@K, Recall@K,\n",
    "            Diversity@K, Novelty@K и Serendipity@K для каждого значения K\n",
    "        \"\"\"\n",
    "        # Подготавливаем общие данные\n",
    "        prepared_data = self._prepare_data(recommendations, test_data)\n",
    "        true_items = test_data.groupby(\"buyer_id\")[\"product_id\"].agg(set).to_dict()\n",
    "        user_history = train_data.groupby(\"buyer_id\")[\"product_id\"].agg(set).to_dict()\n",
    "\n",
    "        # Подготавливаем данные для serendipity\n",
    "        item_counts = train_data[\"product_id\"].value_counts()\n",
    "        threshold = np.percentile(\n",
    "            item_counts.values, 80\n",
    "        )  # 1 - 0.2 (percentile по умолчанию)\n",
    "        popular_items = set(item_counts[item_counts >= threshold].index)\n",
    "\n",
    "        # Инициализируем словарь с метриками\n",
    "        metrics = {}\n",
    "\n",
    "        # Инициализируем списки для хранения значений метрик\n",
    "        precision_values = []\n",
    "        recall_values = []\n",
    "        diversity_values = []\n",
    "        novelty_values = []\n",
    "        serendipity_values = []\n",
    "\n",
    "        # Рассчитываем все метрики\n",
    "        for k in self.k_values:\n",
    "            metrics[f\"ndcg_{k}\"] = self._calculate_ndcg_at_k(k, prepared_data)\n",
    "\n",
    "            for user_id, pred_items in recommendations.items():\n",
    "                pred_items_at_k = pred_items[:k]\n",
    "                user_id_in_true_items = user_id in true_items\n",
    "\n",
    "                # Precision\n",
    "                if user_id_in_true_items:\n",
    "                    precision_value = self._calculate_precision_for_user(\n",
    "                        pred_items_at_k=pred_items_at_k,\n",
    "                        true_items=true_items[user_id],\n",
    "                        k=k,\n",
    "                    )\n",
    "                    if precision_value is not None:\n",
    "                        precision_values.append(precision_value)\n",
    "\n",
    "                # Recall\n",
    "                if user_id_in_true_items:\n",
    "                    recall_value = self._calculate_recall_for_user(\n",
    "                        pred_items_at_k=pred_items_at_k, true_items=true_items[user_id]\n",
    "                    )\n",
    "                    if recall_value is not None:\n",
    "                        recall_values.append(recall_value)\n",
    "\n",
    "                # Diversity\n",
    "                diversity_value = self._calculate_diversity_for_user(\n",
    "                    pred_items_at_k=pred_items_at_k,\n",
    "                    item_categories=item_categories or {},\n",
    "                )\n",
    "                if diversity_value is not None:\n",
    "                    diversity_values.append(diversity_value)\n",
    "\n",
    "                # Novelty\n",
    "                if user_id in user_history:\n",
    "                    novelty_value = self._calculate_novelty_for_user(\n",
    "                        pred_items_at_k=pred_items_at_k,\n",
    "                        user_items=user_history[user_id],\n",
    "                    )\n",
    "                    if novelty_value is not None:\n",
    "                        novelty_values.append(novelty_value)\n",
    "\n",
    "                # Serendipity\n",
    "                if user_id_in_true_items:\n",
    "                    serendipity_value = self._calculate_serendipity_for_user(\n",
    "                        pred_items_at_k=pred_items_at_k,\n",
    "                        true_items=true_items[user_id],\n",
    "                        popular_items=popular_items,\n",
    "                    )\n",
    "                    if serendipity_value is not None:\n",
    "                        serendipity_values.append(serendipity_value)\n",
    "\n",
    "            # Сохраняем средние значения метрик\n",
    "            metrics[f\"precision_{k}\"] = (\n",
    "                float(np.mean(precision_values)) if precision_values else 0.0\n",
    "            )\n",
    "            metrics[f\"recall_{k}\"] = (\n",
    "                float(np.mean(recall_values)) if recall_values else 0.0\n",
    "            )\n",
    "            metrics[f\"diversity_{k}\"] = (\n",
    "                float(np.mean(diversity_values)) if diversity_values else 0.0\n",
    "            )\n",
    "            metrics[f\"novelty_{k}\"] = (\n",
    "                float(np.mean(novelty_values)) if novelty_values else 0.0\n",
    "            )\n",
    "            metrics[f\"serendipity_{k}\"] = (\n",
    "                float(np.mean(serendipity_values)) if serendipity_values else 0.0\n",
    "            )\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бейзлайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_train.csv\"\n",
    ")\n",
    "test_data = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_test.csv\"\n",
    ")\n",
    "candidates = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_als_recommendations.csv\"\n",
    ")\n",
    "\n",
    "item_categories = {\n",
    "    row[\"product_id\"]: row[\"product_group\"] for _, row in train_data.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularItemsRanker:\n",
    "    \"\"\"\n",
    "    Ранжировщик на основе популярности товаров.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.item_scores_ = None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        item_col: str = \"product_id\",\n",
    "    ) -> \"PopularItemsRanker\":\n",
    "        \"\"\"\n",
    "        Обучение ранжировщика на исторических данных.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            Датафрейм с историческими данными о покупках\n",
    "        item_col : str, по умолчанию 'product_id'\n",
    "            Название столбца с идентификаторами товаров\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self : PopularItemsRanker\n",
    "            Обученный объект\n",
    "        \"\"\"\n",
    "        item_counts = data[item_col].value_counts()\n",
    "\n",
    "        self.item_scores_ = (item_counts / item_counts.max()).to_dict()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def recommend(\n",
    "        self,\n",
    "        candidates: pd.DataFrame,\n",
    "        top_n: int = 10,\n",
    "        user_col: str = \"buyer_id\",\n",
    "        item_col: str = \"product_id\",\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ранжирование кандидатов на основе популярности товаров.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "        candidates : pd.DataFrame\n",
    "            Датафрейм с парами пользователь-товар для ранжирования\n",
    "        top_n : int, по умолчанию 10\n",
    "            Количество рекомендуемых товаров для каждого пользователя\n",
    "        user_col : str, по умолчанию 'buyer_id'\n",
    "            Название столбца с идентификаторами пользователей\n",
    "        item_col : str, по умолчанию 'product_id'\n",
    "            Название столбца с идентификаторами товаров\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        pd.DataFrame\n",
    "            Датафрейм с ранжированными рекомендациями и их скорами\n",
    "        \"\"\"\n",
    "        if self.item_scores_ is None:\n",
    "            raise ValueError(\"Модель не обучена. Сначала вызовите метод fit().\")\n",
    "\n",
    "        recommendations = candidates.copy()\n",
    "\n",
    "        recommendations[\"score\"] = recommendations[item_col].map(\n",
    "            lambda x: self.item_scores_.get(x, 0.0)\n",
    "        )\n",
    "\n",
    "        recommendations = recommendations.sort_values(\n",
    "            by=[user_col, \"score\"], ascending=[True, False]\n",
    "        )\n",
    "\n",
    "        recommendations = recommendations.groupby(user_col).head(top_n)\n",
    "\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 11:21:30 - INFO - Создаем и обучаем ранжировщик\n",
      "2025-04-18 11:21:30 - INFO - Получаем рекомендации от ранжировщика\n",
      "2025-04-18 11:22:23 - INFO - Расчет метрик для рекомендаций от PopularItemsRanker\n",
      "2025-04-18 11:24:27 - INFO - Результаты:\n",
      "2025-04-18 11:24:27 - INFO - Метрики для K=10:\n",
      "2025-04-18 11:24:27 - INFO - NDCG@10: 0.3458\n",
      "2025-04-18 11:24:27 - INFO - Precision@10: 0.0623\n",
      "2025-04-18 11:24:27 - INFO - Recall@10: 0.1476\n",
      "2025-04-18 11:24:27 - INFO - Diversity@10: 0.0049\n",
      "2025-04-18 11:24:27 - INFO - Novelty@10: 0.8546\n",
      "2025-04-18 11:24:27 - INFO - Serendipity@10: 0.0000\n",
      "2025-04-18 11:24:27 - INFO - --------------------------------\n",
      "2025-04-18 11:24:27 - INFO - Метрики для K=100:\n",
      "2025-04-18 11:24:27 - INFO - NDCG@100: 0.4384\n",
      "2025-04-18 11:24:27 - INFO - Precision@100: 0.0342\n",
      "2025-04-18 11:24:27 - INFO - Recall@100: 0.1476\n",
      "2025-04-18 11:24:27 - INFO - Diversity@100: 0.0049\n",
      "2025-04-18 11:24:27 - INFO - Novelty@100: 0.8546\n",
      "2025-04-18 11:24:27 - INFO - Serendipity@100: 0.0000\n",
      "2025-04-18 11:24:27 - INFO - --------------------------------\n",
      "2025-04-18 11:24:27 - INFO - Метрики для K=1000:\n",
      "2025-04-18 11:24:27 - INFO - NDCG@1000: 0.4384\n",
      "2025-04-18 11:24:27 - INFO - Precision@1000: 0.0230\n",
      "2025-04-18 11:24:27 - INFO - Recall@1000: 0.1476\n",
      "2025-04-18 11:24:27 - INFO - Diversity@1000: 0.0049\n",
      "2025-04-18 11:24:27 - INFO - Novelty@1000: 0.8546\n",
      "2025-04-18 11:24:27 - INFO - Serendipity@1000: 0.0000\n",
      "2025-04-18 11:24:27 - INFO - --------------------------------\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Создаем и обучаем ранжировщик\")\n",
    "ranker = PopularItemsRanker()\n",
    "ranker.fit(train_data)\n",
    "\n",
    "logging.info(\"Получаем рекомендации от ранжировщика\")\n",
    "ranked_recommendations = ranker.recommend(candidates, top_n=10)\n",
    "\n",
    "ranker_recommendations = (\n",
    "    ranked_recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    ")\n",
    "\n",
    "metrics_calculator = MetricsCalculator(k_values=[10, 100, 1000])\n",
    "\n",
    "logging.info(\"Расчет метрик для рекомендаций от PopularItemsRanker\")\n",
    "best_metrics = metrics_calculator.calculate(\n",
    "    recommendations=ranker_recommendations,\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    item_categories=item_categories,\n",
    ")\n",
    "\n",
    "logging.info(\"Результаты:\")\n",
    "for k in metrics_calculator.k_values:\n",
    "    logging.info(f\"Метрики для K={k}:\")\n",
    "    logging.info(f\"NDCG@{k}: {best_metrics[f'ndcg_{k}']:.4f}\")\n",
    "    logging.info(f\"Precision@{k}: {best_metrics[f'precision_{k}']:.4f}\")\n",
    "    logging.info(f\"Recall@{k}: {best_metrics[f'recall_{k}']:.4f}\")\n",
    "    logging.info(f\"Diversity@{k}: {best_metrics[f'diversity_{k}']:.4f}\")\n",
    "    logging.info(f\"Novelty@{k}: {best_metrics[f'novelty_{k}']:.4f}\")\n",
    "    logging.info(f\"Serendipity@{k}: {best_metrics[f'serendipity_{k}']:.4f}\")\n",
    "    logging.info(\"--------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель 1: CatBoost (Градиентный бустинг над решающими деревьями)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_train.csv\"\n",
    ")\n",
    "test_data = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_test.csv\"\n",
    ")\n",
    "candidates = pd.read_csv(\n",
    "    f\"{DATA_PATH}/{ORGANIZATION_ID}_{PROCESSING_DATE}_als_recommendations.csv\"\n",
    ")\n",
    "\n",
    "item_categories = {\n",
    "    row[\"product_id\"]: row[\"product_group\"] for _, row in train_data.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostRanker:\n",
    "    \"\"\"\n",
    "    Ранжировщик на основе CatBoost.\n",
    "\n",
    "    Attrs:\n",
    "        model: CatBoost\n",
    "            Модель CatBoost для ранжирования\n",
    "        feature_names: List[str]\n",
    "            Список используемых признаков\n",
    "        categorical_features: List[str]\n",
    "            Список категориальных признаков\n",
    "        numeric_fill_values_: Dict[str, float]\n",
    "            Словарь для хранения значений для заполнения пропусков в числовых признаках\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        iterations: int = 1000,\n",
    "        depth: int = 6,\n",
    "        l2_leaf_reg: float = 3.0,\n",
    "        random_seed: int = 42,\n",
    "        thread_count: int = -1,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация ранжировщика.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: скорость обучения\n",
    "            iterations: количество итераций\n",
    "            depth: глубина деревьев\n",
    "            l2_leaf_reg: L2 регуляризация\n",
    "            random_seed: seed для воспроизводимости\n",
    "            thread_count: количество потоков (-1 для использования всех)\n",
    "            verbose: выводить ли прогресс обучения\n",
    "        \"\"\"\n",
    "        self.model_params = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"iterations\": iterations,\n",
    "            \"depth\": depth,\n",
    "            \"l2_leaf_reg\": l2_leaf_reg,\n",
    "            \"random_seed\": random_seed,\n",
    "            \"thread_count\": thread_count,\n",
    "            \"verbose\": verbose,\n",
    "            \"task_type\": \"CPU\",\n",
    "            \"loss_function\": \"Logloss\",\n",
    "            \"eval_metric\": \"NDCG\",\n",
    "            \"early_stopping_rounds\": 50,\n",
    "        }\n",
    "\n",
    "        self.feature_names = [\n",
    "            # Социально-демографические признаки\n",
    "            \"buyer_sms_allowed\",\n",
    "            \"buyer_emails_allowed\",\n",
    "            \"buyer_account_status_loyal\",\n",
    "            \"buyer_account_status_unregistered\",\n",
    "            \"buyer_account_status_new\",\n",
    "            \"buyer_account_status_potential\",\n",
    "            \"buyer_account_status_lost\",\n",
    "            \"buyer_account_status_sleeping\",\n",
    "            \"buyer_is_female\",\n",
    "            \"buyer_age\",\n",
    "            # Временные признаки\n",
    "            \"order_day_of_week\",\n",
    "            \"order_hour_of_day\",\n",
    "            # Признаки покупок\n",
    "            \"product_count\",\n",
    "            \"product_sum\",\n",
    "            # Категориальные признаки\n",
    "            \"product_group\",\n",
    "        ]\n",
    "\n",
    "        self.categorical_features = [\"product_group\"]\n",
    "        self.model = None\n",
    "        self.numeric_fill_values_ = {}  # Добавляем инициализацию словаря\n",
    "\n",
    "    def _prepare_features(\n",
    "        self, data: pd.DataFrame, is_train: bool = True\n",
    "    ) -> Tuple[pd.DataFrame, Optional[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Подготовка признаков для обучения или предсказания.\n",
    "\n",
    "        Args:\n",
    "            data: исходный датафрейм\n",
    "            is_train: флаг обучения/предсказания\n",
    "\n",
    "        Returns:\n",
    "            Tuple с подготовленными признаками и целевой переменной (если is_train=True)\n",
    "        \"\"\"\n",
    "        features = data[self.feature_names].copy()\n",
    "\n",
    "        for col in features.columns:\n",
    "            if col in self.categorical_features:\n",
    "                features[col] = features[col].fillna(\"unknown\")\n",
    "            else:\n",
    "                if is_train:\n",
    "                    fill_value = (\n",
    "                        features[col].median() if features[col].notnull().any() else 0\n",
    "                    )\n",
    "                    self.numeric_fill_values_ = self.numeric_fill_values_ or {}\n",
    "                    self.numeric_fill_values_[col] = fill_value\n",
    "                else:\n",
    "                    fill_value = self.numeric_fill_values_.get(col, 0)\n",
    "                features[col] = features[col].fillna(fill_value)\n",
    "\n",
    "        if is_train:\n",
    "            target = np.ones(len(features))\n",
    "            return features, target\n",
    "        else:\n",
    "            return features, None\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_data: pd.DataFrame,\n",
    "        candidates: pd.DataFrame,\n",
    "        validation_size: float = 0.2,\n",
    "    ) -> \"CatBoostRanker\":\n",
    "        \"\"\"\n",
    "        Обучение модели ранжирования.\n",
    "\n",
    "        Args:\n",
    "            train_data: исторические данные о покупках\n",
    "            candidates: датафрейм с кандидатами\n",
    "            validation_size: доля данных для валидации\n",
    "\n",
    "        Returns:\n",
    "            self: обученная модель\n",
    "        \"\"\"\n",
    "        train_data = train_data.sort_values(\"buyer_id\")\n",
    "        candidates = candidates.sort_values(\"buyer_id\")\n",
    "\n",
    "        X_pos, y_pos = self._prepare_features(train_data, is_train=True)\n",
    "        groups_pos = train_data[\"buyer_id\"].values\n",
    "\n",
    "        features_df = train_data[[\"buyer_id\", \"product_id\"] + self.feature_names].copy()\n",
    "        features_df = features_df.drop_duplicates(subset=[\"buyer_id\", \"product_id\"])\n",
    "\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "\n",
    "        chunk_size = 100000\n",
    "        X_chunks = []\n",
    "        groups_chunks = []\n",
    "        total_negative_samples = 0\n",
    "\n",
    "        for start_idx in tqdm(\n",
    "            range(0, len(candidates), chunk_size),\n",
    "            desc=\"Подготовка отрицательных примеров\",\n",
    "        ):\n",
    "            end_idx = start_idx + chunk_size\n",
    "            chunk = candidates.iloc[start_idx:end_idx]\n",
    "\n",
    "            chunk_with_features = chunk.merge(\n",
    "                features_df, on=[\"buyer_id\", \"product_id\"], how=\"left\"\n",
    "            )\n",
    "\n",
    "            X_chunk, _ = self._prepare_features(chunk_with_features, is_train=True)\n",
    "            groups_chunk = chunk_with_features[\"buyer_id\"].values\n",
    "\n",
    "            X_chunks.append(X_chunk)\n",
    "            groups_chunks.append(groups_chunk)\n",
    "            total_negative_samples += len(X_chunk)\n",
    "\n",
    "            del chunk_with_features, X_chunk, groups_chunk\n",
    "            gc.collect()\n",
    "\n",
    "        del features_df\n",
    "        gc.collect()\n",
    "\n",
    "        X_neg = pd.DataFrame(columns=X_pos.columns)\n",
    "        groups_neg = np.empty(total_negative_samples, dtype=groups_pos.dtype)\n",
    "        y_neg = np.zeros(total_negative_samples)\n",
    "\n",
    "        current_idx = 0\n",
    "        for X_chunk, groups_chunk in zip(X_chunks, groups_chunks):\n",
    "            chunk_size = len(X_chunk)\n",
    "            X_neg = pd.concat([X_neg, X_chunk], ignore_index=True)\n",
    "            groups_neg[current_idx : current_idx + chunk_size] = groups_chunk\n",
    "            current_idx += chunk_size\n",
    "\n",
    "            del X_chunk\n",
    "            gc.collect()\n",
    "\n",
    "        del X_chunks, groups_chunks\n",
    "        gc.collect()\n",
    "\n",
    "        X = pd.concat([X_pos, X_neg], ignore_index=True)\n",
    "        y = np.concatenate([y_pos, y_neg])\n",
    "        groups = np.concatenate([groups_pos, groups_neg])\n",
    "\n",
    "        del X_pos, X_neg, y_pos, y_neg, groups_pos, groups_neg\n",
    "        gc.collect()\n",
    "\n",
    "        sort_idx = np.argsort(groups)\n",
    "        X = X.iloc[sort_idx].reset_index(drop=True)\n",
    "        y = y[sort_idx]\n",
    "        groups = groups[sort_idx]\n",
    "\n",
    "        unique_groups = np.unique(groups)\n",
    "        n_groups = len(unique_groups)\n",
    "        np.random.shuffle(unique_groups)\n",
    "        train_groups = set(unique_groups[: int(n_groups * (1 - validation_size))])\n",
    "\n",
    "        train_mask = np.array([g in train_groups for g in groups])\n",
    "        valid_mask = ~train_mask\n",
    "\n",
    "        train_pool = CBPool(\n",
    "            data=X[train_mask],\n",
    "            label=y[train_mask],\n",
    "            group_id=groups[train_mask],\n",
    "            cat_features=self.categorical_features,\n",
    "        )\n",
    "\n",
    "        valid_pool = CBPool(\n",
    "            data=X[valid_mask],\n",
    "            label=y[valid_mask],\n",
    "            group_id=groups[valid_mask],\n",
    "            cat_features=self.categorical_features,\n",
    "        )\n",
    "\n",
    "        del X, y, groups, train_mask, valid_mask\n",
    "        gc.collect()\n",
    "\n",
    "        self.model = CatBoost(self.model_params)\n",
    "        self.model.fit(train_pool, eval_set=valid_pool, plot=False)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def rank(\n",
    "        self, candidates: pd.DataFrame, train_data: pd.DataFrame, top_n: int = 10\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ранжирование кандидатов.\n",
    "\n",
    "        Args:\n",
    "            candidates: датафрейм с кандидатами\n",
    "            train_data: исторические данные для получения признаков\n",
    "            top_n: количество рекомендаций для каждого пользователя\n",
    "\n",
    "        Returns:\n",
    "            DataFrame с ранжированными рекомендациями\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Модель не обучена. Сначала вызовите метод fit().\")\n",
    "\n",
    "        candidates = candidates.sort_values(\"buyer_id\").reset_index(drop=True)\n",
    "\n",
    "        features_df = train_data[\n",
    "            [\"buyer_id\", \"product_id\"] + self.feature_names\n",
    "        ].drop_duplicates()\n",
    "\n",
    "        candidates_with_features = candidates.merge(\n",
    "            features_df, on=[\"buyer_id\", \"product_id\"], how=\"left\"\n",
    "        )\n",
    "\n",
    "        if len(candidates_with_features) != len(candidates):\n",
    "            candidates_with_features = candidates_with_features.drop_duplicates(\n",
    "                subset=[\"buyer_id\", \"product_id\"]\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "        X_pred, _ = self._prepare_features(candidates_with_features, is_train=False)\n",
    "\n",
    "        scores = self.model.predict(\n",
    "            CBPool(data=X_pred, cat_features=self.categorical_features)\n",
    "        )\n",
    "\n",
    "        if len(scores) != len(candidates):\n",
    "            raise ValueError(\n",
    "                f\"Количество предсказаний ({len(scores)}) не совпадает с \"\n",
    "                f\"количеством кандидатов ({len(candidates)})\"\n",
    "            )\n",
    "\n",
    "        recommendations = candidates.copy()\n",
    "        recommendations[\"score\"] = scores\n",
    "\n",
    "        recommendations = recommendations.sort_values(\n",
    "            by=[\"buyer_id\", \"score\"], ascending=[True, False]\n",
    "        )\n",
    "        recommendations = recommendations.groupby(\"buyer_id\").head(top_n)\n",
    "\n",
    "        return recommendations[[\"buyer_id\", \"product_id\", \"score\"]]\n",
    "\n",
    "    def save_model(\n",
    "        self, model_path: str, format: str = \"cbm\", export_parameters: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Сохранение модели и её параметров.\n",
    "\n",
    "        Args:\n",
    "            model_path: путь для сохранения модели\n",
    "            format: формат сохранения ('cbm' или 'json')\n",
    "            export_parameters: сохранять ли дополнительные параметры модели\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Модель не обучена. Сначала вызовите метод fit()\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "        self.model.save_model(model_path, format=format, pool=None)\n",
    "\n",
    "        if export_parameters:\n",
    "            parameters = {\n",
    "                \"model_params\": self.model_params,\n",
    "                \"feature_names\": self.feature_names,\n",
    "                \"categorical_features\": self.categorical_features,\n",
    "                \"numeric_fill_values\": self.numeric_fill_values_,\n",
    "            }\n",
    "\n",
    "            params_path = f\"{os.path.splitext(model_path)[0]}_params.json\"\n",
    "\n",
    "            with open(params_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(parameters, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(\n",
    "        cls, model_path: str, load_parameters: bool = True\n",
    "    ) -> \"CatBoostRanker\":\n",
    "        \"\"\"\n",
    "        Загрузка сохранённой модели и её параметров.\n",
    "\n",
    "        Args:\n",
    "            model_path: путь к сохранённой модели\n",
    "            load_parameters: загружать ли дополнительные параметры модели\n",
    "\n",
    "        Returns:\n",
    "            CatBoostRanker: загруженная модель\n",
    "        \"\"\"\n",
    "        ranker = cls()\n",
    "\n",
    "        ranker.model = CatBoost()\n",
    "        ranker.model.load_model(model_path)\n",
    "\n",
    "        if load_parameters:\n",
    "            params_path = f\"{os.path.splitext(model_path)[0]}_params.json\"\n",
    "\n",
    "            if os.path.exists(params_path):\n",
    "                with open(params_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    parameters = json.load(f)\n",
    "\n",
    "                ranker.model_params = parameters[\"model_params\"]\n",
    "                ranker.feature_names = parameters[\"feature_names\"]\n",
    "                ranker.categorical_features = parameters[\"categorical_features\"]\n",
    "                ranker.numeric_fill_values_ = parameters[\"numeric_fill_values\"]\n",
    "            else:\n",
    "                logging.warning(\n",
    "                    f\"Файл с параметрами {params_path} не найден. \"\n",
    "                    \"Загружена только модель без дополнительных параметров.\"\n",
    "                )\n",
    "\n",
    "        return ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_catboost_parameters(\n",
    "    train_data: pd.DataFrame,\n",
    "    test_data: pd.DataFrame,\n",
    "    candidates: pd.DataFrame,\n",
    "    metrics_calculator: MetricsCalculator,\n",
    "    n_trials: int = 10,\n",
    "    timeout: int = 7200,\n",
    ") -> Tuple[Dict, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Оптимизация параметров CatBoostRanker с помощью Optuna.\n",
    "\n",
    "    Args:\n",
    "        train_data: тренировочные данные\n",
    "        test_data: тестовые данные\n",
    "        candidates: кандидаты для ранжирования\n",
    "        n_trials: количество итераций оптимизации\n",
    "        timeout: максимальное время оптимизации в секундах\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict, Dict[str, float]]:\n",
    "            - Лучшие параметры\n",
    "            - Значения метрик на лучших параметрах\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial: Trial) -> float:\n",
    "        model_params = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "            'iterations': 1000,\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "            'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 10),\n",
    "            'early_stopping_rounds': 50,\n",
    "            'task_type': 'CPU',\n",
    "            'thread_count': -1,\n",
    "            'verbose': False,\n",
    "            'eval_metric': 'NDCG',\n",
    "            'loss_function': 'Logloss'\n",
    "        }\n",
    "\n",
    "        ranker = CatBoostRanker(\n",
    "            learning_rate=model_params['learning_rate'],\n",
    "            iterations=model_params['iterations'],\n",
    "            depth=model_params['depth'],\n",
    "            l2_leaf_reg=model_params['l2_leaf_reg'],\n",
    "            random_seed=42,\n",
    "            thread_count=model_params['thread_count'],\n",
    "            verbose=model_params['verbose']\n",
    "        )\n",
    "\n",
    "        ranker.model_params.update(model_params)\n",
    "\n",
    "        ranker.fit(train_data, candidates)\n",
    "\n",
    "        recommendations = ranker.rank(\n",
    "            candidates=candidates,\n",
    "            train_data=train_data,\n",
    "            top_n=max([10, 100, 1000]),  # для расчета всех метрик\n",
    "        )\n",
    "\n",
    "        recommendations_dict = (\n",
    "            recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    "        )\n",
    "\n",
    "        metrics = metrics_calculator.calculate(\n",
    "            recommendations=recommendations_dict,\n",
    "            train_data=train_data,\n",
    "            test_data=test_data,\n",
    "            item_categories=item_categories,\n",
    "        )\n",
    "\n",
    "        target_k = max(metrics_calculator.k_values)\n",
    "        return metrics[f\"ndcg_{target_k}\"]\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        study_name=f'catboost_ranker_optimization_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}',\n",
    "    )\n",
    "\n",
    "    logging.info(\"Начало оптимизации параметров...\")\n",
    "    study.optimize(\n",
    "        objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    best_model_params = {\n",
    "        'learning_rate': best_params['learning_rate'],\n",
    "        'iterations': 1000,\n",
    "        'depth': best_params['depth'],\n",
    "        'l2_leaf_reg': best_params['l2_leaf_reg'],\n",
    "        'random_seed': 42,\n",
    "        'thread_count': -1,\n",
    "        'verbose': True,\n",
    "        'task_type': 'CPU',\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'NDCG',\n",
    "        'early_stopping_rounds': 50,\n",
    "        'random_strength': best_params['random_strength'],\n",
    "        'bagging_temperature': best_params['bagging_temperature'],\n",
    "        'leaf_estimation_iterations': best_params['leaf_estimation_iterations']\n",
    "    }\n",
    "\n",
    "    logging.info(\"Обучение модели на лучших параметрах...\")\n",
    "    best_ranker = CatBoostRanker(\n",
    "        learning_rate=best_model_params['learning_rate'],\n",
    "        iterations=best_model_params['iterations'],\n",
    "        depth=best_model_params['depth'],\n",
    "        l2_leaf_reg=best_model_params['l2_leaf_reg'],\n",
    "        random_seed=42,\n",
    "        thread_count=best_model_params['thread_count'],\n",
    "        verbose=best_model_params['verbose']\n",
    "    )\n",
    "    best_ranker.model_params.update(best_model_params)\n",
    "    best_ranker.fit(train_data, candidates)\n",
    "\n",
    "    recommendations = best_ranker.rank(\n",
    "        candidates=candidates, train_data=train_data, top_n=max([10, 100, 1000])\n",
    "    )\n",
    "\n",
    "    recommendations_dict = (\n",
    "        recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    "    )\n",
    "\n",
    "    final_metrics = metrics_calculator.calculate(\n",
    "        recommendations=recommendations_dict,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        item_categories=item_categories,\n",
    "    )\n",
    "\n",
    "    logging.info(\"\\nЛучшие параметры:\")\n",
    "    for param_name, param_value in best_params.items():\n",
    "        logging.info(f\"{param_name}: {param_value}\")\n",
    "\n",
    "    logging.info(\"\\nМетрики на лучших параметрах:\")\n",
    "    for metric_name, metric_value in final_metrics.items():\n",
    "        logging.info(f\"{metric_name}: {metric_value:.4f}\")\n",
    "\n",
    "    return best_params, final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-21 12:00:34,560] A new study created in memory with name: catboost_ranker_optimization_20250421_120034\n",
      "2025-04-21 12:00:34 - INFO - Начало оптимизации параметров...\n",
      "Подготовка отрицательных примеров: 100%|██████████| 601/601 [05:16<00:00,  1.90it/s]\n",
      "IOStream.flush timed out\n",
      "  0%|          | 0/10 [1:28:41<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-21 13:29:15,605] Trial 0 finished with value: 0.8317919801478025 and parameters: {'learning_rate': 0.004684283500572371, 'depth': 6, 'l2_leaf_reg': 5.36104374072008e-06, 'random_strength': 0.007652648646042903, 'bagging_temperature': 0.9487648394691982, 'leaf_estimation_iterations': 9}. Best is trial 0 with value: 0.8317919801478025.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Подготовка отрицательных примеров: 100%|██████████| 601/601 [04:52<00:00,  2.05it/s]5s/it, 5321.05/7200 seconds]\n",
      "Best trial: 0. Best value: 0.831792:  10%|█         | 1/10 [2:37:40<13:18:09, 5321.05s/it, 5321.05/7200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-21 14:38:15,522] Trial 1 finished with value: 0.8317919801478025 and parameters: {'learning_rate': 0.02388179353529231, 'depth': 5, 'l2_leaf_reg': 4.8339398459122034e-08, 'random_strength': 0.0031822125482293437, 'bagging_temperature': 0.5517318159431245, 'leaf_estimation_iterations': 2}. Best is trial 0 with value: 0.8317919801478025.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.831792:  20%|██        | 2/10 [2:37:40<10:30:43, 4730.48s/it, 9460.96/7200 seconds]\n",
      "2025-04-21 14:38:15 - INFO - Обучение модели на лучших параметрах...\n",
      "Подготовка отрицательных примеров: 100%|██████████| 601/601 [04:13<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 6.48s\tremaining: 1h 47m 55s\n",
      "1:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 12.9s\tremaining: 1h 47m 21s\n",
      "2:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 19s\tremaining: 1h 45m 24s\n",
      "3:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 22.2s\tremaining: 1h 32m 9s\n",
      "4:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 25.6s\tremaining: 1h 24m 45s\n",
      "5:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 28.8s\tremaining: 1h 19m 32s\n",
      "6:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 32.1s\tremaining: 1h 15m 53s\n",
      "7:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 35.3s\tremaining: 1h 12m 59s\n",
      "8:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 38.6s\tremaining: 1h 10m 54s\n",
      "9:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 41.9s\tremaining: 1h 9m 12s\n",
      "10:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 45.2s\tremaining: 1h 7m 42s\n",
      "11:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 48.6s\tremaining: 1h 6m 37s\n",
      "12:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 51.8s\tremaining: 1h 5m 34s\n",
      "13:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 55.1s\tremaining: 1h 4m 42s\n",
      "14:\ttest: 0.7768302\tbest: 0.7768302 (0)\ttotal: 58.3s\tremaining: 1h 3m 50s\n",
      "15:\ttest: 0.7779675\tbest: 0.7779675 (15)\ttotal: 1m 1s\tremaining: 1h 3m 13s\n",
      "16:\ttest: 0.7779675\tbest: 0.7779675 (15)\ttotal: 1m 4s\tremaining: 1h 2m 35s\n",
      "17:\ttest: 0.7779675\tbest: 0.7779675 (15)\ttotal: 1m 8s\tremaining: 1h 2m 3s\n",
      "18:\ttest: 0.7779353\tbest: 0.7779675 (15)\ttotal: 1m 11s\tremaining: 1h 1m 39s\n",
      "19:\ttest: 0.7779353\tbest: 0.7779675 (15)\ttotal: 1m 14s\tremaining: 1h 1m 7s\n",
      "20:\ttest: 0.7819910\tbest: 0.7819910 (20)\ttotal: 1m 18s\tremaining: 1h 40s\n",
      "21:\ttest: 0.7819920\tbest: 0.7819920 (21)\ttotal: 1m 21s\tremaining: 1h 16s\n",
      "22:\ttest: 0.7819910\tbest: 0.7819920 (21)\ttotal: 1m 24s\tremaining: 59m 56s\n",
      "23:\ttest: 0.7819897\tbest: 0.7819920 (21)\ttotal: 1m 28s\tremaining: 1h 17s\n",
      "24:\ttest: 0.7825440\tbest: 0.7825440 (24)\ttotal: 1m 32s\tremaining: 59m 59s\n",
      "25:\ttest: 0.7825850\tbest: 0.7825850 (25)\ttotal: 1m 36s\tremaining: 1h 22s\n",
      "26:\ttest: 0.7825926\tbest: 0.7825926 (26)\ttotal: 1m 39s\tremaining: 1h 3s\n",
      "27:\ttest: 0.7824892\tbest: 0.7825926 (26)\ttotal: 1m 43s\tremaining: 59m 47s\n",
      "28:\ttest: 0.7825577\tbest: 0.7825926 (26)\ttotal: 1m 47s\tremaining: 1h 4s\n",
      "29:\ttest: 0.7825435\tbest: 0.7825926 (26)\ttotal: 1m 51s\tremaining: 59m 49s\n",
      "30:\ttest: 0.7825916\tbest: 0.7825926 (26)\ttotal: 1m 54s\tremaining: 59m 29s\n",
      "31:\ttest: 0.7826242\tbest: 0.7826242 (31)\ttotal: 1m 57s\tremaining: 59m 10s\n",
      "32:\ttest: 0.7826181\tbest: 0.7826242 (31)\ttotal: 2m\tremaining: 58m 54s\n",
      "33:\ttest: 0.7825956\tbest: 0.7826242 (31)\ttotal: 2m 3s\tremaining: 58m 38s\n",
      "34:\ttest: 0.7826103\tbest: 0.7826242 (31)\ttotal: 2m 7s\tremaining: 58m 23s\n",
      "35:\ttest: 0.7826058\tbest: 0.7826242 (31)\ttotal: 2m 10s\tremaining: 58m 10s\n",
      "36:\ttest: 0.7825954\tbest: 0.7826242 (31)\ttotal: 2m 13s\tremaining: 57m 57s\n",
      "37:\ttest: 0.7826414\tbest: 0.7826414 (37)\ttotal: 2m 16s\tremaining: 57m 45s\n",
      "38:\ttest: 0.7826590\tbest: 0.7826590 (38)\ttotal: 2m 20s\tremaining: 57m 33s\n",
      "39:\ttest: 0.7826511\tbest: 0.7826590 (38)\ttotal: 2m 23s\tremaining: 57m 25s\n",
      "40:\ttest: 0.7826488\tbest: 0.7826590 (38)\ttotal: 2m 26s\tremaining: 57m 15s\n",
      "41:\ttest: 0.7825666\tbest: 0.7826590 (38)\ttotal: 2m 31s\tremaining: 57m 26s\n",
      "42:\ttest: 0.7825827\tbest: 0.7826590 (38)\ttotal: 2m 34s\tremaining: 57m 17s\n",
      "43:\ttest: 0.7825767\tbest: 0.7826590 (38)\ttotal: 2m 37s\tremaining: 57m 7s\n",
      "44:\ttest: 0.7823806\tbest: 0.7826590 (38)\ttotal: 2m 41s\tremaining: 56m 58s\n",
      "45:\ttest: 0.7823715\tbest: 0.7826590 (38)\ttotal: 2m 44s\tremaining: 56m 50s\n",
      "46:\ttest: 0.7823887\tbest: 0.7826590 (38)\ttotal: 2m 47s\tremaining: 56m 41s\n",
      "47:\ttest: 0.7823688\tbest: 0.7826590 (38)\ttotal: 2m 51s\tremaining: 56m 32s\n",
      "48:\ttest: 0.7823496\tbest: 0.7826590 (38)\ttotal: 2m 54s\tremaining: 56m 23s\n",
      "49:\ttest: 0.7823128\tbest: 0.7826590 (38)\ttotal: 2m 57s\tremaining: 56m 15s\n",
      "50:\ttest: 0.7823189\tbest: 0.7826590 (38)\ttotal: 3m\tremaining: 56m 6s\n",
      "51:\ttest: 0.7823808\tbest: 0.7826590 (38)\ttotal: 3m 4s\tremaining: 55m 57s\n",
      "52:\ttest: 0.7823446\tbest: 0.7826590 (38)\ttotal: 3m 7s\tremaining: 55m 47s\n",
      "53:\ttest: 0.7823438\tbest: 0.7826590 (38)\ttotal: 3m 11s\tremaining: 55m 55s\n",
      "54:\ttest: 0.7823438\tbest: 0.7826590 (38)\ttotal: 3m 14s\tremaining: 55m 45s\n",
      "55:\ttest: 0.7823446\tbest: 0.7826590 (38)\ttotal: 3m 17s\tremaining: 55m 37s\n",
      "56:\ttest: 0.7823643\tbest: 0.7826590 (38)\ttotal: 3m 21s\tremaining: 55m 27s\n",
      "57:\ttest: 0.7823656\tbest: 0.7826590 (38)\ttotal: 3m 24s\tremaining: 55m 18s\n",
      "58:\ttest: 0.7823657\tbest: 0.7826590 (38)\ttotal: 3m 27s\tremaining: 55m 11s\n",
      "59:\ttest: 0.7823933\tbest: 0.7826590 (38)\ttotal: 3m 30s\tremaining: 55m 5s\n",
      "60:\ttest: 0.7823933\tbest: 0.7826590 (38)\ttotal: 3m 34s\tremaining: 54m 55s\n",
      "61:\ttest: 0.7823933\tbest: 0.7826590 (38)\ttotal: 3m 37s\tremaining: 54m 46s\n",
      "62:\ttest: 0.7823994\tbest: 0.7826590 (38)\ttotal: 3m 41s\tremaining: 54m 55s\n",
      "63:\ttest: 0.7823994\tbest: 0.7826590 (38)\ttotal: 3m 44s\tremaining: 54m 46s\n",
      "64:\ttest: 0.7824081\tbest: 0.7826590 (38)\ttotal: 3m 47s\tremaining: 54m 37s\n",
      "65:\ttest: 0.7825094\tbest: 0.7826590 (38)\ttotal: 3m 52s\tremaining: 54m 44s\n",
      "66:\ttest: 0.7825360\tbest: 0.7826590 (38)\ttotal: 3m 55s\tremaining: 54m 35s\n",
      "67:\ttest: 0.7825851\tbest: 0.7826590 (38)\ttotal: 3m 58s\tremaining: 54m 26s\n",
      "68:\ttest: 0.7826132\tbest: 0.7826590 (38)\ttotal: 4m 2s\tremaining: 54m 32s\n",
      "69:\ttest: 0.7827076\tbest: 0.7827076 (69)\ttotal: 4m 5s\tremaining: 54m 25s\n",
      "70:\ttest: 0.7826964\tbest: 0.7827076 (69)\ttotal: 4m 8s\tremaining: 54m 17s\n",
      "71:\ttest: 0.7826870\tbest: 0.7827076 (69)\ttotal: 4m 12s\tremaining: 54m 17s\n",
      "72:\ttest: 0.7827072\tbest: 0.7827076 (69)\ttotal: 4m 15s\tremaining: 54m 9s\n",
      "73:\ttest: 0.7826728\tbest: 0.7827076 (69)\ttotal: 4m 18s\tremaining: 53m 59s\n",
      "74:\ttest: 0.7826974\tbest: 0.7827076 (69)\ttotal: 4m 22s\tremaining: 53m 51s\n",
      "75:\ttest: 0.7827105\tbest: 0.7827105 (75)\ttotal: 4m 25s\tremaining: 53m 51s\n",
      "76:\ttest: 0.7827363\tbest: 0.7827363 (76)\ttotal: 4m 28s\tremaining: 53m 43s\n",
      "77:\ttest: 0.7827269\tbest: 0.7827363 (76)\ttotal: 4m 32s\tremaining: 53m 46s\n",
      "78:\ttest: 0.7826481\tbest: 0.7827363 (76)\ttotal: 4m 36s\tremaining: 53m 40s\n",
      "79:\ttest: 0.7826365\tbest: 0.7827363 (76)\ttotal: 4m 39s\tremaining: 53m 34s\n",
      "80:\ttest: 0.7826096\tbest: 0.7827363 (76)\ttotal: 4m 42s\tremaining: 53m 27s\n",
      "81:\ttest: 0.7826312\tbest: 0.7827363 (76)\ttotal: 4m 45s\tremaining: 53m 20s\n",
      "82:\ttest: 0.7827053\tbest: 0.7827363 (76)\ttotal: 4m 49s\tremaining: 53m 14s\n",
      "83:\ttest: 0.7827009\tbest: 0.7827363 (76)\ttotal: 4m 52s\tremaining: 53m 6s\n",
      "84:\ttest: 0.7826987\tbest: 0.7827363 (76)\ttotal: 4m 55s\tremaining: 52m 59s\n",
      "85:\ttest: 0.7826990\tbest: 0.7827363 (76)\ttotal: 4m 58s\tremaining: 52m 53s\n",
      "86:\ttest: 0.7827182\tbest: 0.7827363 (76)\ttotal: 5m 1s\tremaining: 52m 47s\n",
      "87:\ttest: 0.7827181\tbest: 0.7827363 (76)\ttotal: 5m 4s\tremaining: 52m 40s\n",
      "88:\ttest: 0.7827461\tbest: 0.7827461 (88)\ttotal: 5m 8s\tremaining: 52m 32s\n",
      "89:\ttest: 0.7827453\tbest: 0.7827461 (88)\ttotal: 5m 11s\tremaining: 52m 27s\n",
      "90:\ttest: 0.7827324\tbest: 0.7827461 (88)\ttotal: 5m 14s\tremaining: 52m 19s\n",
      "91:\ttest: 0.7827455\tbest: 0.7827461 (88)\ttotal: 5m 17s\tremaining: 52m 14s\n",
      "92:\ttest: 0.7827167\tbest: 0.7827461 (88)\ttotal: 5m 20s\tremaining: 52m 8s\n",
      "93:\ttest: 0.7828773\tbest: 0.7828773 (93)\ttotal: 5m 23s\tremaining: 52m 1s\n",
      "94:\ttest: 0.7829434\tbest: 0.7829434 (94)\ttotal: 5m 27s\tremaining: 51m 55s\n",
      "95:\ttest: 0.7830301\tbest: 0.7830301 (95)\ttotal: 5m 30s\tremaining: 51m 47s\n",
      "96:\ttest: 0.7829966\tbest: 0.7830301 (95)\ttotal: 5m 33s\tremaining: 51m 40s\n",
      "97:\ttest: 0.7829966\tbest: 0.7830301 (95)\ttotal: 5m 36s\tremaining: 51m 33s\n",
      "98:\ttest: 0.7830186\tbest: 0.7830301 (95)\ttotal: 5m 39s\tremaining: 51m 26s\n",
      "99:\ttest: 0.7830111\tbest: 0.7830301 (95)\ttotal: 5m 42s\tremaining: 51m 20s\n",
      "100:\ttest: 0.7830000\tbest: 0.7830301 (95)\ttotal: 5m 45s\tremaining: 51m 14s\n",
      "101:\ttest: 0.7829988\tbest: 0.7830301 (95)\ttotal: 5m 48s\tremaining: 51m 7s\n",
      "102:\ttest: 0.7829988\tbest: 0.7830301 (95)\ttotal: 5m 51s\tremaining: 51m 1s\n",
      "103:\ttest: 0.7830110\tbest: 0.7830301 (95)\ttotal: 5m 54s\tremaining: 50m 54s\n",
      "104:\ttest: 0.7830077\tbest: 0.7830301 (95)\ttotal: 5m 57s\tremaining: 50m 48s\n",
      "105:\ttest: 0.7830393\tbest: 0.7830393 (105)\ttotal: 6m\tremaining: 50m 42s\n",
      "106:\ttest: 0.7830409\tbest: 0.7830409 (106)\ttotal: 6m 4s\tremaining: 50m 40s\n",
      "107:\ttest: 0.7830514\tbest: 0.7830514 (107)\ttotal: 6m 8s\tremaining: 50m 39s\n",
      "108:\ttest: 0.7830980\tbest: 0.7830980 (108)\ttotal: 6m 11s\tremaining: 50m 33s\n",
      "109:\ttest: 0.7830782\tbest: 0.7830980 (108)\ttotal: 6m 14s\tremaining: 50m 26s\n",
      "110:\ttest: 0.7830887\tbest: 0.7830980 (108)\ttotal: 6m 17s\tremaining: 50m 24s\n",
      "111:\ttest: 0.7830415\tbest: 0.7830980 (108)\ttotal: 6m 20s\tremaining: 50m 18s\n",
      "112:\ttest: 0.7830948\tbest: 0.7830980 (108)\ttotal: 6m 23s\tremaining: 50m 12s\n",
      "113:\ttest: 0.7831506\tbest: 0.7831506 (113)\ttotal: 6m 26s\tremaining: 50m 6s\n",
      "114:\ttest: 0.7831525\tbest: 0.7831525 (114)\ttotal: 6m 29s\tremaining: 50m\n",
      "115:\ttest: 0.7831531\tbest: 0.7831531 (115)\ttotal: 6m 33s\tremaining: 49m 55s\n",
      "116:\ttest: 0.7831610\tbest: 0.7831610 (116)\ttotal: 6m 36s\tremaining: 49m 50s\n",
      "117:\ttest: 0.7831709\tbest: 0.7831709 (117)\ttotal: 6m 39s\tremaining: 49m 44s\n",
      "118:\ttest: 0.7831777\tbest: 0.7831777 (118)\ttotal: 6m 42s\tremaining: 49m 38s\n",
      "119:\ttest: 0.7831735\tbest: 0.7831777 (118)\ttotal: 6m 45s\tremaining: 49m 36s\n",
      "120:\ttest: 0.7833201\tbest: 0.7833201 (120)\ttotal: 6m 48s\tremaining: 49m 29s\n",
      "121:\ttest: 0.7832540\tbest: 0.7833201 (120)\ttotal: 6m 51s\tremaining: 49m 24s\n",
      "122:\ttest: 0.7833260\tbest: 0.7833260 (122)\ttotal: 6m 55s\tremaining: 49m 19s\n",
      "123:\ttest: 0.7833241\tbest: 0.7833260 (122)\ttotal: 6m 58s\tremaining: 49m 14s\n",
      "124:\ttest: 0.7832874\tbest: 0.7833260 (122)\ttotal: 7m 1s\tremaining: 49m 12s\n",
      "125:\ttest: 0.7832881\tbest: 0.7833260 (122)\ttotal: 7m 6s\tremaining: 49m 18s\n",
      "126:\ttest: 0.7951483\tbest: 0.7951483 (126)\ttotal: 7m 11s\tremaining: 49m 25s\n",
      "127:\ttest: 0.8053362\tbest: 0.8053362 (127)\ttotal: 7m 14s\tremaining: 49m 20s\n",
      "128:\ttest: 0.8060745\tbest: 0.8060745 (128)\ttotal: 7m 17s\tremaining: 49m 13s\n",
      "129:\ttest: 0.8067180\tbest: 0.8067180 (129)\ttotal: 7m 20s\tremaining: 49m 8s\n",
      "130:\ttest: 0.8069621\tbest: 0.8069621 (130)\ttotal: 7m 23s\tremaining: 49m 3s\n",
      "131:\ttest: 0.8070977\tbest: 0.8070977 (131)\ttotal: 7m 26s\tremaining: 48m 58s\n",
      "132:\ttest: 0.8073467\tbest: 0.8073467 (132)\ttotal: 7m 29s\tremaining: 48m 52s\n",
      "133:\ttest: 0.8074564\tbest: 0.8074564 (133)\ttotal: 7m 33s\tremaining: 48m 47s\n",
      "134:\ttest: 0.8075805\tbest: 0.8075805 (134)\ttotal: 7m 36s\tremaining: 48m 43s\n",
      "135:\ttest: 0.8077673\tbest: 0.8077673 (135)\ttotal: 7m 39s\tremaining: 48m 38s\n",
      "136:\ttest: 0.8078937\tbest: 0.8078937 (136)\ttotal: 7m 42s\tremaining: 48m 33s\n",
      "137:\ttest: 0.8081131\tbest: 0.8081131 (137)\ttotal: 7m 45s\tremaining: 48m 27s\n",
      "138:\ttest: 0.8083270\tbest: 0.8083270 (138)\ttotal: 7m 48s\tremaining: 48m 21s\n",
      "139:\ttest: 0.8084985\tbest: 0.8084985 (139)\ttotal: 7m 51s\tremaining: 48m 15s\n",
      "140:\ttest: 0.8087326\tbest: 0.8087326 (140)\ttotal: 7m 54s\tremaining: 48m 9s\n",
      "141:\ttest: 0.8090104\tbest: 0.8090104 (141)\ttotal: 7m 57s\tremaining: 48m 3s\n",
      "142:\ttest: 0.8094966\tbest: 0.8094966 (142)\ttotal: 8m\tremaining: 47m 57s\n",
      "143:\ttest: 0.8098249\tbest: 0.8098249 (143)\ttotal: 8m 3s\tremaining: 47m 52s\n",
      "144:\ttest: 0.8100758\tbest: 0.8100758 (144)\ttotal: 8m 6s\tremaining: 47m 48s\n",
      "145:\ttest: 0.8102108\tbest: 0.8102108 (145)\ttotal: 8m 9s\tremaining: 47m 43s\n",
      "146:\ttest: 0.8105237\tbest: 0.8105237 (146)\ttotal: 8m 12s\tremaining: 47m 39s\n",
      "147:\ttest: 0.8108824\tbest: 0.8108824 (147)\ttotal: 8m 15s\tremaining: 47m 34s\n",
      "148:\ttest: 0.8114435\tbest: 0.8114435 (148)\ttotal: 8m 19s\tremaining: 47m 30s\n",
      "149:\ttest: 0.8116759\tbest: 0.8116759 (149)\ttotal: 8m 21s\tremaining: 47m 24s\n",
      "150:\ttest: 0.8120468\tbest: 0.8120468 (150)\ttotal: 8m 25s\tremaining: 47m 20s\n",
      "151:\ttest: 0.8123611\tbest: 0.8123611 (151)\ttotal: 8m 28s\tremaining: 47m 16s\n",
      "152:\ttest: 0.8127372\tbest: 0.8127372 (152)\ttotal: 8m 31s\tremaining: 47m 12s\n",
      "153:\ttest: 0.8133548\tbest: 0.8133548 (153)\ttotal: 8m 34s\tremaining: 47m 8s\n",
      "154:\ttest: 0.8137173\tbest: 0.8137173 (154)\ttotal: 8m 37s\tremaining: 47m 2s\n",
      "155:\ttest: 0.8139056\tbest: 0.8139056 (155)\ttotal: 8m 40s\tremaining: 46m 58s\n",
      "156:\ttest: 0.8140177\tbest: 0.8140177 (156)\ttotal: 8m 44s\tremaining: 46m 53s\n",
      "157:\ttest: 0.8140724\tbest: 0.8140724 (157)\ttotal: 8m 46s\tremaining: 46m 47s\n",
      "158:\ttest: 0.8142113\tbest: 0.8142113 (158)\ttotal: 8m 50s\tremaining: 46m 43s\n",
      "159:\ttest: 0.8142724\tbest: 0.8142724 (159)\ttotal: 8m 52s\tremaining: 46m 36s\n",
      "160:\ttest: 0.8143983\tbest: 0.8143983 (160)\ttotal: 8m 55s\tremaining: 46m 32s\n",
      "161:\ttest: 0.8154798\tbest: 0.8154798 (161)\ttotal: 8m 58s\tremaining: 46m 28s\n",
      "162:\ttest: 0.8156186\tbest: 0.8156186 (162)\ttotal: 9m 2s\tremaining: 46m 23s\n",
      "163:\ttest: 0.8157127\tbest: 0.8157127 (163)\ttotal: 9m 5s\tremaining: 46m 18s\n",
      "164:\ttest: 0.8159110\tbest: 0.8159110 (164)\ttotal: 9m 7s\tremaining: 46m 12s\n",
      "165:\ttest: 0.8159340\tbest: 0.8159340 (165)\ttotal: 9m 11s\tremaining: 46m 8s\n",
      "166:\ttest: 0.8160455\tbest: 0.8160455 (166)\ttotal: 9m 14s\tremaining: 46m 4s\n",
      "167:\ttest: 0.8161053\tbest: 0.8161053 (167)\ttotal: 9m 17s\tremaining: 45m 59s\n",
      "168:\ttest: 0.8162427\tbest: 0.8162427 (168)\ttotal: 9m 20s\tremaining: 45m 55s\n",
      "169:\ttest: 0.8163462\tbest: 0.8163462 (169)\ttotal: 9m 23s\tremaining: 45m 51s\n",
      "170:\ttest: 0.8164525\tbest: 0.8164525 (170)\ttotal: 9m 26s\tremaining: 45m 44s\n",
      "171:\ttest: 0.8165453\tbest: 0.8165453 (171)\ttotal: 9m 29s\tremaining: 45m 41s\n",
      "172:\ttest: 0.8166499\tbest: 0.8166499 (172)\ttotal: 9m 32s\tremaining: 45m 36s\n",
      "173:\ttest: 0.8167769\tbest: 0.8167769 (173)\ttotal: 9m 35s\tremaining: 45m 32s\n",
      "174:\ttest: 0.8168449\tbest: 0.8168449 (174)\ttotal: 9m 38s\tremaining: 45m 26s\n",
      "175:\ttest: 0.8169987\tbest: 0.8169987 (175)\ttotal: 9m 41s\tremaining: 45m 21s\n",
      "176:\ttest: 0.8171020\tbest: 0.8171020 (176)\ttotal: 9m 44s\tremaining: 45m 18s\n",
      "177:\ttest: 0.8172481\tbest: 0.8172481 (177)\ttotal: 9m 47s\tremaining: 45m 14s\n",
      "178:\ttest: 0.8174576\tbest: 0.8174576 (178)\ttotal: 9m 50s\tremaining: 45m 10s\n",
      "179:\ttest: 0.8175576\tbest: 0.8175576 (179)\ttotal: 9m 54s\tremaining: 45m 6s\n",
      "180:\ttest: 0.8176320\tbest: 0.8176320 (180)\ttotal: 9m 56s\tremaining: 44m 59s\n",
      "181:\ttest: 0.8177407\tbest: 0.8177407 (181)\ttotal: 9m 59s\tremaining: 44m 55s\n",
      "182:\ttest: 0.8178204\tbest: 0.8178204 (182)\ttotal: 10m 2s\tremaining: 44m 51s\n",
      "183:\ttest: 0.8178397\tbest: 0.8178397 (183)\ttotal: 10m 6s\tremaining: 44m 47s\n",
      "184:\ttest: 0.8179257\tbest: 0.8179257 (184)\ttotal: 10m 9s\tremaining: 44m 44s\n",
      "185:\ttest: 0.8180383\tbest: 0.8180383 (185)\ttotal: 10m 12s\tremaining: 44m 38s\n",
      "186:\ttest: 0.8180437\tbest: 0.8180437 (186)\ttotal: 10m 15s\tremaining: 44m 34s\n",
      "187:\ttest: 0.8181292\tbest: 0.8181292 (187)\ttotal: 10m 18s\tremaining: 44m 30s\n",
      "188:\ttest: 0.8181941\tbest: 0.8181941 (188)\ttotal: 10m 21s\tremaining: 44m 28s\n",
      "189:\ttest: 0.8182748\tbest: 0.8182748 (189)\ttotal: 10m 24s\tremaining: 44m 22s\n",
      "190:\ttest: 0.8182869\tbest: 0.8182869 (190)\ttotal: 10m 27s\tremaining: 44m 17s\n",
      "191:\ttest: 0.8183445\tbest: 0.8183445 (191)\ttotal: 10m 30s\tremaining: 44m 13s\n",
      "192:\ttest: 0.8184491\tbest: 0.8184491 (192)\ttotal: 10m 33s\tremaining: 44m 8s\n",
      "193:\ttest: 0.8184630\tbest: 0.8184630 (193)\ttotal: 10m 36s\tremaining: 44m 4s\n",
      "194:\ttest: 0.8184852\tbest: 0.8184852 (194)\ttotal: 10m 39s\tremaining: 44m\n",
      "195:\ttest: 0.8185108\tbest: 0.8185108 (195)\ttotal: 10m 42s\tremaining: 43m 55s\n",
      "196:\ttest: 0.8186484\tbest: 0.8186484 (196)\ttotal: 10m 45s\tremaining: 43m 51s\n",
      "197:\ttest: 0.8187010\tbest: 0.8187010 (197)\ttotal: 10m 48s\tremaining: 43m 47s\n",
      "198:\ttest: 0.8187377\tbest: 0.8187377 (198)\ttotal: 10m 51s\tremaining: 43m 42s\n",
      "199:\ttest: 0.8188906\tbest: 0.8188906 (199)\ttotal: 10m 54s\tremaining: 43m 38s\n",
      "200:\ttest: 0.8189023\tbest: 0.8189023 (200)\ttotal: 10m 57s\tremaining: 43m 34s\n",
      "201:\ttest: 0.8189596\tbest: 0.8189596 (201)\ttotal: 11m\tremaining: 43m 30s\n",
      "202:\ttest: 0.8190209\tbest: 0.8190209 (202)\ttotal: 11m 3s\tremaining: 43m 25s\n",
      "203:\ttest: 0.8191111\tbest: 0.8191111 (203)\ttotal: 11m 6s\tremaining: 43m 21s\n",
      "204:\ttest: 0.8191349\tbest: 0.8191349 (204)\ttotal: 11m 9s\tremaining: 43m 17s\n",
      "205:\ttest: 0.8191752\tbest: 0.8191752 (205)\ttotal: 11m 12s\tremaining: 43m 10s\n",
      "206:\ttest: 0.8191691\tbest: 0.8191752 (205)\ttotal: 11m 15s\tremaining: 43m 6s\n",
      "207:\ttest: 0.8192374\tbest: 0.8192374 (207)\ttotal: 11m 18s\tremaining: 43m 2s\n",
      "208:\ttest: 0.8192887\tbest: 0.8192887 (208)\ttotal: 11m 21s\tremaining: 42m 58s\n",
      "209:\ttest: 0.8193242\tbest: 0.8193242 (209)\ttotal: 11m 24s\tremaining: 42m 54s\n",
      "210:\ttest: 0.8193985\tbest: 0.8193985 (210)\ttotal: 11m 27s\tremaining: 42m 50s\n",
      "211:\ttest: 0.8194247\tbest: 0.8194247 (211)\ttotal: 11m 30s\tremaining: 42m 45s\n",
      "212:\ttest: 0.8195004\tbest: 0.8195004 (212)\ttotal: 11m 33s\tremaining: 42m 42s\n",
      "213:\ttest: 0.8195560\tbest: 0.8195560 (213)\ttotal: 11m 36s\tremaining: 42m 36s\n",
      "214:\ttest: 0.8195703\tbest: 0.8195703 (214)\ttotal: 11m 38s\tremaining: 42m 31s\n",
      "215:\ttest: 0.8195939\tbest: 0.8195939 (215)\ttotal: 11m 41s\tremaining: 42m 26s\n",
      "216:\ttest: 0.8196091\tbest: 0.8196091 (216)\ttotal: 11m 44s\tremaining: 42m 22s\n",
      "217:\ttest: 0.8195949\tbest: 0.8196091 (216)\ttotal: 11m 47s\tremaining: 42m 18s\n",
      "218:\ttest: 0.8195859\tbest: 0.8196091 (216)\ttotal: 11m 50s\tremaining: 42m 14s\n",
      "219:\ttest: 0.8196122\tbest: 0.8196122 (219)\ttotal: 11m 53s\tremaining: 42m 10s\n",
      "220:\ttest: 0.8196201\tbest: 0.8196201 (220)\ttotal: 11m 56s\tremaining: 42m 6s\n",
      "221:\ttest: 0.8196461\tbest: 0.8196461 (221)\ttotal: 11m 59s\tremaining: 42m\n",
      "222:\ttest: 0.8196497\tbest: 0.8196497 (222)\ttotal: 12m 2s\tremaining: 41m 56s\n",
      "223:\ttest: 0.8196398\tbest: 0.8196497 (222)\ttotal: 12m 5s\tremaining: 41m 52s\n",
      "224:\ttest: 0.8196947\tbest: 0.8196947 (224)\ttotal: 12m 7s\tremaining: 41m 46s\n",
      "225:\ttest: 0.8196903\tbest: 0.8196947 (224)\ttotal: 12m 10s\tremaining: 41m 41s\n",
      "226:\ttest: 0.8197330\tbest: 0.8197330 (226)\ttotal: 12m 13s\tremaining: 41m 37s\n",
      "227:\ttest: 0.8197310\tbest: 0.8197330 (226)\ttotal: 12m 16s\tremaining: 41m 33s\n",
      "228:\ttest: 0.8197031\tbest: 0.8197330 (226)\ttotal: 12m 19s\tremaining: 41m 29s\n",
      "229:\ttest: 0.8197258\tbest: 0.8197330 (226)\ttotal: 12m 22s\tremaining: 41m 25s\n",
      "230:\ttest: 0.8197255\tbest: 0.8197330 (226)\ttotal: 12m 25s\tremaining: 41m 21s\n",
      "231:\ttest: 0.8197503\tbest: 0.8197503 (231)\ttotal: 12m 28s\tremaining: 41m 16s\n",
      "232:\ttest: 0.8197462\tbest: 0.8197503 (231)\ttotal: 12m 30s\tremaining: 41m 12s\n",
      "233:\ttest: 0.8197899\tbest: 0.8197899 (233)\ttotal: 12m 33s\tremaining: 41m 6s\n",
      "234:\ttest: 0.8197926\tbest: 0.8197926 (234)\ttotal: 12m 36s\tremaining: 41m 2s\n",
      "235:\ttest: 0.8198308\tbest: 0.8198308 (235)\ttotal: 12m 39s\tremaining: 40m 57s\n",
      "236:\ttest: 0.8198741\tbest: 0.8198741 (236)\ttotal: 12m 42s\tremaining: 40m 53s\n",
      "237:\ttest: 0.8199164\tbest: 0.8199164 (237)\ttotal: 12m 44s\tremaining: 40m 48s\n",
      "238:\ttest: 0.8199510\tbest: 0.8199510 (238)\ttotal: 12m 47s\tremaining: 40m 45s\n",
      "239:\ttest: 0.8199720\tbest: 0.8199720 (239)\ttotal: 12m 50s\tremaining: 40m 41s\n",
      "240:\ttest: 0.8199847\tbest: 0.8199847 (240)\ttotal: 12m 53s\tremaining: 40m 36s\n",
      "241:\ttest: 0.8200007\tbest: 0.8200007 (241)\ttotal: 12m 56s\tremaining: 40m 32s\n",
      "242:\ttest: 0.8199932\tbest: 0.8200007 (241)\ttotal: 12m 59s\tremaining: 40m 27s\n",
      "243:\ttest: 0.8200788\tbest: 0.8200788 (243)\ttotal: 13m 2s\tremaining: 40m 24s\n",
      "244:\ttest: 0.8201487\tbest: 0.8201487 (244)\ttotal: 13m 5s\tremaining: 40m 20s\n",
      "245:\ttest: 0.8201623\tbest: 0.8201623 (245)\ttotal: 13m 8s\tremaining: 40m 15s\n",
      "246:\ttest: 0.8201224\tbest: 0.8201623 (245)\ttotal: 13m 10s\tremaining: 40m 11s\n",
      "247:\ttest: 0.8201393\tbest: 0.8201623 (245)\ttotal: 13m 13s\tremaining: 40m 6s\n",
      "248:\ttest: 0.8201597\tbest: 0.8201623 (245)\ttotal: 13m 16s\tremaining: 40m 2s\n",
      "249:\ttest: 0.8201729\tbest: 0.8201729 (249)\ttotal: 13m 19s\tremaining: 39m 57s\n",
      "250:\ttest: 0.8201559\tbest: 0.8201729 (249)\ttotal: 13m 21s\tremaining: 39m 52s\n",
      "251:\ttest: 0.8201815\tbest: 0.8201815 (251)\ttotal: 13m 24s\tremaining: 39m 48s\n",
      "252:\ttest: 0.8201621\tbest: 0.8201815 (251)\ttotal: 13m 27s\tremaining: 39m 44s\n",
      "253:\ttest: 0.8201841\tbest: 0.8201841 (253)\ttotal: 13m 30s\tremaining: 39m 39s\n",
      "254:\ttest: 0.8202119\tbest: 0.8202119 (254)\ttotal: 13m 32s\tremaining: 39m 34s\n",
      "255:\ttest: 0.8201999\tbest: 0.8202119 (254)\ttotal: 13m 35s\tremaining: 39m 30s\n",
      "256:\ttest: 0.8201959\tbest: 0.8202119 (254)\ttotal: 13m 38s\tremaining: 39m 26s\n",
      "257:\ttest: 0.8202079\tbest: 0.8202119 (254)\ttotal: 13m 41s\tremaining: 39m 22s\n",
      "258:\ttest: 0.8202347\tbest: 0.8202347 (258)\ttotal: 13m 44s\tremaining: 39m 18s\n",
      "259:\ttest: 0.8203119\tbest: 0.8203119 (259)\ttotal: 13m 47s\tremaining: 39m 14s\n",
      "260:\ttest: 0.8203297\tbest: 0.8203297 (260)\ttotal: 13m 50s\tremaining: 39m 10s\n",
      "261:\ttest: 0.8203411\tbest: 0.8203411 (261)\ttotal: 13m 52s\tremaining: 39m 5s\n",
      "262:\ttest: 0.8203390\tbest: 0.8203411 (261)\ttotal: 13m 55s\tremaining: 39m 2s\n",
      "263:\ttest: 0.8203056\tbest: 0.8203411 (261)\ttotal: 13m 58s\tremaining: 38m 56s\n",
      "264:\ttest: 0.8203233\tbest: 0.8203411 (261)\ttotal: 14m\tremaining: 38m 51s\n",
      "265:\ttest: 0.8203390\tbest: 0.8203411 (261)\ttotal: 14m 3s\tremaining: 38m 46s\n",
      "266:\ttest: 0.8203592\tbest: 0.8203592 (266)\ttotal: 14m 6s\tremaining: 38m 42s\n",
      "267:\ttest: 0.8203994\tbest: 0.8203994 (267)\ttotal: 14m 8s\tremaining: 38m 37s\n",
      "268:\ttest: 0.8204338\tbest: 0.8204338 (268)\ttotal: 14m 11s\tremaining: 38m 33s\n",
      "269:\ttest: 0.8204486\tbest: 0.8204486 (269)\ttotal: 14m 13s\tremaining: 38m 28s\n",
      "270:\ttest: 0.8204523\tbest: 0.8204523 (270)\ttotal: 14m 16s\tremaining: 38m 23s\n",
      "271:\ttest: 0.8204410\tbest: 0.8204523 (270)\ttotal: 14m 19s\tremaining: 38m 19s\n",
      "272:\ttest: 0.8204588\tbest: 0.8204588 (272)\ttotal: 14m 21s\tremaining: 38m 14s\n",
      "273:\ttest: 0.8204761\tbest: 0.8204761 (273)\ttotal: 14m 24s\tremaining: 38m 9s\n",
      "274:\ttest: 0.8204659\tbest: 0.8204761 (273)\ttotal: 14m 26s\tremaining: 38m 5s\n",
      "275:\ttest: 0.8204984\tbest: 0.8204984 (275)\ttotal: 14m 29s\tremaining: 38m 1s\n",
      "276:\ttest: 0.8204923\tbest: 0.8204984 (275)\ttotal: 14m 32s\tremaining: 37m 56s\n",
      "277:\ttest: 0.8205047\tbest: 0.8205047 (277)\ttotal: 14m 35s\tremaining: 37m 53s\n",
      "278:\ttest: 0.8205147\tbest: 0.8205147 (278)\ttotal: 14m 38s\tremaining: 37m 49s\n",
      "279:\ttest: 0.8205163\tbest: 0.8205163 (279)\ttotal: 14m 40s\tremaining: 37m 44s\n",
      "280:\ttest: 0.8205663\tbest: 0.8205663 (280)\ttotal: 14m 43s\tremaining: 37m 40s\n",
      "281:\ttest: 0.8205664\tbest: 0.8205664 (281)\ttotal: 14m 46s\tremaining: 37m 36s\n",
      "282:\ttest: 0.8205861\tbest: 0.8205861 (282)\ttotal: 14m 48s\tremaining: 37m 31s\n",
      "283:\ttest: 0.8206247\tbest: 0.8206247 (283)\ttotal: 14m 51s\tremaining: 37m 28s\n",
      "284:\ttest: 0.8206446\tbest: 0.8206446 (284)\ttotal: 14m 54s\tremaining: 37m 23s\n",
      "285:\ttest: 0.8206522\tbest: 0.8206522 (285)\ttotal: 14m 56s\tremaining: 37m 18s\n",
      "286:\ttest: 0.8206435\tbest: 0.8206522 (285)\ttotal: 14m 59s\tremaining: 37m 14s\n",
      "287:\ttest: 0.8206620\tbest: 0.8206620 (287)\ttotal: 15m 2s\tremaining: 37m 11s\n",
      "288:\ttest: 0.8206419\tbest: 0.8206620 (287)\ttotal: 15m 5s\tremaining: 37m 7s\n",
      "289:\ttest: 0.8206744\tbest: 0.8206744 (289)\ttotal: 15m 7s\tremaining: 37m 2s\n",
      "290:\ttest: 0.8206980\tbest: 0.8206980 (290)\ttotal: 15m 10s\tremaining: 36m 57s\n",
      "291:\ttest: 0.8207292\tbest: 0.8207292 (291)\ttotal: 15m 12s\tremaining: 36m 52s\n",
      "292:\ttest: 0.8207181\tbest: 0.8207292 (291)\ttotal: 15m 15s\tremaining: 36m 49s\n",
      "293:\ttest: 0.8207442\tbest: 0.8207442 (293)\ttotal: 15m 18s\tremaining: 36m 44s\n",
      "294:\ttest: 0.8207285\tbest: 0.8207442 (293)\ttotal: 15m 21s\tremaining: 36m 42s\n",
      "295:\ttest: 0.8207455\tbest: 0.8207455 (295)\ttotal: 15m 23s\tremaining: 36m 37s\n",
      "296:\ttest: 0.8207281\tbest: 0.8207455 (295)\ttotal: 15m 26s\tremaining: 36m 33s\n",
      "297:\ttest: 0.8207399\tbest: 0.8207455 (295)\ttotal: 15m 28s\tremaining: 36m 28s\n",
      "298:\ttest: 0.8207594\tbest: 0.8207594 (298)\ttotal: 15m 31s\tremaining: 36m 23s\n",
      "299:\ttest: 0.8207692\tbest: 0.8207692 (299)\ttotal: 15m 34s\tremaining: 36m 19s\n",
      "300:\ttest: 0.8207734\tbest: 0.8207734 (300)\ttotal: 15m 37s\tremaining: 36m 16s\n",
      "301:\ttest: 0.8207990\tbest: 0.8207990 (301)\ttotal: 15m 39s\tremaining: 36m 12s\n",
      "302:\ttest: 0.8208109\tbest: 0.8208109 (302)\ttotal: 15m 42s\tremaining: 36m 8s\n",
      "303:\ttest: 0.8208039\tbest: 0.8208109 (302)\ttotal: 15m 45s\tremaining: 36m 4s\n",
      "304:\ttest: 0.8207946\tbest: 0.8208109 (302)\ttotal: 15m 47s\tremaining: 35m 59s\n",
      "305:\ttest: 0.8208066\tbest: 0.8208109 (302)\ttotal: 15m 50s\tremaining: 35m 55s\n",
      "306:\ttest: 0.8208387\tbest: 0.8208387 (306)\ttotal: 15m 52s\tremaining: 35m 50s\n",
      "307:\ttest: 0.8208316\tbest: 0.8208387 (306)\ttotal: 15m 55s\tremaining: 35m 47s\n",
      "308:\ttest: 0.8215405\tbest: 0.8215405 (308)\ttotal: 15m 59s\tremaining: 35m 45s\n",
      "309:\ttest: 0.8215373\tbest: 0.8215405 (308)\ttotal: 16m 2s\tremaining: 35m 41s\n",
      "310:\ttest: 0.8214555\tbest: 0.8215405 (308)\ttotal: 16m 4s\tremaining: 35m 37s\n",
      "311:\ttest: 0.8214712\tbest: 0.8215405 (308)\ttotal: 16m 7s\tremaining: 35m 33s\n",
      "312:\ttest: 0.8214719\tbest: 0.8215405 (308)\ttotal: 16m 10s\tremaining: 35m 29s\n",
      "313:\ttest: 0.8214779\tbest: 0.8215405 (308)\ttotal: 16m 13s\tremaining: 35m 25s\n",
      "314:\ttest: 0.8214782\tbest: 0.8215405 (308)\ttotal: 16m 15s\tremaining: 35m 20s\n",
      "315:\ttest: 0.8214805\tbest: 0.8215405 (308)\ttotal: 16m 17s\tremaining: 35m 16s\n",
      "316:\ttest: 0.8214914\tbest: 0.8215405 (308)\ttotal: 16m 20s\tremaining: 35m 13s\n",
      "317:\ttest: 0.8214888\tbest: 0.8215405 (308)\ttotal: 16m 23s\tremaining: 35m 9s\n",
      "318:\ttest: 0.8214864\tbest: 0.8215405 (308)\ttotal: 16m 26s\tremaining: 35m 5s\n",
      "319:\ttest: 0.8214899\tbest: 0.8215405 (308)\ttotal: 16m 28s\tremaining: 35m 1s\n",
      "320:\ttest: 0.8215646\tbest: 0.8215646 (320)\ttotal: 16m 31s\tremaining: 34m 57s\n",
      "321:\ttest: 0.8215667\tbest: 0.8215667 (321)\ttotal: 16m 34s\tremaining: 34m 53s\n",
      "322:\ttest: 0.8215673\tbest: 0.8215673 (322)\ttotal: 16m 37s\tremaining: 34m 50s\n",
      "323:\ttest: 0.8215817\tbest: 0.8215817 (323)\ttotal: 16m 40s\tremaining: 34m 46s\n",
      "324:\ttest: 0.8215473\tbest: 0.8215817 (323)\ttotal: 16m 42s\tremaining: 34m 42s\n",
      "325:\ttest: 0.8215588\tbest: 0.8215817 (323)\ttotal: 16m 45s\tremaining: 34m 39s\n",
      "326:\ttest: 0.8215521\tbest: 0.8215817 (323)\ttotal: 16m 48s\tremaining: 34m 35s\n",
      "327:\ttest: 0.8215549\tbest: 0.8215817 (323)\ttotal: 16m 51s\tremaining: 34m 31s\n",
      "328:\ttest: 0.8215307\tbest: 0.8215817 (323)\ttotal: 16m 54s\tremaining: 34m 28s\n",
      "329:\ttest: 0.8215518\tbest: 0.8215817 (323)\ttotal: 16m 56s\tremaining: 34m 24s\n",
      "330:\ttest: 0.8215571\tbest: 0.8215817 (323)\ttotal: 16m 59s\tremaining: 34m 20s\n",
      "331:\ttest: 0.8215558\tbest: 0.8215817 (323)\ttotal: 17m 2s\tremaining: 34m 17s\n",
      "332:\ttest: 0.8216039\tbest: 0.8216039 (332)\ttotal: 17m 5s\tremaining: 34m 14s\n",
      "333:\ttest: 0.8216116\tbest: 0.8216116 (333)\ttotal: 17m 8s\tremaining: 34m 10s\n",
      "334:\ttest: 0.8216228\tbest: 0.8216228 (334)\ttotal: 17m 11s\tremaining: 34m 7s\n",
      "335:\ttest: 0.8220531\tbest: 0.8220531 (335)\ttotal: 17m 14s\tremaining: 34m 5s\n",
      "336:\ttest: 0.8220340\tbest: 0.8220531 (335)\ttotal: 17m 17s\tremaining: 34m\n",
      "337:\ttest: 0.8220323\tbest: 0.8220531 (335)\ttotal: 17m 19s\tremaining: 33m 56s\n",
      "338:\ttest: 0.8220441\tbest: 0.8220531 (335)\ttotal: 17m 22s\tremaining: 33m 52s\n",
      "339:\ttest: 0.8220362\tbest: 0.8220531 (335)\ttotal: 17m 24s\tremaining: 33m 48s\n",
      "340:\ttest: 0.8220577\tbest: 0.8220577 (340)\ttotal: 17m 27s\tremaining: 33m 44s\n",
      "341:\ttest: 0.8220546\tbest: 0.8220577 (340)\ttotal: 17m 29s\tremaining: 33m 40s\n",
      "342:\ttest: 0.8220603\tbest: 0.8220603 (342)\ttotal: 17m 32s\tremaining: 33m 36s\n",
      "343:\ttest: 0.8220559\tbest: 0.8220603 (342)\ttotal: 17m 35s\tremaining: 33m 31s\n",
      "344:\ttest: 0.8220286\tbest: 0.8220603 (342)\ttotal: 17m 37s\tremaining: 33m 27s\n",
      "345:\ttest: 0.8220604\tbest: 0.8220604 (345)\ttotal: 17m 40s\tremaining: 33m 24s\n",
      "346:\ttest: 0.8220527\tbest: 0.8220604 (345)\ttotal: 17m 43s\tremaining: 33m 20s\n",
      "347:\ttest: 0.8220605\tbest: 0.8220605 (347)\ttotal: 17m 45s\tremaining: 33m 16s\n",
      "348:\ttest: 0.8220684\tbest: 0.8220684 (348)\ttotal: 17m 48s\tremaining: 33m 13s\n",
      "349:\ttest: 0.8220744\tbest: 0.8220744 (349)\ttotal: 17m 51s\tremaining: 33m 9s\n",
      "350:\ttest: 0.8220567\tbest: 0.8220744 (349)\ttotal: 17m 53s\tremaining: 33m 5s\n",
      "351:\ttest: 0.8220690\tbest: 0.8220744 (349)\ttotal: 17m 56s\tremaining: 33m 1s\n",
      "352:\ttest: 0.8220669\tbest: 0.8220744 (349)\ttotal: 17m 59s\tremaining: 32m 58s\n",
      "353:\ttest: 0.8220837\tbest: 0.8220837 (353)\ttotal: 18m 1s\tremaining: 32m 54s\n",
      "354:\ttest: 0.8227408\tbest: 0.8227408 (354)\ttotal: 18m 4s\tremaining: 32m 50s\n",
      "355:\ttest: 0.8227402\tbest: 0.8227408 (354)\ttotal: 18m 7s\tremaining: 32m 46s\n",
      "356:\ttest: 0.8227368\tbest: 0.8227408 (354)\ttotal: 18m 9s\tremaining: 32m 42s\n",
      "357:\ttest: 0.8227583\tbest: 0.8227583 (357)\ttotal: 18m 12s\tremaining: 32m 38s\n",
      "358:\ttest: 0.8227716\tbest: 0.8227716 (358)\ttotal: 18m 14s\tremaining: 32m 35s\n",
      "359:\ttest: 0.8227812\tbest: 0.8227812 (359)\ttotal: 18m 17s\tremaining: 32m 31s\n",
      "360:\ttest: 0.8227935\tbest: 0.8227935 (360)\ttotal: 18m 20s\tremaining: 32m 27s\n",
      "361:\ttest: 0.8229874\tbest: 0.8229874 (361)\ttotal: 18m 23s\tremaining: 32m 24s\n",
      "362:\ttest: 0.8230102\tbest: 0.8230102 (362)\ttotal: 18m 25s\tremaining: 32m 20s\n",
      "363:\ttest: 0.8230008\tbest: 0.8230102 (362)\ttotal: 18m 28s\tremaining: 32m 16s\n",
      "364:\ttest: 0.8229885\tbest: 0.8230102 (362)\ttotal: 18m 30s\tremaining: 32m 12s\n",
      "365:\ttest: 0.8230199\tbest: 0.8230199 (365)\ttotal: 18m 33s\tremaining: 32m 9s\n",
      "366:\ttest: 0.8230161\tbest: 0.8230199 (365)\ttotal: 18m 36s\tremaining: 32m 5s\n",
      "367:\ttest: 0.8230177\tbest: 0.8230199 (365)\ttotal: 18m 38s\tremaining: 32m 1s\n",
      "368:\ttest: 0.8230314\tbest: 0.8230314 (368)\ttotal: 18m 42s\tremaining: 31m 59s\n",
      "369:\ttest: 0.8230220\tbest: 0.8230314 (368)\ttotal: 18m 45s\tremaining: 31m 56s\n",
      "370:\ttest: 0.8230183\tbest: 0.8230314 (368)\ttotal: 18m 47s\tremaining: 31m 52s\n",
      "371:\ttest: 0.8230049\tbest: 0.8230314 (368)\ttotal: 18m 51s\tremaining: 31m 49s\n",
      "372:\ttest: 0.8230046\tbest: 0.8230314 (368)\ttotal: 18m 53s\tremaining: 31m 46s\n",
      "373:\ttest: 0.8230068\tbest: 0.8230314 (368)\ttotal: 18m 56s\tremaining: 31m 41s\n",
      "374:\ttest: 0.8230016\tbest: 0.8230314 (368)\ttotal: 18m 59s\tremaining: 31m 38s\n",
      "375:\ttest: 0.8229703\tbest: 0.8230314 (368)\ttotal: 19m 2s\tremaining: 31m 35s\n",
      "376:\ttest: 0.8229956\tbest: 0.8230314 (368)\ttotal: 19m 5s\tremaining: 31m 32s\n",
      "377:\ttest: 0.8229850\tbest: 0.8230314 (368)\ttotal: 19m 8s\tremaining: 31m 29s\n",
      "378:\ttest: 0.8229838\tbest: 0.8230314 (368)\ttotal: 19m 10s\tremaining: 31m 25s\n",
      "379:\ttest: 0.8229689\tbest: 0.8230314 (368)\ttotal: 19m 13s\tremaining: 31m 21s\n",
      "380:\ttest: 0.8229469\tbest: 0.8230314 (368)\ttotal: 19m 16s\tremaining: 31m 18s\n",
      "381:\ttest: 0.8229687\tbest: 0.8230314 (368)\ttotal: 19m 18s\tremaining: 31m 14s\n",
      "382:\ttest: 0.8229462\tbest: 0.8230314 (368)\ttotal: 19m 22s\tremaining: 31m 12s\n",
      "383:\ttest: 0.8229426\tbest: 0.8230314 (368)\ttotal: 19m 24s\tremaining: 31m 8s\n",
      "384:\ttest: 0.8229287\tbest: 0.8230314 (368)\ttotal: 19m 27s\tremaining: 31m 4s\n",
      "385:\ttest: 0.8228868\tbest: 0.8230314 (368)\ttotal: 19m 30s\tremaining: 31m 1s\n",
      "386:\ttest: 0.8228622\tbest: 0.8230314 (368)\ttotal: 19m 32s\tremaining: 30m 57s\n",
      "387:\ttest: 0.8228741\tbest: 0.8230314 (368)\ttotal: 19m 35s\tremaining: 30m 54s\n",
      "388:\ttest: 0.8228993\tbest: 0.8230314 (368)\ttotal: 19m 38s\tremaining: 30m 51s\n",
      "389:\ttest: 0.8228973\tbest: 0.8230314 (368)\ttotal: 19m 41s\tremaining: 30m 48s\n",
      "390:\ttest: 0.8228642\tbest: 0.8230314 (368)\ttotal: 19m 44s\tremaining: 30m 44s\n",
      "391:\ttest: 0.8228654\tbest: 0.8230314 (368)\ttotal: 19m 46s\tremaining: 30m 40s\n",
      "392:\ttest: 0.8228658\tbest: 0.8230314 (368)\ttotal: 19m 49s\tremaining: 30m 37s\n",
      "393:\ttest: 0.8228816\tbest: 0.8230314 (368)\ttotal: 19m 52s\tremaining: 30m 33s\n",
      "394:\ttest: 0.8228677\tbest: 0.8230314 (368)\ttotal: 19m 54s\tremaining: 30m 30s\n",
      "395:\ttest: 0.8229042\tbest: 0.8230314 (368)\ttotal: 19m 57s\tremaining: 30m 26s\n",
      "396:\ttest: 0.8228999\tbest: 0.8230314 (368)\ttotal: 19m 59s\tremaining: 30m 22s\n",
      "397:\ttest: 0.8228875\tbest: 0.8230314 (368)\ttotal: 20m 2s\tremaining: 30m 19s\n",
      "398:\ttest: 0.8228987\tbest: 0.8230314 (368)\ttotal: 20m 5s\tremaining: 30m 15s\n",
      "399:\ttest: 0.8228912\tbest: 0.8230314 (368)\ttotal: 20m 7s\tremaining: 30m 11s\n",
      "400:\ttest: 0.8228975\tbest: 0.8230314 (368)\ttotal: 20m 10s\tremaining: 30m 8s\n",
      "401:\ttest: 0.8228796\tbest: 0.8230314 (368)\ttotal: 20m 12s\tremaining: 30m 4s\n",
      "402:\ttest: 0.8228636\tbest: 0.8230314 (368)\ttotal: 20m 15s\tremaining: 30m\n",
      "403:\ttest: 0.8228667\tbest: 0.8230314 (368)\ttotal: 20m 17s\tremaining: 29m 56s\n",
      "404:\ttest: 0.8228585\tbest: 0.8230314 (368)\ttotal: 20m 20s\tremaining: 29m 53s\n",
      "405:\ttest: 0.8228564\tbest: 0.8230314 (368)\ttotal: 20m 23s\tremaining: 29m 49s\n",
      "406:\ttest: 0.8228711\tbest: 0.8230314 (368)\ttotal: 20m 25s\tremaining: 29m 46s\n",
      "407:\ttest: 0.8228401\tbest: 0.8230314 (368)\ttotal: 20m 28s\tremaining: 29m 42s\n",
      "408:\ttest: 0.8228609\tbest: 0.8230314 (368)\ttotal: 20m 31s\tremaining: 29m 39s\n",
      "409:\ttest: 0.8228824\tbest: 0.8230314 (368)\ttotal: 20m 34s\tremaining: 29m 35s\n",
      "410:\ttest: 0.8228921\tbest: 0.8230314 (368)\ttotal: 20m 36s\tremaining: 29m 32s\n",
      "411:\ttest: 0.8228957\tbest: 0.8230314 (368)\ttotal: 20m 39s\tremaining: 29m 28s\n",
      "412:\ttest: 0.8228928\tbest: 0.8230314 (368)\ttotal: 20m 41s\tremaining: 29m 25s\n",
      "413:\ttest: 0.8229052\tbest: 0.8230314 (368)\ttotal: 20m 44s\tremaining: 29m 21s\n",
      "414:\ttest: 0.8228878\tbest: 0.8230314 (368)\ttotal: 20m 47s\tremaining: 29m 18s\n",
      "415:\ttest: 0.8228744\tbest: 0.8230314 (368)\ttotal: 20m 49s\tremaining: 29m 14s\n",
      "416:\ttest: 0.8228748\tbest: 0.8230314 (368)\ttotal: 20m 52s\tremaining: 29m 11s\n",
      "417:\ttest: 0.8229083\tbest: 0.8230314 (368)\ttotal: 20m 54s\tremaining: 29m 7s\n",
      "418:\ttest: 0.8228976\tbest: 0.8230314 (368)\ttotal: 20m 57s\tremaining: 29m 3s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.8230314322\n",
      "bestIteration = 368\n",
      "\n",
      "Shrink model to first 369 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 15:27:34 - INFO - \n",
      "Лучшие параметры:\n",
      "2025-04-21 15:27:34 - INFO - learning_rate: 0.004684283500572371\n",
      "2025-04-21 15:27:34 - INFO - depth: 6\n",
      "2025-04-21 15:27:34 - INFO - l2_leaf_reg: 5.36104374072008e-06\n",
      "2025-04-21 15:27:34 - INFO - random_strength: 0.007652648646042903\n",
      "2025-04-21 15:27:34 - INFO - bagging_temperature: 0.9487648394691982\n",
      "2025-04-21 15:27:34 - INFO - leaf_estimation_iterations: 9\n",
      "2025-04-21 15:27:34 - INFO - \n",
      "Метрики на лучших параметрах:\n",
      "2025-04-21 15:27:34 - INFO - ndcg_10: 0.8393\n",
      "2025-04-21 15:27:34 - INFO - precision_10: 0.1210\n",
      "2025-04-21 15:27:34 - INFO - recall_10: 0.3156\n",
      "2025-04-21 15:27:34 - INFO - diversity_10: 0.0052\n",
      "2025-04-21 15:27:34 - INFO - novelty_10: 0.3672\n",
      "2025-04-21 15:27:34 - INFO - serendipity_10: 0.0039\n",
      "2025-04-21 15:27:34 - INFO - ndcg_100: 0.8272\n",
      "2025-04-21 15:27:34 - INFO - precision_100: 0.0697\n",
      "2025-04-21 15:27:34 - INFO - recall_100: 0.3538\n",
      "2025-04-21 15:27:34 - INFO - diversity_100: 0.0225\n",
      "2025-04-21 15:27:34 - INFO - novelty_100: 0.6363\n",
      "2025-04-21 15:27:34 - INFO - serendipity_100: 0.0022\n",
      "2025-04-21 15:27:34 - INFO - ndcg_1000: 0.8318\n",
      "2025-04-21 15:27:34 - INFO - precision_1000: 0.0477\n",
      "2025-04-21 15:27:34 - INFO - recall_1000: 0.4952\n",
      "2025-04-21 15:27:34 - INFO - diversity_1000: 0.0777\n",
      "2025-04-21 15:27:34 - INFO - novelty_1000: 0.7544\n",
      "2025-04-21 15:27:34 - INFO - serendipity_1000: 0.0015\n",
      "2025-04-21 15:27:37 - INFO - Лучшие параметры: {'learning_rate': 0.004684283500572371, 'depth': 6, 'l2_leaf_reg': 5.36104374072008e-06, 'random_strength': 0.007652648646042903, 'bagging_temperature': 0.9487648394691982, 'leaf_estimation_iterations': 9}\n",
      "2025-04-21 15:27:37 - INFO - Результаты:\n",
      "2025-04-21 15:27:37 - INFO - Метрики для K=10:\n",
      "2025-04-21 15:27:37 - INFO - NDCG@10: 0.8393\n",
      "2025-04-21 15:27:37 - INFO - Precision@10: 0.1210\n",
      "2025-04-21 15:27:37 - INFO - Recall@10: 0.3156\n",
      "2025-04-21 15:27:37 - INFO - Diversity@10: 0.0052\n",
      "2025-04-21 15:27:37 - INFO - Novelty@10: 0.3672\n",
      "2025-04-21 15:27:37 - INFO - Serendipity@10: 0.0039\n",
      "2025-04-21 15:27:37 - INFO - --------------------------------\n",
      "2025-04-21 15:27:37 - INFO - Метрики для K=100:\n",
      "2025-04-21 15:27:37 - INFO - NDCG@100: 0.8272\n",
      "2025-04-21 15:27:37 - INFO - Precision@100: 0.0697\n",
      "2025-04-21 15:27:37 - INFO - Recall@100: 0.3538\n",
      "2025-04-21 15:27:37 - INFO - Diversity@100: 0.0225\n",
      "2025-04-21 15:27:37 - INFO - Novelty@100: 0.6363\n",
      "2025-04-21 15:27:37 - INFO - Serendipity@100: 0.0022\n",
      "2025-04-21 15:27:37 - INFO - --------------------------------\n",
      "2025-04-21 15:27:37 - INFO - Метрики для K=1000:\n",
      "2025-04-21 15:27:37 - INFO - NDCG@1000: 0.8318\n",
      "2025-04-21 15:27:37 - INFO - Precision@1000: 0.0477\n",
      "2025-04-21 15:27:37 - INFO - Recall@1000: 0.4952\n",
      "2025-04-21 15:27:37 - INFO - Diversity@1000: 0.0777\n",
      "2025-04-21 15:27:37 - INFO - Novelty@1000: 0.7544\n",
      "2025-04-21 15:27:37 - INFO - Serendipity@1000: 0.0015\n",
      "2025-04-21 15:27:37 - INFO - --------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    n_samples = 100\n",
    "    # buyer_ids = test_data[\"buyer_id\"].unique()[:n_samples]\n",
    "    buyer_ids = test_data[\"buyer_id\"].unique()\n",
    "\n",
    "    temp_candidates = candidates[candidates[\"buyer_id\"].isin(buyer_ids)].copy()\n",
    "\n",
    "    product_ids = temp_candidates[\"product_id\"].unique()\n",
    "    temp_train_data = train_data[\n",
    "        (train_data[\"buyer_id\"].isin(buyer_ids))\n",
    "        & (train_data[\"product_id\"].isin(product_ids))\n",
    "    ].copy()\n",
    "\n",
    "    temp_test_data = test_data[\n",
    "        (test_data[\"buyer_id\"].isin(buyer_ids))\n",
    "        & (test_data[\"product_id\"].isin(product_ids))\n",
    "    ].copy()\n",
    "\n",
    "    metrics_calculator = MetricsCalculator([10, 100, 1000])\n",
    "\n",
    "    best_params, best_metrics = optimize_catboost_parameters(\n",
    "        train_data=temp_train_data,\n",
    "        test_data=temp_test_data,\n",
    "        candidates=temp_candidates,\n",
    "        metrics_calculator=metrics_calculator,\n",
    "        n_trials=10,\n",
    "        timeout=7200,\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Лучшие параметры: {best_params}\")\n",
    "    logging.info(\"Результаты:\")\n",
    "    for k in metrics_calculator.k_values:\n",
    "        logging.info(f\"Метрики для K={k}:\")\n",
    "        logging.info(f\"NDCG@{k}: {best_metrics[f'ndcg_{k}']:.4f}\")\n",
    "        logging.info(f\"Precision@{k}: {best_metrics[f'precision_{k}']:.4f}\")\n",
    "        logging.info(f\"Recall@{k}: {best_metrics[f'recall_{k}']:.4f}\")\n",
    "        logging.info(f\"Diversity@{k}: {best_metrics[f'diversity_{k}']:.4f}\")\n",
    "        logging.info(f\"Novelty@{k}: {best_metrics[f'novelty_{k}']:.4f}\")\n",
    "        logging.info(f\"Serendipity@{k}: {best_metrics[f'serendipity_{k}']:.4f}\")\n",
    "        logging.info(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение лучших моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 13:23:03 - INFO - Обучение модели CatBoostRanker...\n",
      "Подготовка отрицательных примеров: 100%|██████████| 601/601 [05:40<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.8775637\tbest: 0.8775637 (0)\ttotal: 7.58s\tremaining: 2h 6m 9s\n",
      "1:\ttest: 0.8775637\tbest: 0.8775637 (0)\ttotal: 13.4s\tremaining: 1h 51m 29s\n",
      "2:\ttest: 0.8775637\tbest: 0.8775637 (0)\ttotal: 15.7s\tremaining: 1h 26m 57s\n",
      "3:\ttest: 0.8775637\tbest: 0.8775637 (0)\ttotal: 18.2s\tremaining: 1h 15m 35s\n",
      "4:\ttest: 0.8775637\tbest: 0.8775637 (0)\ttotal: 20.5s\tremaining: 1h 8m\n",
      "5:\ttest: 0.8775637\tbest: 0.8775637 (0)\ttotal: 26.1s\tremaining: 1h 12m 8s\n",
      "6:\ttest: 0.8775637\tbest: 0.8775637 (0)\ttotal: 28.5s\tremaining: 1h 7m 29s\n",
      "7:\ttest: 0.8775637\tbest: 0.8775637 (0)\ttotal: 30.8s\tremaining: 1h 3m 44s\n",
      "8:\ttest: 0.8846712\tbest: 0.8846712 (8)\ttotal: 34.2s\tremaining: 1h 2m 48s\n",
      "9:\ttest: 0.8846712\tbest: 0.8846712 (8)\ttotal: 36.4s\tremaining: 1h 1s\n",
      "10:\ttest: 0.8846442\tbest: 0.8846712 (8)\ttotal: 39.8s\tremaining: 59m 36s\n",
      "11:\ttest: 0.8846431\tbest: 0.8846712 (8)\ttotal: 42.1s\tremaining: 57m 42s\n",
      "12:\ttest: 0.8847303\tbest: 0.8847303 (12)\ttotal: 44.3s\tremaining: 56m 2s\n",
      "13:\ttest: 0.8846362\tbest: 0.8847303 (12)\ttotal: 46.5s\tremaining: 54m 33s\n",
      "14:\ttest: 0.8847345\tbest: 0.8847345 (14)\ttotal: 49.9s\tremaining: 54m 39s\n",
      "15:\ttest: 0.8847378\tbest: 0.8847378 (15)\ttotal: 52.3s\tremaining: 53m 34s\n",
      "16:\ttest: 0.8847203\tbest: 0.8847378 (15)\ttotal: 54.5s\tremaining: 52m 31s\n",
      "17:\ttest: 0.8846959\tbest: 0.8847378 (15)\ttotal: 56.7s\tremaining: 51m 33s\n",
      "18:\ttest: 0.8847189\tbest: 0.8847378 (15)\ttotal: 58.9s\tremaining: 50m 42s\n",
      "19:\ttest: 0.8847024\tbest: 0.8847378 (15)\ttotal: 1m 1s\tremaining: 49m 59s\n",
      "20:\ttest: 0.8846904\tbest: 0.8847378 (15)\ttotal: 1m 3s\tremaining: 49m 18s\n",
      "21:\ttest: 0.8847141\tbest: 0.8847378 (15)\ttotal: 1m 5s\tremaining: 48m 37s\n",
      "22:\ttest: 0.8847175\tbest: 0.8847378 (15)\ttotal: 1m 7s\tremaining: 48m 2s\n",
      "23:\ttest: 0.8847283\tbest: 0.8847378 (15)\ttotal: 1m 10s\tremaining: 47m 27s\n",
      "24:\ttest: 0.8847582\tbest: 0.8847582 (24)\ttotal: 1m 12s\tremaining: 47m 3s\n",
      "25:\ttest: 0.8847677\tbest: 0.8847677 (25)\ttotal: 1m 14s\tremaining: 46m 34s\n",
      "26:\ttest: 0.8847692\tbest: 0.8847692 (26)\ttotal: 1m 16s\tremaining: 46m 5s\n",
      "27:\ttest: 0.8847630\tbest: 0.8847692 (26)\ttotal: 1m 18s\tremaining: 45m 36s\n",
      "28:\ttest: 0.8847680\tbest: 0.8847692 (26)\ttotal: 1m 20s\tremaining: 45m 10s\n",
      "29:\ttest: 0.8847692\tbest: 0.8847692 (26)\ttotal: 1m 23s\tremaining: 44m 49s\n",
      "30:\ttest: 0.8847630\tbest: 0.8847692 (26)\ttotal: 1m 25s\tremaining: 44m 30s\n",
      "31:\ttest: 0.8847630\tbest: 0.8847692 (26)\ttotal: 1m 27s\tremaining: 44m 12s\n",
      "32:\ttest: 0.8847630\tbest: 0.8847692 (26)\ttotal: 1m 29s\tremaining: 43m 53s\n",
      "33:\ttest: 0.8847630\tbest: 0.8847692 (26)\ttotal: 1m 32s\tremaining: 43m 38s\n",
      "34:\ttest: 0.8847499\tbest: 0.8847692 (26)\ttotal: 1m 34s\tremaining: 43m 23s\n",
      "35:\ttest: 0.8847499\tbest: 0.8847692 (26)\ttotal: 1m 36s\tremaining: 43m 7s\n",
      "36:\ttest: 0.8848291\tbest: 0.8848291 (36)\ttotal: 1m 38s\tremaining: 42m 55s\n",
      "37:\ttest: 0.8847498\tbest: 0.8848291 (36)\ttotal: 1m 41s\tremaining: 42m 43s\n",
      "38:\ttest: 0.8847499\tbest: 0.8848291 (36)\ttotal: 1m 43s\tremaining: 42m 29s\n",
      "39:\ttest: 0.8847692\tbest: 0.8848291 (36)\ttotal: 1m 45s\tremaining: 42m 14s\n",
      "40:\ttest: 0.8849352\tbest: 0.8849352 (40)\ttotal: 1m 47s\tremaining: 42m 1s\n",
      "41:\ttest: 0.8849363\tbest: 0.8849363 (41)\ttotal: 1m 50s\tremaining: 41m 50s\n",
      "42:\ttest: 0.8853038\tbest: 0.8853038 (42)\ttotal: 1m 52s\tremaining: 41m 37s\n",
      "43:\ttest: 0.8853790\tbest: 0.8853790 (43)\ttotal: 1m 54s\tremaining: 41m 27s\n",
      "44:\ttest: 0.8854052\tbest: 0.8854052 (44)\ttotal: 1m 56s\tremaining: 41m 16s\n",
      "45:\ttest: 0.8854005\tbest: 0.8854052 (44)\ttotal: 1m 58s\tremaining: 41m 4s\n",
      "46:\ttest: 0.8855005\tbest: 0.8855005 (46)\ttotal: 2m 1s\tremaining: 40m 53s\n",
      "47:\ttest: 0.8856360\tbest: 0.8856360 (47)\ttotal: 2m 3s\tremaining: 40m 44s\n",
      "48:\ttest: 0.8856360\tbest: 0.8856360 (47)\ttotal: 2m 5s\tremaining: 40m 35s\n",
      "49:\ttest: 0.8856665\tbest: 0.8856665 (49)\ttotal: 2m 7s\tremaining: 40m 23s\n",
      "50:\ttest: 0.8857299\tbest: 0.8857299 (50)\ttotal: 2m 9s\tremaining: 40m 14s\n",
      "51:\ttest: 0.8858659\tbest: 0.8858659 (51)\ttotal: 2m 13s\tremaining: 40m 24s\n",
      "52:\ttest: 0.8858657\tbest: 0.8858659 (51)\ttotal: 2m 15s\tremaining: 40m 15s\n",
      "53:\ttest: 0.8858659\tbest: 0.8858659 (51)\ttotal: 2m 17s\tremaining: 40m 5s\n",
      "54:\ttest: 0.8858571\tbest: 0.8858659 (51)\ttotal: 2m 20s\tremaining: 40m 13s\n",
      "55:\ttest: 0.8858585\tbest: 0.8858659 (51)\ttotal: 2m 22s\tremaining: 40m 3s\n",
      "56:\ttest: 0.8858583\tbest: 0.8858659 (51)\ttotal: 2m 24s\tremaining: 39m 55s\n",
      "57:\ttest: 0.8857522\tbest: 0.8858659 (51)\ttotal: 2m 26s\tremaining: 39m 46s\n",
      "58:\ttest: 0.8857788\tbest: 0.8858659 (51)\ttotal: 2m 29s\tremaining: 39m 36s\n",
      "59:\ttest: 0.8857658\tbest: 0.8858659 (51)\ttotal: 2m 31s\tremaining: 39m 26s\n",
      "60:\ttest: 0.8857373\tbest: 0.8858659 (51)\ttotal: 2m 33s\tremaining: 39m 19s\n",
      "61:\ttest: 0.8857730\tbest: 0.8858659 (51)\ttotal: 2m 35s\tremaining: 39m 10s\n",
      "62:\ttest: 0.8858419\tbest: 0.8858659 (51)\ttotal: 2m 37s\tremaining: 39m 3s\n",
      "63:\ttest: 0.8857916\tbest: 0.8858659 (51)\ttotal: 2m 39s\tremaining: 38m 55s\n",
      "64:\ttest: 0.8858527\tbest: 0.8858659 (51)\ttotal: 2m 42s\tremaining: 38m 56s\n",
      "65:\ttest: 0.8858954\tbest: 0.8858954 (65)\ttotal: 2m 44s\tremaining: 38m 49s\n",
      "66:\ttest: 0.8858866\tbest: 0.8858954 (65)\ttotal: 2m 46s\tremaining: 38m 43s\n",
      "67:\ttest: 0.8859011\tbest: 0.8859011 (67)\ttotal: 2m 48s\tremaining: 38m 34s\n",
      "68:\ttest: 0.8859186\tbest: 0.8859186 (68)\ttotal: 2m 51s\tremaining: 38m 33s\n",
      "69:\ttest: 0.8859505\tbest: 0.8859505 (69)\ttotal: 2m 53s\tremaining: 38m 25s\n",
      "70:\ttest: 0.8859190\tbest: 0.8859505 (69)\ttotal: 2m 55s\tremaining: 38m 18s\n",
      "71:\ttest: 0.8859602\tbest: 0.8859602 (71)\ttotal: 2m 57s\tremaining: 38m 13s\n",
      "72:\ttest: 0.8859594\tbest: 0.8859602 (71)\ttotal: 3m\tremaining: 38m 7s\n",
      "73:\ttest: 0.8859526\tbest: 0.8859602 (71)\ttotal: 3m 2s\tremaining: 37m 59s\n",
      "74:\ttest: 0.8859596\tbest: 0.8859602 (71)\ttotal: 3m 4s\tremaining: 37m 53s\n",
      "75:\ttest: 0.8859618\tbest: 0.8859618 (75)\ttotal: 3m 6s\tremaining: 37m 47s\n",
      "76:\ttest: 0.8859617\tbest: 0.8859618 (75)\ttotal: 3m 9s\tremaining: 37m 53s\n",
      "77:\ttest: 0.8859098\tbest: 0.8859618 (75)\ttotal: 3m 11s\tremaining: 37m 48s\n",
      "78:\ttest: 0.8859477\tbest: 0.8859618 (75)\ttotal: 3m 14s\tremaining: 37m 41s\n",
      "79:\ttest: 0.8859618\tbest: 0.8859618 (75)\ttotal: 3m 16s\tremaining: 37m 36s\n",
      "80:\ttest: 0.8859618\tbest: 0.8859618 (80)\ttotal: 3m 18s\tremaining: 37m 30s\n",
      "81:\ttest: 0.8859251\tbest: 0.8859618 (80)\ttotal: 3m 20s\tremaining: 37m 24s\n",
      "82:\ttest: 0.8859633\tbest: 0.8859633 (82)\ttotal: 3m 22s\tremaining: 37m 18s\n",
      "83:\ttest: 0.8859630\tbest: 0.8859633 (82)\ttotal: 3m 24s\tremaining: 37m 12s\n",
      "84:\ttest: 0.8859345\tbest: 0.8859633 (82)\ttotal: 3m 26s\tremaining: 37m 6s\n",
      "85:\ttest: 0.8859342\tbest: 0.8859633 (82)\ttotal: 3m 28s\tremaining: 37m\n",
      "86:\ttest: 0.8859319\tbest: 0.8859633 (82)\ttotal: 3m 31s\tremaining: 36m 55s\n",
      "87:\ttest: 0.8859320\tbest: 0.8859633 (82)\ttotal: 3m 33s\tremaining: 36m 50s\n",
      "88:\ttest: 0.8859253\tbest: 0.8859633 (82)\ttotal: 3m 35s\tremaining: 36m 44s\n",
      "89:\ttest: 0.8859173\tbest: 0.8859633 (82)\ttotal: 3m 37s\tremaining: 36m 39s\n",
      "90:\ttest: 0.8859253\tbest: 0.8859633 (82)\ttotal: 3m 39s\tremaining: 36m 34s\n",
      "91:\ttest: 0.8859280\tbest: 0.8859633 (82)\ttotal: 3m 41s\tremaining: 36m 27s\n",
      "92:\ttest: 0.8859063\tbest: 0.8859633 (82)\ttotal: 3m 43s\tremaining: 36m 21s\n",
      "93:\ttest: 0.8859063\tbest: 0.8859633 (82)\ttotal: 3m 45s\tremaining: 36m 16s\n",
      "94:\ttest: 0.8859250\tbest: 0.8859633 (82)\ttotal: 3m 47s\tremaining: 36m 10s\n",
      "95:\ttest: 0.8859171\tbest: 0.8859633 (82)\ttotal: 3m 50s\tremaining: 36m 6s\n",
      "96:\ttest: 0.8859091\tbest: 0.8859633 (82)\ttotal: 3m 52s\tremaining: 36m 1s\n",
      "97:\ttest: 0.8859130\tbest: 0.8859633 (82)\ttotal: 3m 54s\tremaining: 35m 56s\n",
      "98:\ttest: 0.8859130\tbest: 0.8859633 (82)\ttotal: 3m 56s\tremaining: 35m 51s\n",
      "99:\ttest: 0.8859191\tbest: 0.8859633 (82)\ttotal: 3m 58s\tremaining: 35m 46s\n",
      "100:\ttest: 0.8859194\tbest: 0.8859633 (82)\ttotal: 4m\tremaining: 35m 41s\n",
      "101:\ttest: 0.8859114\tbest: 0.8859633 (82)\ttotal: 4m 2s\tremaining: 35m 37s\n",
      "102:\ttest: 0.8859114\tbest: 0.8859633 (82)\ttotal: 4m 4s\tremaining: 35m 31s\n",
      "103:\ttest: 0.8859052\tbest: 0.8859633 (82)\ttotal: 4m 6s\tremaining: 35m 27s\n",
      "104:\ttest: 0.8859828\tbest: 0.8859828 (104)\ttotal: 4m 9s\tremaining: 35m 30s\n",
      "105:\ttest: 0.8859853\tbest: 0.8859853 (105)\ttotal: 4m 12s\tremaining: 35m 25s\n",
      "106:\ttest: 0.8859853\tbest: 0.8859853 (105)\ttotal: 4m 14s\tremaining: 35m 21s\n",
      "107:\ttest: 0.8859864\tbest: 0.8859864 (107)\ttotal: 4m 16s\tremaining: 35m 16s\n",
      "108:\ttest: 0.8859861\tbest: 0.8859864 (107)\ttotal: 4m 18s\tremaining: 35m 10s\n",
      "109:\ttest: 0.8863362\tbest: 0.8863362 (109)\ttotal: 4m 22s\tremaining: 35m 26s\n",
      "110:\ttest: 0.8863780\tbest: 0.8863780 (110)\ttotal: 4m 27s\tremaining: 35m 42s\n",
      "111:\ttest: 0.8863782\tbest: 0.8863782 (111)\ttotal: 4m 30s\tremaining: 35m 46s\n",
      "112:\ttest: 0.8863784\tbest: 0.8863784 (112)\ttotal: 4m 35s\tremaining: 36m 2s\n",
      "113:\ttest: 0.8863766\tbest: 0.8863784 (112)\ttotal: 4m 38s\tremaining: 36m 6s\n",
      "114:\ttest: 0.8863736\tbest: 0.8863784 (112)\ttotal: 4m 40s\tremaining: 36m 1s\n",
      "115:\ttest: 0.8864072\tbest: 0.8864072 (115)\ttotal: 4m 44s\tremaining: 36m 5s\n",
      "116:\ttest: 0.8863980\tbest: 0.8864072 (115)\ttotal: 4m 46s\tremaining: 36m\n",
      "117:\ttest: 0.8863671\tbest: 0.8864072 (115)\ttotal: 4m 49s\tremaining: 36m 3s\n",
      "118:\ttest: 0.8863474\tbest: 0.8864072 (115)\ttotal: 4m 51s\tremaining: 35m 59s\n",
      "119:\ttest: 0.8863733\tbest: 0.8864072 (115)\ttotal: 4m 54s\tremaining: 35m 59s\n",
      "120:\ttest: 0.8863845\tbest: 0.8864072 (115)\ttotal: 4m 56s\tremaining: 35m 55s\n",
      "121:\ttest: 0.8863722\tbest: 0.8864072 (115)\ttotal: 5m\tremaining: 35m 59s\n",
      "122:\ttest: 0.8863545\tbest: 0.8864072 (115)\ttotal: 5m 2s\tremaining: 35m 54s\n",
      "123:\ttest: 0.8863650\tbest: 0.8864072 (115)\ttotal: 5m 4s\tremaining: 35m 50s\n",
      "124:\ttest: 0.8863737\tbest: 0.8864072 (115)\ttotal: 5m 7s\tremaining: 35m 51s\n",
      "125:\ttest: 0.8862958\tbest: 0.8864072 (115)\ttotal: 5m 10s\tremaining: 35m 53s\n",
      "126:\ttest: 0.8862789\tbest: 0.8864072 (115)\ttotal: 5m 13s\tremaining: 35m 56s\n",
      "127:\ttest: 0.8862606\tbest: 0.8864072 (115)\ttotal: 5m 15s\tremaining: 35m 51s\n",
      "128:\ttest: 0.8862541\tbest: 0.8864072 (115)\ttotal: 5m 17s\tremaining: 35m 46s\n",
      "129:\ttest: 0.8862768\tbest: 0.8864072 (115)\ttotal: 5m 20s\tremaining: 35m 41s\n",
      "130:\ttest: 0.8862820\tbest: 0.8864072 (115)\ttotal: 5m 21s\tremaining: 35m 35s\n",
      "131:\ttest: 0.8862810\tbest: 0.8864072 (115)\ttotal: 5m 23s\tremaining: 35m 30s\n",
      "132:\ttest: 0.8862771\tbest: 0.8864072 (115)\ttotal: 5m 26s\tremaining: 35m 28s\n",
      "133:\ttest: 0.8862797\tbest: 0.8864072 (115)\ttotal: 5m 28s\tremaining: 35m 23s\n",
      "134:\ttest: 0.8862805\tbest: 0.8864072 (115)\ttotal: 5m 30s\tremaining: 35m 18s\n",
      "135:\ttest: 0.8862681\tbest: 0.8864072 (115)\ttotal: 5m 32s\tremaining: 35m 14s\n",
      "136:\ttest: 0.8862786\tbest: 0.8864072 (115)\ttotal: 5m 35s\tremaining: 35m 16s\n",
      "137:\ttest: 0.8882342\tbest: 0.8882342 (137)\ttotal: 5m 38s\tremaining: 35m 14s\n",
      "138:\ttest: 0.8961625\tbest: 0.8961625 (138)\ttotal: 5m 41s\tremaining: 35m 16s\n",
      "139:\ttest: 0.8968280\tbest: 0.8968280 (139)\ttotal: 5m 43s\tremaining: 35m 11s\n",
      "140:\ttest: 0.8973114\tbest: 0.8973114 (140)\ttotal: 5m 45s\tremaining: 35m 6s\n",
      "141:\ttest: 0.8975983\tbest: 0.8975983 (141)\ttotal: 5m 48s\tremaining: 35m 5s\n",
      "142:\ttest: 0.8977064\tbest: 0.8977064 (142)\ttotal: 5m 51s\tremaining: 35m 9s\n",
      "143:\ttest: 0.8977557\tbest: 0.8977557 (143)\ttotal: 5m 54s\tremaining: 35m 9s\n",
      "144:\ttest: 0.8977775\tbest: 0.8977775 (144)\ttotal: 5m 56s\tremaining: 35m 4s\n",
      "145:\ttest: 0.8978066\tbest: 0.8978066 (145)\ttotal: 5m 58s\tremaining: 34m 59s\n",
      "146:\ttest: 0.8978448\tbest: 0.8978448 (146)\ttotal: 6m\tremaining: 34m 53s\n",
      "147:\ttest: 0.8979110\tbest: 0.8979110 (147)\ttotal: 6m 2s\tremaining: 34m 49s\n",
      "148:\ttest: 0.8979883\tbest: 0.8979883 (148)\ttotal: 6m 4s\tremaining: 34m 44s\n",
      "149:\ttest: 0.8980561\tbest: 0.8980561 (149)\ttotal: 6m 6s\tremaining: 34m 39s\n",
      "150:\ttest: 0.8981530\tbest: 0.8981530 (150)\ttotal: 6m 9s\tremaining: 34m 35s\n",
      "151:\ttest: 0.8982648\tbest: 0.8982648 (151)\ttotal: 6m 11s\tremaining: 34m 30s\n",
      "152:\ttest: 0.8984699\tbest: 0.8984699 (152)\ttotal: 6m 13s\tremaining: 34m 25s\n",
      "153:\ttest: 0.8986869\tbest: 0.8986869 (153)\ttotal: 6m 15s\tremaining: 34m 21s\n",
      "154:\ttest: 0.8988516\tbest: 0.8988516 (154)\ttotal: 6m 17s\tremaining: 34m 16s\n",
      "155:\ttest: 0.8991199\tbest: 0.8991199 (155)\ttotal: 6m 19s\tremaining: 34m 11s\n",
      "156:\ttest: 0.8993765\tbest: 0.8993765 (156)\ttotal: 6m 21s\tremaining: 34m 7s\n",
      "157:\ttest: 0.8996749\tbest: 0.8996749 (157)\ttotal: 6m 23s\tremaining: 34m 2s\n",
      "158:\ttest: 0.8999055\tbest: 0.8999055 (158)\ttotal: 6m 25s\tremaining: 33m 58s\n",
      "159:\ttest: 0.9001140\tbest: 0.9001140 (159)\ttotal: 6m 27s\tremaining: 33m 53s\n",
      "160:\ttest: 0.9002734\tbest: 0.9002734 (160)\ttotal: 6m 29s\tremaining: 33m 49s\n",
      "161:\ttest: 0.9004040\tbest: 0.9004040 (161)\ttotal: 6m 31s\tremaining: 33m 44s\n",
      "162:\ttest: 0.9005206\tbest: 0.9005206 (162)\ttotal: 6m 33s\tremaining: 33m 40s\n",
      "163:\ttest: 0.9006297\tbest: 0.9006297 (163)\ttotal: 6m 36s\tremaining: 33m 39s\n",
      "164:\ttest: 0.9007492\tbest: 0.9007492 (164)\ttotal: 6m 38s\tremaining: 33m 35s\n",
      "165:\ttest: 0.9008543\tbest: 0.9008543 (165)\ttotal: 6m 40s\tremaining: 33m 30s\n",
      "166:\ttest: 0.9009857\tbest: 0.9009857 (166)\ttotal: 6m 42s\tremaining: 33m 26s\n",
      "167:\ttest: 0.9018190\tbest: 0.9018190 (167)\ttotal: 6m 44s\tremaining: 33m 21s\n",
      "168:\ttest: 0.9019374\tbest: 0.9019374 (168)\ttotal: 6m 46s\tremaining: 33m 19s\n",
      "169:\ttest: 0.9020548\tbest: 0.9020548 (169)\ttotal: 6m 49s\tremaining: 33m 18s\n",
      "170:\ttest: 0.9021948\tbest: 0.9021948 (170)\ttotal: 6m 51s\tremaining: 33m 13s\n",
      "171:\ttest: 0.9029389\tbest: 0.9029389 (171)\ttotal: 6m 53s\tremaining: 33m 9s\n",
      "172:\ttest: 0.9031164\tbest: 0.9031164 (172)\ttotal: 6m 55s\tremaining: 33m 5s\n",
      "173:\ttest: 0.9032350\tbest: 0.9032350 (173)\ttotal: 6m 57s\tremaining: 33m 1s\n",
      "174:\ttest: 0.9034814\tbest: 0.9034814 (174)\ttotal: 6m 59s\tremaining: 32m 55s\n",
      "175:\ttest: 0.9035726\tbest: 0.9035726 (175)\ttotal: 7m 1s\tremaining: 32m 51s\n",
      "176:\ttest: 0.9036007\tbest: 0.9036007 (176)\ttotal: 7m 3s\tremaining: 32m 47s\n",
      "177:\ttest: 0.9036601\tbest: 0.9036601 (177)\ttotal: 7m 5s\tremaining: 32m 42s\n",
      "178:\ttest: 0.9036882\tbest: 0.9036882 (178)\ttotal: 7m 7s\tremaining: 32m 38s\n",
      "179:\ttest: 0.9037052\tbest: 0.9037052 (179)\ttotal: 7m 9s\tremaining: 32m 34s\n",
      "180:\ttest: 0.9037298\tbest: 0.9037298 (180)\ttotal: 7m 11s\tremaining: 32m 30s\n",
      "181:\ttest: 0.9037992\tbest: 0.9037992 (181)\ttotal: 7m 13s\tremaining: 32m 26s\n",
      "182:\ttest: 0.9038506\tbest: 0.9038506 (182)\ttotal: 7m 15s\tremaining: 32m 22s\n",
      "183:\ttest: 0.9039129\tbest: 0.9039129 (183)\ttotal: 7m 17s\tremaining: 32m 18s\n",
      "184:\ttest: 0.9039327\tbest: 0.9039327 (184)\ttotal: 7m 18s\tremaining: 32m 13s\n",
      "185:\ttest: 0.9039993\tbest: 0.9039993 (185)\ttotal: 7m 21s\tremaining: 32m 10s\n",
      "186:\ttest: 0.9040795\tbest: 0.9040795 (186)\ttotal: 7m 22s\tremaining: 32m 5s\n",
      "187:\ttest: 0.9041339\tbest: 0.9041339 (187)\ttotal: 7m 24s\tremaining: 32m 1s\n",
      "188:\ttest: 0.9041523\tbest: 0.9041523 (188)\ttotal: 7m 27s\tremaining: 31m 58s\n",
      "189:\ttest: 0.9042288\tbest: 0.9042288 (189)\ttotal: 7m 29s\tremaining: 31m 54s\n",
      "190:\ttest: 0.9042868\tbest: 0.9042868 (190)\ttotal: 7m 31s\tremaining: 31m 50s\n",
      "191:\ttest: 0.9043659\tbest: 0.9043659 (191)\ttotal: 7m 33s\tremaining: 31m 46s\n",
      "192:\ttest: 0.9044969\tbest: 0.9044969 (192)\ttotal: 7m 34s\tremaining: 31m 42s\n",
      "193:\ttest: 0.9046007\tbest: 0.9046007 (193)\ttotal: 7m 36s\tremaining: 31m 38s\n",
      "194:\ttest: 0.9046872\tbest: 0.9046872 (194)\ttotal: 7m 38s\tremaining: 31m 34s\n",
      "195:\ttest: 0.9047572\tbest: 0.9047572 (195)\ttotal: 7m 40s\tremaining: 31m 30s\n",
      "196:\ttest: 0.9048418\tbest: 0.9048418 (196)\ttotal: 7m 42s\tremaining: 31m 27s\n",
      "197:\ttest: 0.9048863\tbest: 0.9048863 (197)\ttotal: 7m 45s\tremaining: 31m 23s\n",
      "198:\ttest: 0.9049688\tbest: 0.9049688 (198)\ttotal: 7m 47s\tremaining: 31m 20s\n",
      "199:\ttest: 0.9050183\tbest: 0.9050183 (199)\ttotal: 7m 49s\tremaining: 31m 16s\n",
      "200:\ttest: 0.9050502\tbest: 0.9050502 (200)\ttotal: 7m 51s\tremaining: 31m 13s\n",
      "201:\ttest: 0.9051114\tbest: 0.9051114 (201)\ttotal: 7m 53s\tremaining: 31m 9s\n",
      "202:\ttest: 0.9051606\tbest: 0.9051606 (202)\ttotal: 7m 55s\tremaining: 31m 5s\n",
      "203:\ttest: 0.9051805\tbest: 0.9051805 (203)\ttotal: 7m 57s\tremaining: 31m 2s\n",
      "204:\ttest: 0.9052186\tbest: 0.9052186 (204)\ttotal: 7m 59s\tremaining: 30m 58s\n",
      "205:\ttest: 0.9052640\tbest: 0.9052640 (205)\ttotal: 8m 1s\tremaining: 30m 54s\n",
      "206:\ttest: 0.9053300\tbest: 0.9053300 (206)\ttotal: 8m 3s\tremaining: 30m 51s\n",
      "207:\ttest: 0.9053534\tbest: 0.9053534 (207)\ttotal: 8m 5s\tremaining: 30m 47s\n",
      "208:\ttest: 0.9053758\tbest: 0.9053758 (208)\ttotal: 8m 7s\tremaining: 30m 44s\n",
      "209:\ttest: 0.9054004\tbest: 0.9054004 (209)\ttotal: 8m 9s\tremaining: 30m 40s\n",
      "210:\ttest: 0.9053981\tbest: 0.9054004 (209)\ttotal: 8m 11s\tremaining: 30m 37s\n",
      "211:\ttest: 0.9053916\tbest: 0.9054004 (209)\ttotal: 8m 13s\tremaining: 30m 33s\n",
      "212:\ttest: 0.9054217\tbest: 0.9054217 (212)\ttotal: 8m 15s\tremaining: 30m 30s\n",
      "213:\ttest: 0.9054133\tbest: 0.9054217 (212)\ttotal: 8m 17s\tremaining: 30m 26s\n",
      "214:\ttest: 0.9054360\tbest: 0.9054360 (214)\ttotal: 8m 19s\tremaining: 30m 23s\n",
      "215:\ttest: 0.9054639\tbest: 0.9054639 (215)\ttotal: 8m 21s\tremaining: 30m 19s\n",
      "216:\ttest: 0.9054805\tbest: 0.9054805 (216)\ttotal: 8m 23s\tremaining: 30m 16s\n",
      "217:\ttest: 0.9055074\tbest: 0.9055074 (217)\ttotal: 8m 25s\tremaining: 30m 13s\n",
      "218:\ttest: 0.9055247\tbest: 0.9055247 (218)\ttotal: 8m 27s\tremaining: 30m 9s\n",
      "219:\ttest: 0.9055364\tbest: 0.9055364 (219)\ttotal: 8m 29s\tremaining: 30m 6s\n",
      "220:\ttest: 0.9055526\tbest: 0.9055526 (220)\ttotal: 8m 31s\tremaining: 30m 3s\n",
      "221:\ttest: 0.9055485\tbest: 0.9055526 (220)\ttotal: 8m 33s\tremaining: 29m 59s\n",
      "222:\ttest: 0.9055734\tbest: 0.9055734 (222)\ttotal: 8m 35s\tremaining: 29m 56s\n",
      "223:\ttest: 0.9055777\tbest: 0.9055777 (223)\ttotal: 8m 37s\tremaining: 29m 53s\n",
      "224:\ttest: 0.9056070\tbest: 0.9056070 (224)\ttotal: 8m 39s\tremaining: 29m 49s\n",
      "225:\ttest: 0.9056178\tbest: 0.9056178 (225)\ttotal: 8m 41s\tremaining: 29m 46s\n",
      "226:\ttest: 0.9056459\tbest: 0.9056459 (226)\ttotal: 8m 43s\tremaining: 29m 43s\n",
      "227:\ttest: 0.9056642\tbest: 0.9056642 (227)\ttotal: 8m 45s\tremaining: 29m 40s\n",
      "228:\ttest: 0.9056854\tbest: 0.9056854 (228)\ttotal: 8m 47s\tremaining: 29m 36s\n",
      "229:\ttest: 0.9056969\tbest: 0.9056969 (229)\ttotal: 8m 49s\tremaining: 29m 33s\n",
      "230:\ttest: 0.9057131\tbest: 0.9057131 (230)\ttotal: 8m 51s\tremaining: 29m 30s\n",
      "231:\ttest: 0.9057339\tbest: 0.9057339 (231)\ttotal: 8m 53s\tremaining: 29m 27s\n",
      "232:\ttest: 0.9057415\tbest: 0.9057415 (232)\ttotal: 8m 55s\tremaining: 29m 23s\n",
      "233:\ttest: 0.9057649\tbest: 0.9057649 (233)\ttotal: 8m 57s\tremaining: 29m 20s\n",
      "234:\ttest: 0.9057819\tbest: 0.9057819 (234)\ttotal: 8m 59s\tremaining: 29m 17s\n",
      "235:\ttest: 0.9057863\tbest: 0.9057863 (235)\ttotal: 9m 1s\tremaining: 29m 14s\n",
      "236:\ttest: 0.9057958\tbest: 0.9057958 (236)\ttotal: 9m 4s\tremaining: 29m 12s\n",
      "237:\ttest: 0.9058054\tbest: 0.9058054 (237)\ttotal: 9m 6s\tremaining: 29m 9s\n",
      "238:\ttest: 0.9058129\tbest: 0.9058129 (238)\ttotal: 9m 8s\tremaining: 29m 6s\n",
      "239:\ttest: 0.9058203\tbest: 0.9058203 (239)\ttotal: 9m 10s\tremaining: 29m 3s\n",
      "240:\ttest: 0.9058555\tbest: 0.9058555 (240)\ttotal: 9m 12s\tremaining: 29m\n",
      "241:\ttest: 0.9058461\tbest: 0.9058555 (240)\ttotal: 9m 14s\tremaining: 28m 57s\n",
      "242:\ttest: 0.9058730\tbest: 0.9058730 (242)\ttotal: 9m 16s\tremaining: 28m 53s\n",
      "243:\ttest: 0.9058876\tbest: 0.9058876 (243)\ttotal: 9m 18s\tremaining: 28m 50s\n",
      "244:\ttest: 0.9059154\tbest: 0.9059154 (244)\ttotal: 9m 20s\tremaining: 28m 46s\n",
      "245:\ttest: 0.9059490\tbest: 0.9059490 (245)\ttotal: 9m 22s\tremaining: 28m 43s\n",
      "246:\ttest: 0.9059457\tbest: 0.9059490 (245)\ttotal: 9m 24s\tremaining: 28m 40s\n",
      "247:\ttest: 0.9059786\tbest: 0.9059786 (247)\ttotal: 9m 26s\tremaining: 28m 37s\n",
      "248:\ttest: 0.9059864\tbest: 0.9059864 (248)\ttotal: 9m 28s\tremaining: 28m 34s\n",
      "249:\ttest: 0.9060263\tbest: 0.9060263 (249)\ttotal: 9m 30s\tremaining: 28m 31s\n",
      "250:\ttest: 0.9060394\tbest: 0.9060394 (250)\ttotal: 9m 32s\tremaining: 28m 27s\n",
      "251:\ttest: 0.9060621\tbest: 0.9060621 (251)\ttotal: 9m 34s\tremaining: 28m 24s\n",
      "252:\ttest: 0.9060641\tbest: 0.9060641 (252)\ttotal: 9m 36s\tremaining: 28m 21s\n",
      "253:\ttest: 0.9060752\tbest: 0.9060752 (253)\ttotal: 9m 38s\tremaining: 28m 18s\n",
      "254:\ttest: 0.9061240\tbest: 0.9061240 (254)\ttotal: 9m 40s\tremaining: 28m 15s\n",
      "255:\ttest: 0.9061477\tbest: 0.9061477 (255)\ttotal: 9m 42s\tremaining: 28m 12s\n",
      "256:\ttest: 0.9061595\tbest: 0.9061595 (256)\ttotal: 9m 44s\tremaining: 28m 8s\n",
      "257:\ttest: 0.9061665\tbest: 0.9061665 (257)\ttotal: 9m 46s\tremaining: 28m 6s\n",
      "258:\ttest: 0.9061681\tbest: 0.9061681 (258)\ttotal: 9m 48s\tremaining: 28m 3s\n",
      "259:\ttest: 0.9062033\tbest: 0.9062033 (259)\ttotal: 9m 50s\tremaining: 28m\n",
      "260:\ttest: 0.9062147\tbest: 0.9062147 (260)\ttotal: 9m 52s\tremaining: 27m 57s\n",
      "261:\ttest: 0.9062357\tbest: 0.9062357 (261)\ttotal: 9m 54s\tremaining: 27m 54s\n",
      "262:\ttest: 0.9062444\tbest: 0.9062444 (262)\ttotal: 9m 56s\tremaining: 27m 51s\n",
      "263:\ttest: 0.9062632\tbest: 0.9062632 (263)\ttotal: 9m 58s\tremaining: 27m 48s\n",
      "264:\ttest: 0.9062672\tbest: 0.9062672 (264)\ttotal: 10m\tremaining: 27m 45s\n",
      "265:\ttest: 0.9062506\tbest: 0.9062672 (264)\ttotal: 10m 2s\tremaining: 27m 42s\n",
      "266:\ttest: 0.9062606\tbest: 0.9062672 (264)\ttotal: 10m 4s\tremaining: 27m 39s\n",
      "267:\ttest: 0.9062660\tbest: 0.9062672 (264)\ttotal: 10m 6s\tremaining: 27m 36s\n",
      "268:\ttest: 0.9062446\tbest: 0.9062672 (264)\ttotal: 10m 8s\tremaining: 27m 33s\n",
      "269:\ttest: 0.9062520\tbest: 0.9062672 (264)\ttotal: 10m 10s\tremaining: 27m 30s\n",
      "270:\ttest: 0.9062591\tbest: 0.9062672 (264)\ttotal: 10m 12s\tremaining: 27m 27s\n",
      "271:\ttest: 0.9062724\tbest: 0.9062724 (271)\ttotal: 10m 14s\tremaining: 27m 24s\n",
      "272:\ttest: 0.9062798\tbest: 0.9062798 (272)\ttotal: 10m 16s\tremaining: 27m 21s\n",
      "273:\ttest: 0.9062775\tbest: 0.9062798 (272)\ttotal: 10m 18s\tremaining: 27m 18s\n",
      "274:\ttest: 0.9062744\tbest: 0.9062798 (272)\ttotal: 10m 20s\tremaining: 27m 14s\n",
      "275:\ttest: 0.9062903\tbest: 0.9062903 (275)\ttotal: 10m 21s\tremaining: 27m 11s\n",
      "276:\ttest: 0.9062744\tbest: 0.9062903 (275)\ttotal: 10m 23s\tremaining: 27m 8s\n",
      "277:\ttest: 0.9062805\tbest: 0.9062903 (275)\ttotal: 10m 25s\tremaining: 27m 5s\n",
      "278:\ttest: 0.9062768\tbest: 0.9062903 (275)\ttotal: 10m 27s\tremaining: 27m 2s\n",
      "279:\ttest: 0.9062759\tbest: 0.9062903 (275)\ttotal: 10m 29s\tremaining: 26m 59s\n",
      "280:\ttest: 0.9062802\tbest: 0.9062903 (275)\ttotal: 10m 31s\tremaining: 26m 56s\n",
      "281:\ttest: 0.9062806\tbest: 0.9062903 (275)\ttotal: 10m 33s\tremaining: 26m 53s\n",
      "282:\ttest: 0.9062894\tbest: 0.9062903 (275)\ttotal: 10m 35s\tremaining: 26m 50s\n",
      "283:\ttest: 0.9063076\tbest: 0.9063076 (283)\ttotal: 10m 37s\tremaining: 26m 47s\n",
      "284:\ttest: 0.9063167\tbest: 0.9063167 (284)\ttotal: 10m 39s\tremaining: 26m 44s\n",
      "285:\ttest: 0.9063196\tbest: 0.9063196 (285)\ttotal: 10m 41s\tremaining: 26m 41s\n",
      "286:\ttest: 0.9063412\tbest: 0.9063412 (286)\ttotal: 10m 43s\tremaining: 26m 38s\n",
      "287:\ttest: 0.9063572\tbest: 0.9063572 (287)\ttotal: 10m 45s\tremaining: 26m 35s\n",
      "288:\ttest: 0.9063594\tbest: 0.9063594 (288)\ttotal: 10m 47s\tremaining: 26m 32s\n",
      "289:\ttest: 0.9063483\tbest: 0.9063594 (288)\ttotal: 10m 49s\tremaining: 26m 29s\n",
      "290:\ttest: 0.9063427\tbest: 0.9063594 (288)\ttotal: 10m 51s\tremaining: 26m 26s\n",
      "291:\ttest: 0.9063376\tbest: 0.9063594 (288)\ttotal: 10m 53s\tremaining: 26m 23s\n",
      "292:\ttest: 0.9063397\tbest: 0.9063594 (288)\ttotal: 10m 55s\tremaining: 26m 20s\n",
      "293:\ttest: 0.9063451\tbest: 0.9063594 (288)\ttotal: 10m 57s\tremaining: 26m 18s\n",
      "294:\ttest: 0.9063403\tbest: 0.9063594 (288)\ttotal: 10m 59s\tremaining: 26m 15s\n",
      "295:\ttest: 0.9063106\tbest: 0.9063594 (288)\ttotal: 11m\tremaining: 26m 12s\n",
      "296:\ttest: 0.9063229\tbest: 0.9063594 (288)\ttotal: 11m 2s\tremaining: 26m 9s\n",
      "297:\ttest: 0.9063221\tbest: 0.9063594 (288)\ttotal: 11m 4s\tremaining: 26m 6s\n",
      "298:\ttest: 0.9063185\tbest: 0.9063594 (288)\ttotal: 11m 6s\tremaining: 26m 3s\n",
      "299:\ttest: 0.9063261\tbest: 0.9063594 (288)\ttotal: 11m 9s\tremaining: 26m 1s\n",
      "300:\ttest: 0.9063273\tbest: 0.9063594 (288)\ttotal: 11m 11s\tremaining: 25m 58s\n",
      "301:\ttest: 0.9063225\tbest: 0.9063594 (288)\ttotal: 11m 12s\tremaining: 25m 55s\n",
      "302:\ttest: 0.9066854\tbest: 0.9066854 (302)\ttotal: 11m 15s\tremaining: 25m 53s\n",
      "303:\ttest: 0.9066911\tbest: 0.9066911 (303)\ttotal: 11m 17s\tremaining: 25m 50s\n",
      "304:\ttest: 0.9066799\tbest: 0.9066911 (303)\ttotal: 11m 19s\tremaining: 25m 47s\n",
      "305:\ttest: 0.9066801\tbest: 0.9066911 (303)\ttotal: 11m 21s\tremaining: 25m 45s\n",
      "306:\ttest: 0.9066855\tbest: 0.9066911 (303)\ttotal: 11m 23s\tremaining: 25m 42s\n",
      "307:\ttest: 0.9066883\tbest: 0.9066911 (303)\ttotal: 11m 25s\tremaining: 25m 39s\n",
      "308:\ttest: 0.9066921\tbest: 0.9066921 (308)\ttotal: 11m 26s\tremaining: 25m 36s\n",
      "309:\ttest: 0.9066972\tbest: 0.9066972 (309)\ttotal: 11m 28s\tremaining: 25m 33s\n",
      "310:\ttest: 0.9067053\tbest: 0.9067053 (310)\ttotal: 11m 30s\tremaining: 25m 30s\n",
      "311:\ttest: 0.9067085\tbest: 0.9067085 (311)\ttotal: 11m 32s\tremaining: 25m 27s\n",
      "312:\ttest: 0.9067022\tbest: 0.9067085 (311)\ttotal: 11m 34s\tremaining: 25m 25s\n",
      "313:\ttest: 0.9066948\tbest: 0.9067085 (311)\ttotal: 11m 36s\tremaining: 25m 22s\n",
      "314:\ttest: 0.9066806\tbest: 0.9067085 (311)\ttotal: 11m 38s\tremaining: 25m 19s\n",
      "315:\ttest: 0.9066790\tbest: 0.9067085 (311)\ttotal: 11m 40s\tremaining: 25m 16s\n",
      "316:\ttest: 0.9066766\tbest: 0.9067085 (311)\ttotal: 11m 42s\tremaining: 25m 13s\n",
      "317:\ttest: 0.9066839\tbest: 0.9067085 (311)\ttotal: 11m 44s\tremaining: 25m 10s\n",
      "318:\ttest: 0.9066842\tbest: 0.9067085 (311)\ttotal: 11m 46s\tremaining: 25m 8s\n",
      "319:\ttest: 0.9066856\tbest: 0.9067085 (311)\ttotal: 11m 48s\tremaining: 25m 5s\n",
      "320:\ttest: 0.9066959\tbest: 0.9067085 (311)\ttotal: 11m 50s\tremaining: 25m 2s\n",
      "321:\ttest: 0.9066890\tbest: 0.9067085 (311)\ttotal: 11m 52s\tremaining: 24m 59s\n",
      "322:\ttest: 0.9066944\tbest: 0.9067085 (311)\ttotal: 11m 54s\tremaining: 24m 57s\n",
      "323:\ttest: 0.9066872\tbest: 0.9067085 (311)\ttotal: 11m 56s\tremaining: 24m 54s\n",
      "324:\ttest: 0.9066955\tbest: 0.9067085 (311)\ttotal: 11m 58s\tremaining: 24m 51s\n",
      "325:\ttest: 0.9067011\tbest: 0.9067085 (311)\ttotal: 11m 59s\tremaining: 24m 48s\n",
      "326:\ttest: 0.9067104\tbest: 0.9067104 (326)\ttotal: 12m 2s\tremaining: 24m 46s\n",
      "327:\ttest: 0.9066960\tbest: 0.9067104 (326)\ttotal: 12m 4s\tremaining: 24m 43s\n",
      "328:\ttest: 0.9067078\tbest: 0.9067104 (326)\ttotal: 12m 6s\tremaining: 24m 41s\n",
      "329:\ttest: 0.9067031\tbest: 0.9067104 (326)\ttotal: 12m 8s\tremaining: 24m 38s\n",
      "330:\ttest: 0.9067130\tbest: 0.9067130 (330)\ttotal: 12m 9s\tremaining: 24m 35s\n",
      "331:\ttest: 0.9067198\tbest: 0.9067198 (331)\ttotal: 12m 11s\tremaining: 24m 32s\n",
      "332:\ttest: 0.9067221\tbest: 0.9067221 (332)\ttotal: 12m 13s\tremaining: 24m 29s\n",
      "333:\ttest: 0.9067393\tbest: 0.9067393 (333)\ttotal: 12m 16s\tremaining: 24m 29s\n",
      "334:\ttest: 0.9067314\tbest: 0.9067393 (333)\ttotal: 12m 18s\tremaining: 24m 26s\n",
      "335:\ttest: 0.9067260\tbest: 0.9067393 (333)\ttotal: 12m 20s\tremaining: 24m 24s\n",
      "336:\ttest: 0.9067352\tbest: 0.9067393 (333)\ttotal: 12m 23s\tremaining: 24m 22s\n",
      "337:\ttest: 0.9067521\tbest: 0.9067521 (337)\ttotal: 12m 25s\tremaining: 24m 20s\n",
      "338:\ttest: 0.9067342\tbest: 0.9067521 (337)\ttotal: 12m 27s\tremaining: 24m 17s\n",
      "339:\ttest: 0.9067344\tbest: 0.9067521 (337)\ttotal: 12m 29s\tremaining: 24m 14s\n",
      "340:\ttest: 0.9067394\tbest: 0.9067521 (337)\ttotal: 12m 31s\tremaining: 24m 12s\n",
      "341:\ttest: 0.9067594\tbest: 0.9067594 (341)\ttotal: 12m 33s\tremaining: 24m 9s\n",
      "342:\ttest: 0.9067660\tbest: 0.9067660 (342)\ttotal: 12m 35s\tremaining: 24m 7s\n",
      "343:\ttest: 0.9067539\tbest: 0.9067660 (342)\ttotal: 12m 37s\tremaining: 24m 4s\n",
      "344:\ttest: 0.9067622\tbest: 0.9067660 (342)\ttotal: 12m 40s\tremaining: 24m 4s\n",
      "345:\ttest: 0.9067704\tbest: 0.9067704 (345)\ttotal: 12m 42s\tremaining: 24m 1s\n",
      "346:\ttest: 0.9067988\tbest: 0.9067988 (346)\ttotal: 12m 46s\tremaining: 24m 1s\n",
      "347:\ttest: 0.9067964\tbest: 0.9067988 (346)\ttotal: 12m 49s\tremaining: 24m 1s\n",
      "348:\ttest: 0.9068018\tbest: 0.9068018 (348)\ttotal: 12m 51s\tremaining: 23m 59s\n",
      "349:\ttest: 0.9067886\tbest: 0.9068018 (348)\ttotal: 12m 53s\tremaining: 23m 56s\n",
      "350:\ttest: 0.9068206\tbest: 0.9068206 (350)\ttotal: 12m 55s\tremaining: 23m 54s\n",
      "351:\ttest: 0.9068400\tbest: 0.9068400 (351)\ttotal: 12m 58s\tremaining: 23m 53s\n",
      "352:\ttest: 0.9068435\tbest: 0.9068435 (352)\ttotal: 13m\tremaining: 23m 50s\n",
      "353:\ttest: 0.9068406\tbest: 0.9068435 (352)\ttotal: 13m 2s\tremaining: 23m 48s\n",
      "354:\ttest: 0.9068422\tbest: 0.9068435 (352)\ttotal: 13m 4s\tremaining: 23m 45s\n",
      "355:\ttest: 0.9068574\tbest: 0.9068574 (355)\ttotal: 13m 6s\tremaining: 23m 43s\n",
      "356:\ttest: 0.9068667\tbest: 0.9068667 (356)\ttotal: 13m 8s\tremaining: 23m 40s\n",
      "357:\ttest: 0.9068645\tbest: 0.9068667 (356)\ttotal: 13m 12s\tremaining: 23m 40s\n",
      "358:\ttest: 0.9068642\tbest: 0.9068667 (356)\ttotal: 13m 14s\tremaining: 23m 37s\n",
      "359:\ttest: 0.9068626\tbest: 0.9068667 (356)\ttotal: 13m 16s\tremaining: 23m 35s\n",
      "360:\ttest: 0.9069204\tbest: 0.9069204 (360)\ttotal: 13m 19s\tremaining: 23m 34s\n",
      "361:\ttest: 0.9069406\tbest: 0.9069406 (361)\ttotal: 13m 21s\tremaining: 23m 31s\n",
      "362:\ttest: 0.9069402\tbest: 0.9069406 (361)\ttotal: 13m 23s\tremaining: 23m 29s\n",
      "363:\ttest: 0.9069470\tbest: 0.9069470 (363)\ttotal: 13m 25s\tremaining: 23m 27s\n",
      "364:\ttest: 0.9069509\tbest: 0.9069509 (364)\ttotal: 13m 27s\tremaining: 23m 25s\n",
      "365:\ttest: 0.9069478\tbest: 0.9069509 (364)\ttotal: 13m 29s\tremaining: 23m 22s\n",
      "366:\ttest: 0.9069407\tbest: 0.9069509 (364)\ttotal: 13m 31s\tremaining: 23m 19s\n",
      "367:\ttest: 0.9069534\tbest: 0.9069534 (367)\ttotal: 13m 33s\tremaining: 23m 17s\n",
      "368:\ttest: 0.9069621\tbest: 0.9069621 (368)\ttotal: 13m 35s\tremaining: 23m 14s\n",
      "369:\ttest: 0.9069631\tbest: 0.9069631 (369)\ttotal: 13m 37s\tremaining: 23m 11s\n",
      "370:\ttest: 0.9069689\tbest: 0.9069689 (370)\ttotal: 13m 39s\tremaining: 23m 9s\n",
      "371:\ttest: 0.9069818\tbest: 0.9069818 (371)\ttotal: 13m 41s\tremaining: 23m 6s\n",
      "372:\ttest: 0.9069735\tbest: 0.9069818 (371)\ttotal: 13m 42s\tremaining: 23m 3s\n",
      "373:\ttest: 0.9069681\tbest: 0.9069818 (371)\ttotal: 13m 44s\tremaining: 23m\n",
      "374:\ttest: 0.9069519\tbest: 0.9069818 (371)\ttotal: 13m 48s\tremaining: 23m\n",
      "375:\ttest: 0.9069536\tbest: 0.9069818 (371)\ttotal: 13m 50s\tremaining: 22m 58s\n",
      "376:\ttest: 0.9069518\tbest: 0.9069818 (371)\ttotal: 13m 53s\tremaining: 22m 58s\n",
      "377:\ttest: 0.9069638\tbest: 0.9069818 (371)\ttotal: 13m 56s\tremaining: 22m 56s\n",
      "378:\ttest: 0.9069681\tbest: 0.9069818 (371)\ttotal: 13m 59s\tremaining: 22m 55s\n",
      "379:\ttest: 0.9069843\tbest: 0.9069843 (379)\ttotal: 14m 1s\tremaining: 22m 53s\n",
      "380:\ttest: 0.9069983\tbest: 0.9069983 (380)\ttotal: 14m 3s\tremaining: 22m 50s\n",
      "381:\ttest: 0.9070124\tbest: 0.9070124 (381)\ttotal: 14m 5s\tremaining: 22m 47s\n",
      "382:\ttest: 0.9070178\tbest: 0.9070178 (382)\ttotal: 14m 8s\tremaining: 22m 46s\n",
      "383:\ttest: 0.9070251\tbest: 0.9070251 (383)\ttotal: 14m 9s\tremaining: 22m 43s\n",
      "384:\ttest: 0.9070248\tbest: 0.9070251 (383)\ttotal: 14m 11s\tremaining: 22m 40s\n",
      "385:\ttest: 0.9070347\tbest: 0.9070347 (385)\ttotal: 14m 13s\tremaining: 22m 37s\n",
      "386:\ttest: 0.9070493\tbest: 0.9070493 (386)\ttotal: 14m 15s\tremaining: 22m 35s\n",
      "387:\ttest: 0.9070504\tbest: 0.9070504 (387)\ttotal: 14m 17s\tremaining: 22m 33s\n",
      "388:\ttest: 0.9070619\tbest: 0.9070619 (388)\ttotal: 14m 19s\tremaining: 22m 30s\n",
      "389:\ttest: 0.9070632\tbest: 0.9070632 (389)\ttotal: 14m 21s\tremaining: 22m 27s\n",
      "390:\ttest: 0.9070617\tbest: 0.9070632 (389)\ttotal: 14m 23s\tremaining: 22m 25s\n",
      "391:\ttest: 0.9070625\tbest: 0.9070632 (389)\ttotal: 14m 25s\tremaining: 22m 22s\n",
      "392:\ttest: 0.9070740\tbest: 0.9070740 (392)\ttotal: 14m 27s\tremaining: 22m 19s\n",
      "393:\ttest: 0.9070763\tbest: 0.9070763 (393)\ttotal: 14m 29s\tremaining: 22m 17s\n",
      "394:\ttest: 0.9071031\tbest: 0.9071031 (394)\ttotal: 14m 31s\tremaining: 22m 14s\n",
      "395:\ttest: 0.9070749\tbest: 0.9071031 (394)\ttotal: 14m 33s\tremaining: 22m 12s\n",
      "396:\ttest: 0.9070715\tbest: 0.9071031 (394)\ttotal: 14m 35s\tremaining: 22m 9s\n",
      "397:\ttest: 0.9070850\tbest: 0.9071031 (394)\ttotal: 14m 37s\tremaining: 22m 7s\n",
      "398:\ttest: 0.9071085\tbest: 0.9071085 (398)\ttotal: 14m 39s\tremaining: 22m 4s\n",
      "399:\ttest: 0.9070923\tbest: 0.9071085 (398)\ttotal: 14m 41s\tremaining: 22m 2s\n",
      "400:\ttest: 0.9070954\tbest: 0.9071085 (398)\ttotal: 14m 43s\tremaining: 21m 59s\n",
      "401:\ttest: 0.9071191\tbest: 0.9071191 (401)\ttotal: 14m 46s\tremaining: 21m 58s\n",
      "402:\ttest: 0.9071329\tbest: 0.9071329 (402)\ttotal: 14m 48s\tremaining: 21m 55s\n",
      "403:\ttest: 0.9071593\tbest: 0.9071593 (403)\ttotal: 14m 51s\tremaining: 21m 54s\n",
      "404:\ttest: 0.9071429\tbest: 0.9071593 (403)\ttotal: 14m 53s\tremaining: 21m 52s\n",
      "405:\ttest: 0.9071385\tbest: 0.9071593 (403)\ttotal: 14m 55s\tremaining: 21m 49s\n",
      "406:\ttest: 0.9071512\tbest: 0.9071593 (403)\ttotal: 14m 57s\tremaining: 21m 47s\n",
      "407:\ttest: 0.9071590\tbest: 0.9071593 (403)\ttotal: 14m 59s\tremaining: 21m 44s\n",
      "408:\ttest: 0.9071506\tbest: 0.9071593 (403)\ttotal: 15m 1s\tremaining: 21m 42s\n",
      "409:\ttest: 0.9071551\tbest: 0.9071593 (403)\ttotal: 15m 3s\tremaining: 21m 39s\n",
      "410:\ttest: 0.9071712\tbest: 0.9071712 (410)\ttotal: 15m 5s\tremaining: 21m 37s\n",
      "411:\ttest: 0.9071610\tbest: 0.9071712 (410)\ttotal: 15m 7s\tremaining: 21m 34s\n",
      "412:\ttest: 0.9071652\tbest: 0.9071712 (410)\ttotal: 15m 9s\tremaining: 21m 32s\n",
      "413:\ttest: 0.9071721\tbest: 0.9071721 (413)\ttotal: 15m 11s\tremaining: 21m 29s\n",
      "414:\ttest: 0.9071692\tbest: 0.9071721 (413)\ttotal: 15m 12s\tremaining: 21m 26s\n",
      "415:\ttest: 0.9071698\tbest: 0.9071721 (413)\ttotal: 15m 14s\tremaining: 21m 24s\n",
      "416:\ttest: 0.9071803\tbest: 0.9071803 (416)\ttotal: 15m 16s\tremaining: 21m 21s\n",
      "417:\ttest: 0.9071835\tbest: 0.9071835 (417)\ttotal: 15m 18s\tremaining: 21m 19s\n",
      "418:\ttest: 0.9072418\tbest: 0.9072418 (418)\ttotal: 15m 21s\tremaining: 21m 18s\n",
      "419:\ttest: 0.9072510\tbest: 0.9072510 (419)\ttotal: 15m 23s\tremaining: 21m 15s\n",
      "420:\ttest: 0.9072635\tbest: 0.9072635 (420)\ttotal: 15m 25s\tremaining: 21m 12s\n",
      "421:\ttest: 0.9072503\tbest: 0.9072635 (420)\ttotal: 15m 28s\tremaining: 21m 11s\n",
      "422:\ttest: 0.9072582\tbest: 0.9072635 (420)\ttotal: 15m 30s\tremaining: 21m 9s\n",
      "423:\ttest: 0.9072646\tbest: 0.9072646 (423)\ttotal: 15m 32s\tremaining: 21m 6s\n",
      "424:\ttest: 0.9072667\tbest: 0.9072667 (424)\ttotal: 15m 35s\tremaining: 21m 5s\n",
      "425:\ttest: 0.9072678\tbest: 0.9072678 (425)\ttotal: 15m 37s\tremaining: 21m 2s\n",
      "426:\ttest: 0.9072890\tbest: 0.9072890 (426)\ttotal: 15m 39s\tremaining: 21m\n",
      "427:\ttest: 0.9072847\tbest: 0.9072890 (426)\ttotal: 15m 40s\tremaining: 20m 57s\n",
      "428:\ttest: 0.9072807\tbest: 0.9072890 (426)\ttotal: 15m 42s\tremaining: 20m 54s\n",
      "429:\ttest: 0.9072778\tbest: 0.9072890 (426)\ttotal: 15m 44s\tremaining: 20m 52s\n",
      "430:\ttest: 0.9072857\tbest: 0.9072890 (426)\ttotal: 15m 46s\tremaining: 20m 49s\n",
      "431:\ttest: 0.9072854\tbest: 0.9072890 (426)\ttotal: 15m 48s\tremaining: 20m 46s\n",
      "432:\ttest: 0.9072863\tbest: 0.9072890 (426)\ttotal: 15m 50s\tremaining: 20m 44s\n",
      "433:\ttest: 0.9072978\tbest: 0.9072978 (433)\ttotal: 15m 51s\tremaining: 20m 41s\n",
      "434:\ttest: 0.9072817\tbest: 0.9072978 (433)\ttotal: 15m 54s\tremaining: 20m 39s\n",
      "435:\ttest: 0.9073109\tbest: 0.9073109 (435)\ttotal: 15m 55s\tremaining: 20m 36s\n",
      "436:\ttest: 0.9072708\tbest: 0.9073109 (435)\ttotal: 15m 58s\tremaining: 20m 34s\n",
      "437:\ttest: 0.9072777\tbest: 0.9073109 (435)\ttotal: 16m\tremaining: 20m 32s\n",
      "438:\ttest: 0.9072866\tbest: 0.9073109 (435)\ttotal: 16m 2s\tremaining: 20m 29s\n",
      "439:\ttest: 0.9072944\tbest: 0.9073109 (435)\ttotal: 16m 4s\tremaining: 20m 27s\n",
      "440:\ttest: 0.9072968\tbest: 0.9073109 (435)\ttotal: 16m 6s\tremaining: 20m 25s\n",
      "441:\ttest: 0.9073084\tbest: 0.9073109 (435)\ttotal: 16m 8s\tremaining: 20m 22s\n",
      "442:\ttest: 0.9073110\tbest: 0.9073110 (442)\ttotal: 16m 10s\tremaining: 20m 20s\n",
      "443:\ttest: 0.9073156\tbest: 0.9073156 (443)\ttotal: 16m 12s\tremaining: 20m 17s\n",
      "444:\ttest: 0.9073240\tbest: 0.9073240 (444)\ttotal: 16m 13s\tremaining: 20m 14s\n",
      "445:\ttest: 0.9073085\tbest: 0.9073240 (444)\ttotal: 16m 15s\tremaining: 20m 12s\n",
      "446:\ttest: 0.9073098\tbest: 0.9073240 (444)\ttotal: 16m 17s\tremaining: 20m 9s\n",
      "447:\ttest: 0.9073185\tbest: 0.9073240 (444)\ttotal: 16m 19s\tremaining: 20m 7s\n",
      "448:\ttest: 0.9073561\tbest: 0.9073561 (448)\ttotal: 16m 21s\tremaining: 20m 4s\n",
      "449:\ttest: 0.9073553\tbest: 0.9073561 (448)\ttotal: 16m 23s\tremaining: 20m 2s\n",
      "450:\ttest: 0.9073431\tbest: 0.9073561 (448)\ttotal: 16m 25s\tremaining: 19m 59s\n",
      "451:\ttest: 0.9073318\tbest: 0.9073561 (448)\ttotal: 16m 27s\tremaining: 19m 56s\n",
      "452:\ttest: 0.9073447\tbest: 0.9073561 (448)\ttotal: 16m 29s\tremaining: 19m 54s\n",
      "453:\ttest: 0.9073499\tbest: 0.9073561 (448)\ttotal: 16m 30s\tremaining: 19m 51s\n",
      "454:\ttest: 0.9073446\tbest: 0.9073561 (448)\ttotal: 16m 32s\tremaining: 19m 49s\n",
      "455:\ttest: 0.9073468\tbest: 0.9073561 (448)\ttotal: 16m 35s\tremaining: 19m 47s\n",
      "456:\ttest: 0.9073523\tbest: 0.9073561 (448)\ttotal: 16m 36s\tremaining: 19m 44s\n",
      "457:\ttest: 0.9073592\tbest: 0.9073592 (457)\ttotal: 16m 38s\tremaining: 19m 42s\n",
      "458:\ttest: 0.9073530\tbest: 0.9073592 (457)\ttotal: 16m 42s\tremaining: 19m 41s\n",
      "459:\ttest: 0.9073510\tbest: 0.9073592 (457)\ttotal: 16m 44s\tremaining: 19m 38s\n",
      "460:\ttest: 0.9073486\tbest: 0.9073592 (457)\ttotal: 16m 46s\tremaining: 19m 36s\n",
      "461:\ttest: 0.9073575\tbest: 0.9073592 (457)\ttotal: 16m 48s\tremaining: 19m 33s\n",
      "462:\ttest: 0.9073587\tbest: 0.9073592 (457)\ttotal: 16m 49s\tremaining: 19m 31s\n",
      "463:\ttest: 0.9073627\tbest: 0.9073627 (463)\ttotal: 16m 51s\tremaining: 19m 28s\n",
      "464:\ttest: 0.9077010\tbest: 0.9077010 (464)\ttotal: 16m 54s\tremaining: 19m 27s\n",
      "465:\ttest: 0.9076897\tbest: 0.9077010 (464)\ttotal: 16m 56s\tremaining: 19m 24s\n",
      "466:\ttest: 0.9076859\tbest: 0.9077010 (464)\ttotal: 16m 58s\tremaining: 19m 22s\n",
      "467:\ttest: 0.9076961\tbest: 0.9077010 (464)\ttotal: 17m\tremaining: 19m 19s\n",
      "468:\ttest: 0.9076868\tbest: 0.9077010 (464)\ttotal: 17m 2s\tremaining: 19m 17s\n",
      "469:\ttest: 0.9077001\tbest: 0.9077010 (464)\ttotal: 17m 4s\tremaining: 19m 14s\n",
      "470:\ttest: 0.9077076\tbest: 0.9077076 (470)\ttotal: 17m 5s\tremaining: 19m 12s\n",
      "471:\ttest: 0.9077131\tbest: 0.9077131 (471)\ttotal: 17m 7s\tremaining: 19m 9s\n",
      "472:\ttest: 0.9077300\tbest: 0.9077300 (472)\ttotal: 17m 9s\tremaining: 19m 7s\n",
      "473:\ttest: 0.9077287\tbest: 0.9077300 (472)\ttotal: 17m 11s\tremaining: 19m 4s\n",
      "474:\ttest: 0.9077442\tbest: 0.9077442 (474)\ttotal: 17m 13s\tremaining: 19m 2s\n",
      "475:\ttest: 0.9077528\tbest: 0.9077528 (475)\ttotal: 17m 15s\tremaining: 18m 59s\n",
      "476:\ttest: 0.9077489\tbest: 0.9077528 (475)\ttotal: 17m 17s\tremaining: 18m 57s\n",
      "477:\ttest: 0.9077164\tbest: 0.9077528 (475)\ttotal: 17m 19s\tremaining: 18m 54s\n",
      "478:\ttest: 0.9077164\tbest: 0.9077528 (475)\ttotal: 17m 20s\tremaining: 18m 52s\n",
      "479:\ttest: 0.9077130\tbest: 0.9077528 (475)\ttotal: 17m 22s\tremaining: 18m 49s\n",
      "480:\ttest: 0.9077424\tbest: 0.9077528 (475)\ttotal: 17m 24s\tremaining: 18m 47s\n",
      "481:\ttest: 0.9077370\tbest: 0.9077528 (475)\ttotal: 17m 26s\tremaining: 18m 44s\n",
      "482:\ttest: 0.9077426\tbest: 0.9077528 (475)\ttotal: 17m 28s\tremaining: 18m 42s\n",
      "483:\ttest: 0.9077346\tbest: 0.9077528 (475)\ttotal: 17m 30s\tremaining: 18m 39s\n",
      "484:\ttest: 0.9077377\tbest: 0.9077528 (475)\ttotal: 17m 32s\tremaining: 18m 37s\n",
      "485:\ttest: 0.9077376\tbest: 0.9077528 (475)\ttotal: 17m 34s\tremaining: 18m 34s\n",
      "486:\ttest: 0.9077311\tbest: 0.9077528 (475)\ttotal: 17m 36s\tremaining: 18m 33s\n",
      "487:\ttest: 0.9077307\tbest: 0.9077528 (475)\ttotal: 17m 38s\tremaining: 18m 30s\n",
      "488:\ttest: 0.9077306\tbest: 0.9077528 (475)\ttotal: 17m 40s\tremaining: 18m 28s\n",
      "489:\ttest: 0.9077263\tbest: 0.9077528 (475)\ttotal: 17m 42s\tremaining: 18m 25s\n",
      "490:\ttest: 0.9077258\tbest: 0.9077528 (475)\ttotal: 17m 44s\tremaining: 18m 23s\n",
      "491:\ttest: 0.9077342\tbest: 0.9077528 (475)\ttotal: 17m 46s\tremaining: 18m 20s\n",
      "492:\ttest: 0.9077369\tbest: 0.9077528 (475)\ttotal: 17m 47s\tremaining: 18m 18s\n",
      "493:\ttest: 0.9077380\tbest: 0.9077528 (475)\ttotal: 17m 49s\tremaining: 18m 15s\n",
      "494:\ttest: 0.9077394\tbest: 0.9077528 (475)\ttotal: 17m 51s\tremaining: 18m 13s\n",
      "495:\ttest: 0.9077404\tbest: 0.9077528 (475)\ttotal: 17m 53s\tremaining: 18m 11s\n",
      "496:\ttest: 0.9077530\tbest: 0.9077530 (496)\ttotal: 17m 55s\tremaining: 18m 8s\n",
      "497:\ttest: 0.9077533\tbest: 0.9077533 (497)\ttotal: 17m 57s\tremaining: 18m 6s\n",
      "498:\ttest: 0.9077144\tbest: 0.9077533 (497)\ttotal: 17m 59s\tremaining: 18m 4s\n",
      "499:\ttest: 0.9078110\tbest: 0.9078110 (499)\ttotal: 18m 3s\tremaining: 18m 3s\n",
      "500:\ttest: 0.9078176\tbest: 0.9078176 (500)\ttotal: 18m 5s\tremaining: 18m 1s\n",
      "501:\ttest: 0.9078109\tbest: 0.9078176 (500)\ttotal: 18m 7s\tremaining: 17m 59s\n",
      "502:\ttest: 0.9078133\tbest: 0.9078176 (500)\ttotal: 18m 9s\tremaining: 17m 56s\n",
      "503:\ttest: 0.9078434\tbest: 0.9078434 (503)\ttotal: 18m 11s\tremaining: 17m 53s\n",
      "504:\ttest: 0.9078395\tbest: 0.9078434 (503)\ttotal: 18m 13s\tremaining: 17m 51s\n",
      "505:\ttest: 0.9078529\tbest: 0.9078529 (505)\ttotal: 18m 15s\tremaining: 17m 49s\n",
      "506:\ttest: 0.9078572\tbest: 0.9078572 (506)\ttotal: 18m 17s\tremaining: 17m 47s\n",
      "507:\ttest: 0.9078573\tbest: 0.9078573 (507)\ttotal: 18m 20s\tremaining: 17m 45s\n",
      "508:\ttest: 0.9078498\tbest: 0.9078573 (507)\ttotal: 18m 21s\tremaining: 17m 42s\n",
      "509:\ttest: 0.9078497\tbest: 0.9078573 (507)\ttotal: 18m 23s\tremaining: 17m 40s\n",
      "510:\ttest: 0.9078620\tbest: 0.9078620 (510)\ttotal: 18m 25s\tremaining: 17m 37s\n",
      "511:\ttest: 0.9078636\tbest: 0.9078636 (511)\ttotal: 18m 27s\tremaining: 17m 35s\n",
      "512:\ttest: 0.9078552\tbest: 0.9078636 (511)\ttotal: 18m 30s\tremaining: 17m 33s\n",
      "\n",
      "bestTest = 0.907863634\n",
      "bestIteration = 511\n",
      "\n",
      "Shrink model to first 512 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training has stopped (degenerate solution on iteration 513, probably too small l2-regularization, try to increase it)\n",
      "2025-04-22 14:05:26 - INFO - Сохранение модели CatBoostRanker...\n",
      "2025-04-22 14:05:26 - INFO - Загрузка модели CatBoostRanker...\n",
      "2025-04-22 14:05:27 - INFO - Генерация финальных рекомендаций...\n",
      "IOStream.flush timed out\n",
      "2025-04-22 14:08:33 - INFO - Расчет финальных метрик...\n",
      "2025-04-22 14:11:44 - INFO - Результаты:\n",
      "2025-04-22 14:11:44 - INFO - Метрики для K=10:\n",
      "2025-04-22 14:11:44 - INFO - NDCG@10: 0.8367\n",
      "2025-04-22 14:11:44 - INFO - Precision@10: 0.1193\n",
      "2025-04-22 14:11:44 - INFO - Recall@10: 0.3068\n",
      "2025-04-22 14:11:44 - INFO - Diversity@10: 0.0052\n",
      "2025-04-22 14:11:44 - INFO - Novelty@10: 0.3679\n",
      "2025-04-22 14:11:44 - INFO - Serendipity@10: 0.0013\n",
      "2025-04-22 14:11:44 - INFO - --------------------------------\n",
      "2025-04-22 14:11:44 - INFO - Метрики для K=100:\n",
      "2025-04-22 14:11:44 - INFO - NDCG@100: 0.8246\n",
      "2025-04-22 14:11:44 - INFO - Precision@100: 0.0688\n",
      "2025-04-22 14:11:44 - INFO - Recall@100: 0.3453\n",
      "2025-04-22 14:11:44 - INFO - Diversity@100: 0.0225\n",
      "2025-04-22 14:11:44 - INFO - Novelty@100: 0.6367\n",
      "2025-04-22 14:11:44 - INFO - Serendipity@100: 0.0007\n",
      "2025-04-22 14:11:44 - INFO - --------------------------------\n",
      "2025-04-22 14:11:44 - INFO - Метрики для K=1000:\n",
      "2025-04-22 14:11:44 - INFO - NDCG@1000: 0.8291\n",
      "2025-04-22 14:11:44 - INFO - Precision@1000: 0.0471\n",
      "2025-04-22 14:11:44 - INFO - Recall@1000: 0.4839\n",
      "2025-04-22 14:11:44 - INFO - Diversity@1000: 0.0777\n",
      "2025-04-22 14:11:44 - INFO - Novelty@1000: 0.7546\n",
      "2025-04-22 14:11:44 - INFO - Serendipity@1000: 0.0005\n",
      "2025-04-22 14:11:44 - INFO - --------------------------------\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Обучение модели CatBoostRanker...\")\n",
    "model_params = {\n",
    "    \"learning_rate\": 0.004684283500572371,\n",
    "    \"iterations\": 1000,\n",
    "    \"depth\": 6,\n",
    "    \"l2_leaf_reg\": 5.36104374072008e-06,\n",
    "    \"random_seed\": 42,\n",
    "    \"thread_count\": -1,\n",
    "    \"verbose\": True,\n",
    "    \"task_type\": \"CPU\",\n",
    "    \"loss_function\": \"Logloss\",\n",
    "    \"eval_metric\": \"NDCG\",\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"random_strength\": 0.007652648646042903,\n",
    "    \"bagging_temperature\": 0.9487648394691982,\n",
    "    \"leaf_estimation_iterations\": 9,\n",
    "}\n",
    "\n",
    "cat_boost_ranker = CatBoostRanker(\n",
    "    learning_rate=model_params[\"learning_rate\"],\n",
    "    iterations=model_params[\"iterations\"],\n",
    "    depth=model_params[\"depth\"],\n",
    "    l2_leaf_reg=model_params[\"l2_leaf_reg\"],\n",
    "    random_seed=42,\n",
    "    thread_count=model_params[\"thread_count\"],\n",
    "    verbose=model_params[\"verbose\"],\n",
    ")\n",
    "cat_boost_ranker.model_params.update(model_params)\n",
    "cat_boost_ranker.fit(train_data, candidates)\n",
    "\n",
    "logging.info(\"Сохранение модели CatBoostRanker...\")\n",
    "model_path = (\n",
    "    f\"{DATA_PATH}/models/cat_boost_ranker_{ORGANIZATION_ID}_{PROCESSING_DATE}.cbm\"\n",
    ")\n",
    "cat_boost_ranker.save_model(model_path)\n",
    "\n",
    "logging.info(\"Загрузка модели CatBoostRanker...\")\n",
    "cat_boost_ranker = CatBoostRanker.load_model(model_path)\n",
    "\n",
    "logging.info(\"Генерация финальных рекомендаций...\")\n",
    "ranked_recommendations = cat_boost_ranker.rank(\n",
    "    candidates=candidates, train_data=train_data, top_n=1000\n",
    ")\n",
    "\n",
    "recommendations_dict = (\n",
    "    ranked_recommendations.groupby(\"buyer_id\")[\"product_id\"].agg(list).to_dict()\n",
    ")\n",
    "\n",
    "logging.info(\"Расчет финальных метрик...\")\n",
    "metrics_calculator = MetricsCalculator([10, 100, 1000])\n",
    "final_metrics = metrics_calculator.calculate(\n",
    "    recommendations=recommendations_dict,\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    item_categories=item_categories,\n",
    ")\n",
    "\n",
    "logging.info(\"Результаты:\")\n",
    "for k in metrics_calculator.k_values:\n",
    "    logging.info(f\"Метрики для K={k}:\")\n",
    "    logging.info(f\"NDCG@{k}: {final_metrics[f'ndcg_{k}']:.4f}\")\n",
    "    logging.info(f\"Precision@{k}: {final_metrics[f'precision_{k}']:.4f}\")\n",
    "    logging.info(f\"Recall@{k}: {final_metrics[f'recall_{k}']:.4f}\")\n",
    "    logging.info(f\"Diversity@{k}: {final_metrics[f'diversity_{k}']:.4f}\")\n",
    "    logging.info(f\"Novelty@{k}: {final_metrics[f'novelty_{k}']:.4f}\")\n",
    "    logging.info(f\"Serendipity@{k}: {final_metrics[f'serendipity_{k}']:.4f}\")\n",
    "    logging.info(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лучшие гиперпараметры моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoost (Градиентный бустинг над решающими деревьями)**:\n",
    "```json\n",
    "{\n",
    "    'learning_rate': 0.004684283500572371, \n",
    "    'depth': 6, \n",
    "    'l2_leaf_reg': 5.36104374072008e-06,\n",
    "    'random_strength': 0.007652648646042903,\n",
    "    'bagging_temperature': 0.9487648394691982,\n",
    "    'leaf_estimation_iterations': 9\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики работы моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Метрика  | CatBoost  |\n",
    "|---|---|\n",
    "|  Время обучения |  40 мин. |\n",
    "|  Время получения предсказаний |  3 мин. |\n",
    "|  NDCG@10 |  0.8367 |\n",
    "|  NDCG@100 |  0.8246 |\n",
    "|  NDCG@1000 |  0.8291 |\n",
    "|  Precision@10 |  0.1193 |\n",
    "|  Precision@100 |  0.0688 |\n",
    "|  Precision@1000 |  0.0471 |\n",
    "|  Recall@10 |  0.3068 |\n",
    "|  Recall@100 |  0.3453 |\n",
    "|  Recall@1000 |  0.4839 |\n",
    "|  Diversity@10 |  0.0052 |\n",
    "|  Diversity@100 |  0.0225 |\n",
    "|  Diversity@1000 |  0.0777 |\n",
    "|  Novelty@10 |  0.3679 |\n",
    "|  Novelty@100 |  0.6367 |\n",
    "|  Novelty@1000 |  0.7546 |\n",
    "|  Serendipity@10 |  0.0013 |\n",
    "|  Serendipity@100 |  0.0007 |\n",
    "|  Serendipity@1000 |  0.0005 |\n",
    "\n",
    "**Сравнение временных затрат производилось на одинаковых датасетах и одинаковом оборудовании на платформе Intel Ice Lake с 32 vCPU и 256 Гб RAM**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
